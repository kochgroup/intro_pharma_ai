{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a9797f",
   "metadata": {},
   "source": [
    "# CNN & Transfer Learning\n",
    "\n",
    "\n",
    "\n",
    "**Lernziele**\n",
    "\n",
    "---\n",
    "* Sie verstehen, was mit Transfer Learning gemeint ist.\n",
    "* Sie können vortrainierte Modelle laden und anpassen.\n",
    "* Sie können Bilder aus Ordnern in Python einlesen.\n",
    "---\n",
    "\n",
    "Letzte Woche haben Sie ein CNN entwickelt, das Zahlen erkennen kann. Heute geht es darum, die Vorteile von bereits trainierten Netzwerken zu nutzen. Zunächst werden Sie ein Modelltrainieren, das zwischen Hunde- und Katzenrassen unterscheiden kann. In der Übungsaufgabe werden Sie ResNet verwenden, um Lungenentzündung in Röntgenbildern zu erkennen.\n",
    "\n",
    "Beim Transfer Lerning wird ein bereits trainiertes Modell für ein anderes Machine Learning Problem verwendet. \n",
    "\n",
    "<img align=\"center\" src=\"https://pennylane.ai/qml/_images/transfer_learning_general.png\" width=\"400\">\n",
    "<h6 align=\"center\">Pennylane.ai</h6>\n",
    "\n",
    "\n",
    "Das pre-trained Modell ist meist ein Modell, das mit vielen allgemeinen Daten trainiert wurde. Dadurch konnte das Modell genügend generelle Informationen lernen, welche auch für sehr spezifisches Problem relevant sein können.\n",
    "\n",
    "ResNet wurde zum Beispiel mit den Daten von ImageNet trainiert. Diese enthalten keine Röntgenbilder. Aber durch die Kombination von bereits trainierten Layers des ResNet-Modells und neuen, nicht trainierten Layers können wir das \"Wissen\" von ResNet nutzen. \n",
    "\n",
    "Das Trainieren dieser Modelle kann diese Woche dauern!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92596ce",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Das heutige Training wird viel länger dauern als bisher. Wenn Sie dieses Notebook auf Google Collab laufen lassen, können Sie einfach einen GPU verwenden, um das Training zu beschleunigen. Damit Sie auf einem GPU trainieren können, müssen Sie einfach den folgenden Anweisungen folgen:**\n",
    "\n",
    "Wählen Sie oben auf der Website die folgenden Menüs aus.\n",
    "\n",
    "```\n",
    "Lauftzeit > Laufzeittyp ändern > Hardwarebeschleuniger \n",
    "```\n",
    "\n",
    "Falls Ihr Google Collab auf Englisch ist:\n",
    "\n",
    "```\n",
    "Runtime > Change runtime type > Hardware Accelerator\n",
    "```\n",
    "\n",
    "Wählen Sie hier *GPU*.\n",
    "\n",
    "Wenn Sie dieses Notebook lokal (auf Ihrem eigenen Rechner) laufen lassen, müssen Sie PyTorch mit Cuda-Unterstützung und der richtigen Version von Cuda auf Ihrem System installieren, um einen GPU zu verwenden. Dies funktioniert nur, wenn Sie eine GPU in Ihr Laptop oder PC über einen GPU verfügt. Aber dieses Notebook funktioniert auch, wenn Sie keinen Zugang zu einem GPU haben, es wird nur etwas länger dauern.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e15f358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "!pip install rdkit==2022.3.4\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch import sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "%matplotlib inline\n",
    "if 'google.colab' in sys.modules: # checks whether the notebook runs on collab\n",
    "    !pip install rdkit==2022.3.4\n",
    "    !wget https://raw.githubusercontent.com/kochgroup/intro_pharma_ai/main/utils/utils.py\n",
    "    %run utils.py\n",
    "else:\n",
    "    %run ../utils/utils.py # lädt vorgeschriebene Funktionen\n",
    "\n",
    "plt.ion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff50bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der folgende Code lädt die Daten für das Notebook herunter, kann etwas länger dauern\n",
    "if 'google.colab' in sys.modules:\n",
    "    !wget https://uni-muenster.sciebo.de/s/TaOR0Lk50rjPHUU/download\n",
    "    !unzip -q download -d  ../\n",
    "    !rm download\n",
    "else:\n",
    "    import wget\n",
    "    import zipfile\n",
    "    wget.download(\"https://uni-muenster.sciebo.de/s/TaOR0Lk50rjPHUU/download\")\n",
    "    with zipfile.ZipFile(\"data.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274e9e4",
   "metadata": {},
   "source": [
    "MNIST ist ein relativ kleiner Datensatz und kann daher auf einmal in den Speicher geladen werden. Wie in der Vorlesung besprochen, sind Bilder jedoch normalerweise größer als die Zahlen in MNIST. Um mit großen Bilddatensätzen umgehen zu können, hat PyTorch seine eigene Library `torchvision`. Hier sind wichtige Funktionen enthalten, die wir im normalen `torch` nicht haben. \n",
    "\n",
    "Wichtig zu beachten ist auch, wie die Daten nun gespeichert werden. Wenn Sie in den Ordner `data/images_animals/` navigieren, sehen Sie zwei Ordner. Der erste Ordner `train` enthält die Trainingsbilder. Der zweite Ordner `val` enthält die Testdaten. Innerhalb dieser Ordner gibt es wiederum Unterordner, die nach den Labels der Bilder benannt sind. Das bedeutet, dass z.B. der Ordner `beagel` nur Bilder von Beageln enthält.\n",
    "\n",
    "Wenn genau eine solche Ordnerstruktur existiert, können wir die Daten sehr einfach mit `torchvision` einlesen.\n",
    "Aber bevor wir die Daten einlesen können, müssen wir Transformationen der Bilder definieren. \n",
    "\n",
    "Erstens sind die Bilder zu groß. Die meisten vortrainierten Modelle erwarten eine Bildgröße von 224 x 224 Pixeln, da dies die Größe der Bilder im ImageNet-Datensatz ist. Außerdem müssen die Bilder noch in einen `tensor` umgewandelt werden. In einem letzten Schritt skalieren wir die Daten. Diesmal verwenden wir nicht den `minmax`-Skalierer, sondern normalisieren die Bilder. **Dazu werden der Mittelwert und die Standardabweichung der ImageNet-Bilder verwendet. Denn mit diesen Bildern (deren Mittelwertzen) wurde das Netzwerk trainiert.\n",
    "\n",
    "Die Funktion `transforms.Compose()` funktioniert ähnlich wie `nn.Sequential`. Auf alle Bilder werden nacheinander alle Transformationen angewandt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c885c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize((224,224)), #reduziert die Größe des Bildes\n",
    "        transforms.ToTensor(), #konvertiert das Bild zu einem Tensor\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) #Normalisiert die Bilder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d194a454",
   "metadata": {},
   "source": [
    "Nun, da die Transformationen definiert sind, können Sie einen PyTorch-Datensatz erstellen. Diesmal verwenden wir aber die spezielle Klasse `datasets.ImageFolder`. Diese spezielle Klasse `dataset` ist genau auf unsere Ordnerstruktur abgestimmt. Wir müssen nur den `path` zu den Bildern angeben und welche Transformationen wir anwenden wollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e112fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder('../data/images_animals/train',data_transforms)\n",
    "test_data = datasets.ImageFolder('../data/images_animals/val',data_transforms)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef23177",
   "metadata": {},
   "source": [
    "Sie können sehen, dass wir insgesamt 5913 Bilder in unserem Trainingsordner haben. Außerdem sind die Transformationen, die angewendet werden sollen, aufgelistet.\n",
    "\n",
    "Als letzten Schritt erstellen wir den `DataLoader`. Diesmal auch für den Testdatensatz, da wir nicht alle Bilder auf einmal \"ins Netzwerk laden\" können und daher auch die Evaluierung in Batches erfolgen müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f0709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1235)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=16,\n",
    "                                             shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=16,\n",
    "                                             shuffle=True)\n",
    "\n",
    "example_batch = datasets.ImageFolder('../data/example_batch',data_transforms)\n",
    "example_batch = torch.utils.data.DataLoader(example_batch, batch_size=6,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a273dbf4",
   "metadata": {},
   "source": [
    "Sie wissen bis jetzt noch nicht, welche und wie viele verschiedene Klasse wir haben. Wir können uns aus dem Datensatz diese Information holen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf26858d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_names = train_data.classes\n",
    "print(class_names)\n",
    "print(\"\\nAnzahl Klassen: \",len(class_names))\n",
    "\n",
    "# Wir speichern einen Batch, um diesen besser zu analysieren\n",
    "inputs_example, targets_example = next(iter(example_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ce1ec8",
   "metadata": {},
   "source": [
    "Insgesamt haben wir 37 verschiedene Arten von Hunden und Katzen. Wir können uns die Bilder auch mit einer vorgeschriebenen Funktion anschauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c51084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = torchvision.utils.make_grid(inputs_example[:6])\n",
    "imshow(out, title=[\"birman\", \"birman\", \"persian\", \"persian\", \"pug\", \"sphynx\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecd8cc9",
   "metadata": {},
   "source": [
    "### RESNET\n",
    "\n",
    "Sie haben jetzt die Daten im richtigen Format. Bevor wir jedoch mit dem Training beginnen können, müssen wir auch unser Modell in das richtige Format bringen. \n",
    "Wie bereits erwähnt, bietet PyTorch mehrere Modelle, die bereits trainiert wurden. Diese können einfach geladen werden. *Wenn Sie ein Modell zum ersten Mal laden, müssen die Gewichte noch aus dem Internet geladen werden, was einige Zeit dauern kann.*\n",
    "\n",
    "Wir verwenden auch nur ResNet18, da alle größeren Netze zu langsam wären, um sie auf dem Uniserver zu trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c9febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a668d38",
   "metadata": {},
   "source": [
    "`resnet18` gibt Ihnen eine Übersicht, welche PyTorch Layers in welcher Reihenfolgen benutzt werden. Achten Sie vor allem auf die letzte Layer mit dem Namen `fc`. Diese können wir auch direkt mit `resnet.fc` auswählen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9024958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18.fc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c016962f",
   "metadata": {},
   "source": [
    "Diese Layer ist eine `nn.Linear`-Layer, die Sie aus dem PyTorch-Einführungs Notebook kennen sollten. Sie hat 512 Features als Inputgröße und 1000 als Output. Diese 1000 Outputneuronen entsprechen den 1000 verschiedenen Klassen im ImageNet-Datensatz.\n",
    "\n",
    "Um das \"ResNet\"-Modell weiter vorzubereiten, wollen wir zunächst alle Layers des ResNet einfrieren. Das bedeutet, dass diese Layers keine Weightupdates erhalten und somit nicht weiter trainiert werden können. Wir können das machen, da das Modell bereits trainiert wurde.\n",
    "Der folgende Code iteriert durch alle Schichten und setzt `requires_grad` auf `False`. Das lässt PyTorch wissen, dass für diese Layers keine Gradienten berechnet werden müssen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7be72",
   "metadata": {},
   "source": [
    "Als letzes müssen wir nur noch die `fc` Layer austauschen. Da wir nicht 1000 Klassen, sondern 37 haben. Also brauchen wir eine neue `nn.Linear` Layer, die als Input die Größe 512 hat und als Output die Größe 37."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4f763",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "resnet18.fc = nn.Linear(512, 37) #ersetzten der Linear Layer\n",
    "\n",
    "print(resnet18.fc)\n",
    "list(resnet18.fc.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d356c",
   "metadata": {},
   "source": [
    "Sie können erkennen, dass die neue `fc` Layer `requires_grad=True` gesetzt hat. Das bedeutet diese Weights werden während des Trainings geupdatet. Also ist die `fc` Layer die einzige zu trainiernde Layer im Netzwerk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1beb053",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "Jetzt können wir mit dem Trainingsloop beginnen. Doch zunächst definieren wir die Loss Funktion und den Optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7cfaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet18.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b9df3",
   "metadata": {},
   "source": [
    "Die Trainingsloop wird heute etwas komplexer aussehen. Das liegt daran, dass wir jetzt auch das Testset als Batches durch das Netzwerk führen. Um die Metriken trotzdem korrekt zu berechnen, haben wir `running_loss` und `running_corrects` verwendet, um sie am Ende des Loops zu mitteln.\n",
    "Der Trainingsprozess wird aufgrund der vielen Berechnungen ziemlich lange dauern, selbst wenn nur eine Layer trainiert wird.\n",
    "\n",
    "\n",
    "Der folgende Code prüft, ob ein GPU verfügbar ist. Wenn dies der Fall ist, findet das gesamte Training auf dem GPU statt, andernfalls wird auf dem CPU trainiert.\n",
    "\n",
    "```python\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "```\n",
    "\n",
    "Um auf der GPU zu trainieren, müssen wir das Modell und die Daten auf den GPU verschieben. Dies geschieht mit: `.to(device)` \n",
    "\n",
    "```python\n",
    "resnet18.to(device)\n",
    "```\n",
    "Schließlich müssen die Batches während des Trainings auch auf den GPU verschoben werden:\n",
    "\n",
    "```python\n",
    "inputs = inputs.to(device)\n",
    "targets = targets.to(device)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8137322",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "resnet18.to(device)\n",
    "\n",
    "torch.manual_seed(3333)\n",
    "for epoch in range(3):\n",
    "    #### Training ####\n",
    "    resnet18.train()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output=resnet18(inputs)\n",
    "        _ , preds = torch.max(output, 1)\n",
    "        loss = loss_funktion(output,targets)\n",
    "        running_loss +=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_corrects +=torch.sum(preds == targets.data).cpu() \n",
    "    epoch_loss = running_loss/len(train_loader)    \n",
    "    epoch_acc = running_corrects.double() / len(train_data)  \n",
    "    print('Trainings Loss: {:.4f} Trainings Acc: {:.4f}'.format(\n",
    "        epoch_loss, epoch_acc))\n",
    "    \n",
    "    \n",
    "    #### Evaluierung #####\n",
    "    resnet18.eval()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    for inputs, targets in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        output=resnet18(inputs)\n",
    "        _ , preds =torch.max(output, 1)\n",
    "        loss = loss_funktion(output,targets)\n",
    "        running_loss +=loss.item()\n",
    "        running_corrects +=torch.sum(preds == targets.data).cpu() \n",
    "    epoch_acc = running_corrects.double() / len(test_data) \n",
    "    epoch_loss = running_loss/len(test_loader)    \n",
    "    print('Test Loss: {:.4f} Test Acc: {:.4f}'.format(\n",
    "        epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a29f1c",
   "metadata": {},
   "source": [
    "Nach drei Epochen erreichen wir bereits eine Testaccuracy von 0,8. 80 % der Bilder werden richtig klassifiziert.\n",
    "\n",
    "Um wirklich sicher zu gehen, dass das Pre-Training des Modells einen Unterschied gemacht hat, trainieren wir dasselbe Modell noch einmal. Diesmal jedoch ohne die vortrainierten Weights zu laden:\n",
    "\n",
    "`pretrained=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db76ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=False) #  <- ResNet wird ohne die vortrainierten Weights geladen\n",
    "torch.manual_seed(1234)\n",
    "resnet18.fc = nn.Linear(512, 37)\n",
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet18.parameters(), lr=0.0001)\n",
    "torch.manual_seed(3333)\n",
    "for epoch in range(3):\n",
    "    \n",
    "    #### Training ####\n",
    "    resnet18.train()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output=resnet18(inputs)\n",
    "        _ , preds = torch.max(output, 1)\n",
    "        loss = loss_funktion(output,targets)\n",
    "        running_loss +=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_corrects +=torch.sum(preds == targets.data)   \n",
    "    epoch_loss = running_loss/len(train_loader)    \n",
    "    epoch_acc = running_corrects.double() / len(train_data)  \n",
    "    print('Trainings Loss: {:.4f} Trainings Acc: {:.4f}'.format(\n",
    "        epoch_loss, epoch_acc))\n",
    "    \n",
    "    #### Evaluierung #####\n",
    "    resnet18.eval()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    for inputs, targets in tqdm(test_loader):\n",
    "        output=resnet18(inputs)\n",
    "        _ , preds =torch.max(output, 1)\n",
    "        loss = loss_funktion(output,targets)\n",
    "        running_loss +=loss.item()\n",
    "        running_corrects +=torch.sum(preds == targets.data)\n",
    "    epoch_acc = running_corrects.double() / len(test_data) \n",
    "    epoch_loss = running_loss/len(test_loader)    \n",
    "    print('Test Loss: {:.4f} Test Acc: {:.4f}'.format(\n",
    "        epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9f8c8",
   "metadata": {},
   "source": [
    "Tatsächlich sind wir nach 3 Epochen nicht annähernd so genau, als wenn wir das \"vortrainierte\" Modell verwendet hätten. Das liegt daran, dass die pretrained Convolutions eine Art Feature-Generierung durchführen.\n",
    "\n",
    "Wir können dies deutlicher sehen, wenn wir uns die Aktivierungen der Convolution ansehen. \n",
    "Dazu verwenden wir die Beispielbilder vom Anfang dieses Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d8454a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = torchvision.utils.make_grid(inputs_example[:6])\n",
    "imshow(out, title=[\"birman\", \"birman\", \"persian\", \"persian\", \"pug\", \"sphynx\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064fdb47",
   "metadata": {},
   "source": [
    "Zunächst laden wir wieder das vortrainierte Resnet Model. Auch hier entfernen wir die `fc` Layer, aber ersetzen diese nicht durch eine neue linear Layer. Dadurch haben wir direkt Zugriff auf den Output der Convolution Layer. Diese Model nennen wir `resnet_convolutions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0460706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet_convolutions = nn.Sequential(*list(resnet18.children())[:-1])\n",
    "resnet_convolutions.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26168826",
   "metadata": {},
   "source": [
    "Als Letztes führen wir die 6 Bilder von gerade eben durch dieses spezielle Netzwerk und speichern den Output (`feature_encoding`). Dieser Output dient als der Input für die linear Layer, die wir trainieren würden.\n",
    "\n",
    "<img align=\"center\" src=\"Img/transferlearning/tf_2.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b2aa34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_encodings=resnet_convolutions(inputs_example)[:6,:,0,0]\n",
    "feature_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938748d5",
   "metadata": {},
   "source": [
    "Diese \"Encodings\" sollen eine Art reduzierte Darstellung des Originalbildes sein. Eine Art Fingerprint. Wenn es stimmt, dass die vortrainierten Convolutions bestimmte Merkmale finden, die für die Klassifizierung relevant sind, dann sollten ähnliche Bilder auch ähnliche Merkmale aufweisen.\n",
    "\n",
    "Zum Beispiel zeigen das dritte und vierte Bild jeweils eine \"persische\" Katze. Also sollten auch die Encodings der Bilder ähnlich sein. Wir können mit Hilfe der `cosine_similarity` beurteilen, wie ähnlich zwei Vektoren sind. Die Werte liegen immer zwischen -1 (sehr unähnlich) und 1 (sehr ähnlich). \n",
    "Wir können die Ähnlichkeit zwischen dem dritten Bild (`persian`) und allen anderen Bildern berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06018359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cosine_similarity(feature_encodings[2:3].detach(),feature_encodings.detach()).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824b0499",
   "metadata": {},
   "source": [
    "Die Ähnlichkeit des dritten Bildes zum dritten Bild ist natürlich `1`, weil es das selbe Bild ist. Aber zu den anderen Bildern ist die Ähnlichkeit geringer. Am ähnlichsten ist das vierte Bild mit `0.891`, dieses Bild ist ebenfalls ein Bild einer `persian` Katze. Das bedeutet, dass dieses vortrainierte Modell bereits in der Lage war, bestimmte Ähnlichkeiten in den Bildern zu erkennen.   \n",
    "\n",
    ">Aber die Bilder könnten auch schon vor den Convolutions ähnlich sein?\n",
    "\n",
    "Das ist richtig, aber wir können auc das überprüfen. In der folgenden Zelle berechnen wir die Ähnlichkeit der Originalbilder vor den Convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9dbf5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cosine_similarity(inputs_example.flatten(1)[2:3],inputs_example.flatten(1)[0:6]).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9c1cef",
   "metadata": {},
   "source": [
    "Hier fällt auf, dass das zweite Bild einer `persian`  Katze das unähnlichste ist, obwohl auf beiden Bildern die gleiche Katzenart zu sehen ist. Wir können also schlussfolgern, dass das Netzwerk in der Tat ähnliche Fetaures in Bildern finden kann.\n",
    "\n",
    "**ImageNet, der Datensatz, auf dem ResNet ursprünglich trainiert wurde, enthält auch verschiedene Katzen- und Hunderassen, darunter auch die Rasse \"Perser\". Die Auswirkungen des Pretrainings werden wahrscheinlich weniger ausgeprägt sein, wenn es um die Klassifizierung von Rassen geht, die für ResNet völlig \"neu\" sind.**\n",
    "\n",
    "Zum Schluss probieren wir aus, wie gut unser Netzwerk funktioniert, wenn wir das vortrainierte Modell laden und unsere eigene lineare Schicht erstellen. Diesmal frieren wir jedoch die vortrainierten Convolution Layers nicht ein, sondern trainieren diese auch noch weiter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d6190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True) #PRETRAIN = TRUE\n",
    "torch.manual_seed(1234)\n",
    "resnet18.fc = nn.Linear(512, 37) \n",
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet18.parameters(), lr=0.0001)\n",
    "torch.manual_seed(3333)\n",
    "for epoch in range(3):\n",
    "    \n",
    "    #### Training ####\n",
    "    resnet18.train()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output=resnet18(inputs)\n",
    "        _ , preds = torch.max(output, 1)\n",
    "        loss = loss_funktion(output,targets)\n",
    "        running_loss +=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_corrects +=torch.sum(preds == targets.data)   \n",
    "    epoch_loss = running_loss/len(train_loader)    \n",
    "    epoch_acc = running_corrects.double() / len(train_data)  \n",
    "    print('Trainings Loss: {:.4f} Trainings Acc: {:.4f}'.format(\n",
    "        epoch_loss, epoch_acc))\n",
    "    \n",
    "    #### Evaluierung #####\n",
    "    resnet18.eval()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    for inputs, targets in tqdm(test_loader):\n",
    "        output=resnet18(inputs)\n",
    "        _ , preds =torch.max(output, 1)\n",
    "        loss = loss_funktion(output,targets)\n",
    "        running_loss +=loss.item()\n",
    "        running_corrects +=torch.sum(preds == targets.data)\n",
    "    epoch_acc = running_corrects.double() / len(test_data) \n",
    "    epoch_loss = running_loss/len(test_loader)    \n",
    "    print('Test Loss: {:.4f} Test Acc: {:.4f}'.format(\n",
    "        epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b377c",
   "metadata": {},
   "source": [
    "Dieses Netz führt zu den besten Ergebnissen. Das liegt daran, dass nun auch die Weights der Convolutions weiter trainiert werden können. So wird auch die Feeaturegenerierung besser auf unseren Datensatz abgestimmt. \n",
    "\n",
    "In der Praxis werden oft unterschiedliche Lernraten für die neue lineare Layers und die bereits trainierten Convolutions verwendet. Dadurch kann die neue lineare Layer schneller trainiert werden als die Convolutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c80e4",
   "metadata": {},
   "source": [
    "# Übungsaufagbe\n",
    "\n",
    "Bitte starten Sie den Kernel erneut, bevor Sie die Übung durchführen.\n",
    "\n",
    "Wie in der Vorlesung mehrfach besprochen, benutzen wir heute für die Übung ein vortrainiertes Modell zur Erkennung einer Lungenentzündung an Hand von Röntgenbildern.\n",
    "\n",
    "Dazu müssen Sie die Daten korrekt einlesen, das Modell vorbereiten und die `for-loop` ausfüllen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2aa1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from torch.nn.functional import sigmoid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch import sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from __future__ import print_function, division\n",
    "from torch.nn.functional import sigmoid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch import sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install rdkit==2022.3.4\n",
    "    !wget https://raw.githubusercontent.com/kochgroup/intro_pharma_ai/main/utils/utils.py\n",
    "    %run utils.py\n",
    "else:\n",
    "    %run ../utils/utils.py\n",
    "\n",
    "plt.ion()\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4a6b3c",
   "metadata": {},
   "source": [
    "Navigieren Sie zunächst zu dem Ordner, in dem die Tierbilder bereits gespeichert sind. Dort finden Sie auch einen Ordner `chest_xray`. Dieser Ordner enthält ebenfalls Unterordner mit den jeweiligen Trainings- und Testdatensätzen.\n",
    "Legen Sie zunächst fest, welche Transformationen auf die Bilder angewendet werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce697294",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize((224,224)), #reduziert die Größe des Bildes\n",
    "        transforms.ToTensor(), #konvertiert das Bild zu einem Tensor\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) #Normalisiert die Bilder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0d7dac",
   "metadata": {},
   "source": [
    "Als Nächstes laden Sie die entsprechenden `Datasets` und `DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940feba4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder('../data/chest_xray/train',data_transforms)\n",
    "test_data = datasets.ImageFolder('../data/chest_xray/val',data_transforms)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d40f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1235)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cbd8ce",
   "metadata": {},
   "source": [
    "Finden Sie heraus, wie viele verschiedene Klassen wir haben, und denken Sie daran, dass dies die Definition unserer Lossfunktion und unseres Netzwerks beeinflusst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87273af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_data.classes\n",
    "print(class_names)\n",
    "print(\"\\nAnzahl Klassen: \",len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f56a574",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inputs_example, targets_example = next(iter(train_loader))\n",
    "\n",
    "out = torchvision.utils.make_grid(inputs_example[:2])\n",
    "imshow(out, title=[class_names[x] for x in targets_example[:2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d9cb91",
   "metadata": {},
   "source": [
    "Laden Sie zunächst das **vortrainierte** `resnet18`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa754da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(____________)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c5aaa",
   "metadata": {},
   "source": [
    "Verhindern Sie, dass die `resnet` Layers noch weiter trainiert werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c2475",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet18.parameters():\n",
    "    __________ = ____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f3878f",
   "metadata": {},
   "source": [
    "Ersetzen Sie die richtige Layer mit einer neuen Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5270c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "______ = ___________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2197e868",
   "metadata": {},
   "source": [
    "Definieren Sie Lossfunktion und Optimizer. Welche Lossfunktion sollten wir für diese Anzahl an Klassen nehmen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f3f11c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_funktion = __________\n",
    "optimizer = optim.Adam(_______________, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a6c06d",
   "metadata": {},
   "source": [
    "Füllen Sie als letztes den Trainingsloop aus. Wir benutzen `type_as(output)` um die richtigen `dtype` zuerhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14803a02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(3333)\n",
    "for epoch in range(3):\n",
    "    \n",
    "    #### Training ####\n",
    "    resnet18.train()\n",
    "    \n",
    "    #Brauchen wir für die Loss und AUC Berechnung\n",
    "    running_loss = 0\n",
    "    pred_ll = []\n",
    "    targets_ll = []\n",
    "    \n",
    "    \n",
    "    for inputs, targets in tqdm(_______):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward Propagation\n",
    "        output=resnet18(__________).squeeze()\n",
    "        loss = loss_funktion(_______ , _____.type_as(output))\n",
    "        \n",
    "        # Speichern des Loss und der Predictions\n",
    "        pred_ll.append(sigmoid(output).squeeze().detach().clone().numpy())\n",
    "        targets_ll.append(targets.detach().numpy())\n",
    "        running_loss +=loss.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "         \n",
    "    epoch_loss = running_loss/len(train_loader)    \n",
    "    epoch_auc =  roc_auc_score(targets_ll,pred_ll)\n",
    "    print('Trainings Loss: {:.4f} Trainings AUC: {:.4f}'.format(\n",
    "        epoch_loss, epoch_auc))\n",
    "    \n",
    "    \n",
    "    #### Evaluierung #####\n",
    "    resnet18.eval()\n",
    "    \n",
    "    #Brauchen wir für die Loss und AUC Berechnung\n",
    "    running_loss = 0\n",
    "    pred_ll = []\n",
    "    targets_ll = []\n",
    "    \n",
    "    for inputs, targets in tqdm(__________):\n",
    "        \n",
    "        #Forward Propagation\n",
    "        output=resnet18(_______).squeeze()\n",
    "        loss = loss_funktion(______,______.type_as(output))\n",
    "        \n",
    "        pred_ll.append(sigmoid(output).squeeze().detach().clone().numpy())\n",
    "        targets_ll.append(targets.detach().numpy())\n",
    "        running_loss +=loss.item()\n",
    "\n",
    "    epoch_auc =  roc_auc_score(targets_ll,pred_ll)\n",
    "    epoch_loss = running_loss/len(test_loader)    \n",
    "    print('Test Loss: {:.4f} Test Auc: {:.4f}'.format(\n",
    "        epoch_loss, epoch_auc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
