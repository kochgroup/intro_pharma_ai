{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erstes Neuronales Netzwerk\n",
    "---\n",
    "### Lernziele\n",
    "\n",
    "- Sie lernen, wie One-Hot-Codierung funktioniert.\n",
    "- Sie können ein einfaches neuronales Netz in Python programmieren.\n",
    "---\n",
    "\n",
    "Bevor wir das Netz trainieren, müssen wir als erstes unsere Bibliotheken und Funktionen laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktiverungsfunktionen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)) # np.exp() = e^()\n",
    "\n",
    "def softmax(x, axis=1):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "Wir wollen die Labels nicht als einfache Zahl, sondern als Vektor aus Nullen und Einsen haben. Siehe unten:<br>\n",
    "<center>\n",
    "0  =  [1 0 0 0 0 0 0 0 0 0]<br>\n",
    "1  =  [0 1 0 0 0 0 0 0 0 0]<br>\n",
    "2  =  [0 0 1 0 0 0 0 0 0 0]<br>\n",
    "3  =  [0 0 0 1 0 0 0 0 0 0]<br>\n",
    "4  =  [0 0 0 0 1 0 0 0 0 0]<br>\n",
    "5  =  [0 0 0 0 0 1 0 0 0 0]<br>\n",
    "usw. <br>\n",
    "9  =  [0 0 0 0 0 0 0 0 0 1]<br>\n",
    "</center>\n",
    "\n",
    "Die Funktion `one-hot` macht genau das."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels):\n",
    "    one_hot_matrix = np.zeros(\n",
    "        [\n",
    "            len(labels),\n",
    "            len(set(labels))\n",
    "        ]\n",
    "    )\n",
    "    for i,x in enumerate(labels):\n",
    "        one_hot_matrix[i,x] = 1\n",
    "    return one_hot_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skaliert die originalen Bilder, die Werte zwischen 0 und 255 haben können, zu Bildern mit Werten zwischen 0 und 1.\n",
    "# Damit können Neuronale Netzwerke besser trainiert werden\n",
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten einlesen\n",
    "\n",
    "Heute werden wir mit dem MNIST-Datensatz \\[1\\] arbeiten. Es ist wahrscheinlich der berühmteste Datensatz im Deep Learning. Er enthält Bilder von handgeschriebenen Ziffern zwischen null und neun. Das Ziel ist es, ein Modell zu trainieren, das diese Ziffern korrekt erkennt.\n",
    "\n",
    "\n",
    "Wir können die benötigten Daten problemlos mit `numpy` einlesen. Die Trainingsdaten `mnist_train.csv` enthalten Bilder und deren Labels. Die Bilder sind bereits von einer Matrix in einen Vektor transformiert worden.<br>\n",
    "<center>\n",
    "<img src=\"https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/73_blog_image_1.png\" style=\"width: 800px;\">\n",
    "</center>\n",
    "\n",
    "Der Datensatz kann auch extern eingesehen werden.  Kopieren Sie einfach den Link `'https://uni-muenster.sciebo.de/s/xSU1IKM6ui4WKAV/download'` in Ihren Browser und laden Sie die Datei herunter. Die erste Spalte des jeweiligen Datensatzes enthält die Labels. In jeder Reihe ist ein Bild, zudem ist jeder Spalte (bis auf der ersten) die Werte eines spezifischen Pixels zugeordnet.\n",
    "\n",
    "Mit `np.genfromtxt()` lassen sich aus `.txt`-Datein Arrays in Python erstellen. \n",
    "\n",
    "\n",
    "In diesem Beispiel sind die Labels bzw. Targets in der ersten Spalte und nicht in der letzten.\n",
    "\n",
    "---\n",
    "\\[1\\] Deng, L. (2012). The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6), 141–142."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt('https://uni-muenster.sciebo.de/s/xSU1IKM6ui4WKAV/download', delimiter=',', skip_header =False) \n",
    "#genfromtxt liest .txt Datein, mit delimiter =\",\" können auch .csv (comma seperated values) Datein einglesen werden  \n",
    "test_data = np.genfromtxt('https://uni-muenster.sciebo.de/s/fByBt5wd24chROg/download', delimiter=',', skip_header =False) \n",
    "# Hier lesen wir die Test Daten ein.\n",
    "\n",
    "# Nach dem wir die Daten einglesen haben, teilen wir die Daten noch in Bilder und Labels.\n",
    "# Außerdem konvertieren wir Labels von Float zu Integer mit .astype(int).\n",
    "train_labels=train_data[:,0].astype(int) \n",
    "train_images = train_data[:,1:]\n",
    "\n",
    "test_labels=test_data[:,0].astype(int)\n",
    "test_images = test_data[:,1:]\n",
    "\n",
    "del train_data, test_data # für mehr Speicher löschen wir ursprüblichen Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben nun den Trainingsdatensatz eingelesen. Mit `train_images.shape` können Sie sehen, dass die Variable `train_images` eine 60000 x 784 Matrix ist, also 60000 Zeilen und 784 Spalten hat. Jede Zeile ist ein Bild und jede Spalte ist ein Pixel. Die ursprünglichen Bilder waren 28 x 28 Pixel groß. Im nächsten Schritt verwenden wir die *Funktion* `one_hot`, um die Labels korrekt zu kodieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets=one_hot(train_labels)\n",
    "test_targets = one_hot(test_labels)\n",
    "\n",
    "train_targets[:5,:] # die Labels der ersten fünf Bilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als letzten Schritt müssen wir den `min_max`-Scaler verwenden, um die Pixelwerte zwischen Null und Eins zu skalieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = min_max(train_images)\n",
    "test_images = min_max(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der folgenden Zelle können Sie ein Beispielbild sehen. Mit der Funktion `.reshape([28,28])` wird das Bild in seine ursprüngliche Form zurückverwandelt. Auf diese Weise können Sie das Bild wirklich sehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[0].reshape([28, 28]), cmap=\"gray\")\n",
    "print(\"Correct Label: %s\" % train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das Modell\n",
    "\n",
    "\n",
    "**Initialisierung der Weights**\n",
    "\n",
    "Zunächst müssen die Weight- und Bias-Matrizen/Vektoren mit den richtigen Dimensionen erstellt werden. Zu diesem Zweck wird eine Funktion geschrieben. In ihr muss man die Größe der jeweiligen Layers (`input_size`, `hidden_size`, `output_size`) definieren. Mit diesen kann man die Weightmatrizen erstellen. Während die `b`-Vektoren mit Nullen gefüllt werden können, müssen die Weightsmatrizen mit zufälligen kleinen Zahlen initialisiert werden.\n",
    "\n",
    "\n",
    "Die Funktion `np.random.randn(number_of_rows, number_of_columns)` setzt zufällige Werte zwischen minus Eins und plus Eins in die Matrix ein. Zusätzlich wird der Term `*np.sqrt(2/layer_size)` hinzugefügt. Dieser verkleinert die Werte noch weiter und soll damit ein besseres Trainieren des Netzwerkes garantieren. \n",
    "\n",
    "Im folgenden Code sollen Sie die richtigen Matrixgrößen (Anzahl der Reihen/Spalten) eintragen. Die Inputmatrix `X` hat 60000 Reihen und 784 Spalten und bei einer Matrixmultiplikation müssen die Anzahl der Spalten der ersten Matrix mit der Anzahl der Reihen der zweiten Matrix übereinstimmen.\n",
    "Denken Sie auch daran, dass wir das Transpose der Weightmatrix benutzen werden.\n",
    "\n",
    "Den Bias `b` wird mit Nullen initialisiert. `np.zeros(500)` würde ein Vektor der Länge 500 mit Nullen füllen.\n",
    "Auch brauchen wir nicht die tatsächlichen Zahlen, sondern nur die Namen der Inputvariablen nennen,  also nicht 784, sondern `input_size`.\n",
    "Die tatsächlichen Werte können dann bei der eigentlichen Benutzung der Funktion definiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur intialisierung der Weights\n",
    "def init_weights(input_size, hidden_size, output_size):\n",
    "    # Zwei leere Listen mit Länge zwei\n",
    "    b = [0] * 2\n",
    "    W = [0] * 2\n",
    "\n",
    "    # hier werden die Weights W mit kleinen, zufälligen Gewichten initialisiert\n",
    "    \n",
    "    W[0] = np.random.randn(        ,         ) * np.sqrt(2 / input_size)  # HIER MÜSSEN DIE RICHTIGEN GRÖßEN ANGEGBEN WERDEN\n",
    "    W[1] = np.random.randn(        ,         ) * np.sqrt(2 / hidden_size) # HIER MÜSSEN DIE RICHTIGEN GRÖßEN ANGEGBEN WERDEN\n",
    "\n",
    "    # der bias kann 0 sein\n",
    "    b[0] = np.zeros(       ) # HIER MÜSSEN DIE RICHTIGEN GRÖßEN ANGEGBEN WERDEN\n",
    "    b[1] = np.zeros(       ) # HIER MÜSSEN DIE RICHTIGEN GRÖßEN ANGEGBEN WERDEN\n",
    "\n",
    "\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung:</strong></summary>\n",
    "\n",
    "```python\n",
    "def init_weights(input_size, hidden_size, output_size):\n",
    "    # Zwei leere Listen mit Länge zwei\n",
    "    b = [0] * 2\n",
    "    W = [0] * 2\n",
    "\n",
    "    # hier werden die Weights W mit kleinen, zufälligen Gewichten initialisiert\n",
    "    \n",
    "    W[0] = np.random.randn(hidden_size,input_size) * np.sqrt(2 / input_size)  \n",
    "    W[1] = np.random.randn(output_size,hidden_size) * np.sqrt(2 / hidden_size) \n",
    "\n",
    "    # der bias kann 0 sein\n",
    "    b[0] = np.zeros(hidden_size) # HIER MÜSSEN DIE RICHTIGEN GRÖßEN ANGEGBEN WERDEN\n",
    "    b[1] = np.zeros(output_size) # HIER MÜSSEN DIE RICHTIGEN GRÖßEN ANGEGBEN WERDEN\n",
    "\n",
    "\n",
    "    return W, b\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Weights können nun initialisiert werden: \n",
    "\n",
    "Die Eingabegröße ist vordefiniert, da jedes Bild 784 Pixel hat. Die Ausgangsgröße ist ebenfalls vorgegeben, da das Netz zwischen 10 verschiedenen Ziffern unterscheiden muss.\n",
    "Der einzige Wert, den Sie selbst festlegen können, ist die Größe der Hidden Layers. Es gibt keine einfache Regel für die Größe, aber wenn sie zu klein ist, kann sie die Genauigkeit des neuronalen Netzes verringern. Wenn sie zu groß ist, trainiert das Netz zu langsam und kann instabil werden. Dies kann sich ebenfalls auf die Genauigkeit auswirken. Versuchen Sie anfangs Werte wie 100 oder 200. \n",
    "\n",
    "Parameter wie die Größe der Hidden Layers oder die Lernrate, die Sie frei wählen können, werden als **Hyperparameter** bezeichnet. Anders als die eigentlichen Weights des Netzes werden diese nicht durch die Backpropagation optimiert. Das muss \"von Hand\" geschehen. Dieser Vorgang wird als Hyperparameter-Optimierung bezeichnet.\n",
    "\n",
    "Mit dieser Funktion können Sie nun die Weights initialisieren. Die Funktion gibt zwei Listen `W` und `b` aus. Die beiden Listen enthalten jeweils zwei Matrizen/Vektoren, einmal für die erste und einmal für die zweite Transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b = init_weights(input_size=784, hidden_size=200,output_size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "W[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn Sie die Weights richtig initialisiert haben, sollte `W[0].shape` `(200,784)` sein.\n",
    "\n",
    "---\n",
    "\n",
    "Sie fragen sich vielleicht: Warum initialisieren wir unsere Weights nicht einfach mit ` W[0] = np.random.randn(input_size,hidden_size)`. Dann bräuchten wir später nicht das `.transpose()` zu verwenden, da die Matrizen bereits das richtige Format haben.\n",
    "\n",
    "In der Tat ist dies möglich, aber diese Art der Initialisierung hat sich als Standard durchgesetzt. Um zukünftige Verwirrung zu vermeiden, folgen wir auch diesem Standard. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Pass** \n",
    "\n",
    "Nachdem die Weights initialisiert wurden, können Sie den Forward Pass des Netwerkes durchführen. Die Bilder werden durch das Netz geschickt, das diese Bilder schließlich klassifiziert.\n",
    "Die Funktion `forward_pass()` macht genau das. Ihr Input ist:\n",
    "\n",
    "* `W`: die Liste der Weightmatrizen\n",
    "* `b`: die Liste der Bias Vektoren\n",
    "* `X`: die Inputmatrix der Bilder \n",
    "\n",
    "\n",
    "\n",
    "Als Erstes wird $Z_1 = xW_1^T+b_1$ berechnet. <br>\n",
    "Dannach $A_1= sigmoid(Z_1)$.\n",
    "\n",
    "<br>\n",
    "So werden die Activations für die Hidden Layers berechnet. Für die finale Klassifizierung müssen die Werte ein zweites Mal transformiert werden:<br>\n",
    "$\\hat{Y} = softmax(A_1W_2^T+b_2)$\n",
    "\n",
    "Die Funktion gibt am Ende drei Variablen aus:\n",
    "* `Z_1`: die Werte der ersten linearen Transformation\n",
    "* `A_1`: die Werte nach der ersten Aktivierungsfunktion\n",
    "* `Y_hat`: die Werte der Outputlayer \n",
    "\n",
    "\n",
    "*Denken Sie daran, dass die Weights der ersten Layer nicht in* `W[1]`*, sondern in* `W[0]` *gespeichert sind, da die Indexierung bei Null und nicht bei Eins anfängt.* \n",
    "\n",
    "*Denken Sie auch daran, dass wir zur Matrixmultiplikation* `np.matmul` *und nicht mehr* `np.dot` *benutzen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(W, b, X):\n",
    "    \n",
    "    Z_1 = # CODE UM Z_1 AUZURECHNEN\n",
    "    A_1 = # CODE UM A_1 AUSZURECHNEN\n",
    "    Z_2 = # CODE UM Z_2 AUSZURECHNEN\n",
    "    Y_hat = #CODE UM Y_HAT AUSZURECHNEN\n",
    "    return Z_1, A_1, Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung:</strong></summary>\n",
    "\n",
    "```python\n",
    "def forward_pass(W, b, X):\n",
    "    \n",
    "    Z_1 = np.matmul(X,W[0].transpose())+b[0] # CODE UM Z_1 AUZURECHNEN\n",
    "    A_1 = sigmoid(Z_1) # CODE UM A_1 AUSZURECHNEN\n",
    "    Z_2 = np.matmul(A_1,W[1].transpose())+b[1] # CODE UM Z_2 AUSZURECHNEN\n",
    "    Y_hat = softmax(Z_2) #CODE UM Y_HAT AUSZURECHNEN\n",
    "    return Z_1, A_1, Y_hat\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lossfunktion\n",
    "\n",
    "Nach dem Forward Pass wird der Loss berechnet. Dieser misst, wie gut oder schlecht das Modell die Zahlen in den Bilder erkennen konnte.\n",
    "\n",
    "Hierfür werden nur die Werte der Ausgabeschicht (`y_hat`) und die wahren Werte (`y`) verwendet. Die wahren Werte wurden zuvor mit der Funktion `one_hot` erstellt und sind in der Variable `train_targets` gespeichert. \n",
    "\n",
    "Sie haben bereits den Binary Crossentropy Loss kennengelernt, welcher für die binäre Klassifizierung verwendet wird. Diese Lossfunktion ist nur eine spezielle Version des Crossentropy Loss, welcher für die  Klassifizierung von mehr als zwei Klassen verwendet werden kann. Da wir nun versuchen, zehn verschiedene Zahlen zu unterscheiden, werden wir mit dem allgemeineren Crossentropy Loss weiterarbeiten. \n",
    "\n",
    "Die genaue Lossfunktion ist hier gebeben: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(y_hat, y):\n",
    "    return -np.sum(np.log(y_hat) * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können nun die ersten drei Funktionen zusammen verwenden, um Ihre erste Klassifizierung durchzuführen.\n",
    "Denken Sie daran, dass die `train_images` der Input für das Netzwerk sind. Wir wollen, dass die versteckte Schicht 300 Knoten hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234) # Ein Seed wird gesetzt, damit die Ergebnisse der zufälligen Initialisierung für alle Teilnehmer:innen gleich sind\n",
    "W, b = # CODE, UM DIE WEIGHTS ZU INITIALISIEREN\n",
    "\n",
    "# Benutzen Sie die train_images als X (Input)\n",
    "Z_1, A_1, Y_hat = # CODE FÜR DEN FORWARD PASS\n",
    "\n",
    "# Hier berechnen Sie den Loss\n",
    "calc_loss(           ,           )/Y_hat.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung:</strong></summary>\n",
    "\n",
    "```python\n",
    "np.random.seed(1234)\n",
    "W, b = init_weights(784,300,10)\n",
    "\n",
    "# Benutzen Sie die train_images als X (Input)\n",
    "Z_1, A_1, Y_hat = forward_pass(W,b,train_images)\n",
    "    \n",
    "# Hier berechnen Sie den Loss\n",
    "calc_loss(Y_hat,train_targets)/Y_hat.shape[0]\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Wir berechnen zusätzlich die Accuracy, um einen genaueren Überblick darüber zu erhalten, wie gut unser Modell funktioniert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true_labels,predicted):\n",
    "    pred_labels = np.argmax(predicted, axis=1) # argmax gibt den Index des Maximalwertes wieder\n",
    "    correct_predicted = np.sum(true_labels == pred_labels)\n",
    "    return correct_predicted /true_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(train_labels,Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derzeit hat das Netzwerk eine Accuracy von 10,3 %. Eine Accuracy von 10 % ist zu erwarten, wenn das Netz nach dem Zufallsprinzip entscheidet.\n",
    "Um eine bessere Genauigkeit über 10 % zu erreichen, müssen Sie das Netz trainieren bzw. die Weights verändern. Verwenden Sie dazu die Backpropagation.\n",
    "\n",
    "**Backpropagation**\n",
    "\n",
    "Die Fehler in der Klassifizierung werden nun durch das Netz zurückgeführt, und anhand der Gradienten werden die weights angepasst.\n",
    "\n",
    "* $dZ_2 = \\hat{y} - y$ \n",
    "* $dW_2 = \\frac{1}{n} \\cdot dZ_2^Ta_1$\n",
    "* $db_2 = \\frac{1}{n} \\cdot \\sum_{i=1}^n dZ_2$\n",
    "* $dZ_1 = dZ_2W_2 \\cdot a_1 \\cdot (1-a_1)$\n",
    "* $dW_1 = \\frac{1}{n} \\cdot dZ_1^TX$\n",
    "* $db_1 = \\frac{1}{n} \\cdot\\sum_{i=1}^n dZ_1$\n",
    "\n",
    "Wie die Ableitungen mathematisch zustande kommen, liegt außerhalb des Rahmens dieser Übungen. Für die konkrete Anwendung in späteren Übungen gibt es Libraries, die die Gradienten automatisch berechnen können. Im Moment berechnen wir die Gradienten jedoch noch selbst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(X, Z_1, A_1, Y_hat, y):\n",
    "    n = X.shape[0] # n ist die Anzahl der Bilder\n",
    "    \n",
    "    # Gradients für die Weights der zweiten Layer\n",
    "    dZ_2 =  # CODE UM dZ_2 AUZURECHNEN\n",
    "    dW_2 =  # CODE UM dW_2 AUZURECHNEN\n",
    "    db_2 = np.sum(dZ_2, axis=0) / n\n",
    "    \n",
    "    # Gradients für die Weights der ersten Layer\n",
    "    dZ_1 = np.multiply(np.matmul(dZ_2, W[1]), np.multiply(A_1, 1 - A_1))\n",
    "    dW_1 = # CODE UM dW_1 AUZURECHNEN\n",
    "    db_1 = # CODE UM db_1 AUZURECHNEN\n",
    "\n",
    "    return [dW_1, dW_2], [db_1, db_2] # Hier werden wieder zwei Listen als Output gegeben, in jeder der Listen befinden sich jeweils die Gradienten für W_1,W_2 und b_1 und b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung:</strong></summary>\n",
    "\n",
    "```python\n",
    "def back_prop(X, Z_1, A_1, Y_hat, y):\n",
    "    n = X.shape[0] # n ist die Anzahl der Bilder\n",
    "    # Gradients für die Weights der zweiten Layer\n",
    "    dZ_2 = Y_hat - y\n",
    "    dW_2 = np.matmul(dZ_2.transpose(), A_1) / n\n",
    "    db_2 = np.sum(dZ_2, axis=0) / n\n",
    "    \n",
    "    # Gradients für die Weights der ersten Layer\n",
    "    dZ_1 = np.multiply(np.matmul(dZ_2, W[1]), np.multiply(A_1, 1 - A_1))\n",
    "    dW_1 = np.matmul(dZ_1.transpose(), X) / n\n",
    "    db_1 = np.sum(dZ_1, axis=0) / n\n",
    "\n",
    "    return [dW_1, dW_2], [db_1, db_2] \n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights updaten\n",
    "\n",
    "Im letzten Schritt werden die Weights angepasst. Dazu werden die Weights ein wenig entgegen det Gradienten verschoben. \n",
    "Wie weit genau, hängt von der Lernrate `lr` ab. Je größer die Lernrate ist, desto größer sind die Schritte.\n",
    "Wenn die Lernrate zu klein ist, kann das Training zu lange dauern oder das Netz kann in einem lokalen Minimum stecken bleiben. Ist die Lernrate zu groß, springt der Loss des Netzes zu stark, so dass keine optimale Performance garantiert werden kann.\n",
    "Die Lernrate gehört, wie die Größe der Hidden Layer, zu den Hyperparametern.\n",
    "<center>\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_learning_rate.png\" style=\"width: 800px;\">\n",
    "<h8><center>Quelle: Sebastian Raschka, https://sebastianraschka.com</center></h8>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(W, b, grad_W, grad_b, lr=0.0001):\n",
    "    W[0] = # CODE FÜR W[0]\n",
    "    W[1] = # CODE FÜR W[1]\n",
    "    b[0] = # CODE FÜR b[0]\n",
    "    b[1] = # CODE FÜR b[1]\n",
    "\n",
    "    return W, b # die Funktion gibt die neuen Weights und Biases aus (2 Listen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung:</strong></summary>\n",
    "\n",
    "```python\n",
    "def update(W, b, grad_W, grad_b, lr=0.0001):\n",
    "    W[0] = W[0] - lr * grad_W[0]\n",
    "    W[1] = W[1] - lr * grad_W[1]\n",
    "    b[0] = b[0] - lr * grad_b[0]\n",
    "    b[1] = b[1] - lr * grad_b[1]\n",
    "\n",
    "    return W, b # die Funktion gibt die neuen Weights und Biases aus\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Jetzt können Sie alles zusammenfügen. Zuerst initialisieren Sie die Gewichte, dann werden der Input durch das Netz geführt und der Loss wird berechnet.  Dannach wird der Loss zurück durch das Netz geführt und die Gradienten berechnet. Mit Hilfe der Gradienten werden die Weights aktualisiert. Für die Lernrate verwenden Sie den Wert 0,1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.WEIGHTS INITALISIEREN\n",
    "np.random.seed(1234) \n",
    "W, b = init_weights(input_size=784, hidden_size=300,output_size= 10) \n",
    "\n",
    "# 2.FORWARD PROPAGATION\n",
    "Z_1, A_1, Y_hat = forward_pass(W,b,train_images)\n",
    "\n",
    "# 3.LOSS BERECHNEN\n",
    "print(\"Loss beim ersten Durchlauf:\",calc_loss(Y_hat,train_targets)/Y_hat.shape[0], \"\\nGenauigkeit beim ersten Durchlauf:\", accuracy(train_labels, Y_hat) )\n",
    "\n",
    "# 4. BACKPROPAGATION\n",
    "grad_W, grad_b = # CODE FÜR DIE BACKPROPAGATION\n",
    "\n",
    "# 5. WEIGHTS UPDATEN\n",
    "W, b = # WEIGHT UPDATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung:</strong></summary>\n",
    "\n",
    "```python\n",
    "# 1.WEIGHTS INITALISIEREN\n",
    "np.random.seed(1234) \n",
    "W, b = init_weights(input_size=784, hidden_size=300,output_size= 10) \n",
    "\n",
    "# 2.FORWARD PROPAGATION\n",
    "Z_1, A_1, Y_hat = forward_pass(W,b,train_images)\n",
    "\n",
    "# 3.LOSS BERECHNEN\n",
    "print(\"Loss beim ersten Durchlauf: \",calc_loss(Y_hat,train_targets)/Y_hat.shape[0], \"\\nGenauigkeit beim ersten Durchlauf:\", accuracy(train_labels, Y_hat) )\n",
    "\n",
    "# 4. BACKPROPAGATION\n",
    "grad_W, grad_b = back_prop(train_images, Z_1, A_1, Y_hat, train_targets)\n",
    "\n",
    "# 5. WEIGHTS UPDATEN\n",
    "W, b = update(W, b, grad_W, grad_b, lr = 0.1)\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Loss und die Accuracy sind noch unverändert. Erst wenn Sie erneut den Input durch das Netzwerk schicken, können Sie den Effekt der aktualisierten Weights sehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_1, A_1, Y_hat = forward_pass(W,b,train_images)\n",
    "\n",
    "# jetzt berechnen Sie den Loss\n",
    "print(\"Loss beim zweiten Durchlauf:\",calc_loss(Y_hat,train_targets)/Y_hat.shape[0], \"\\nGenauigkeit beim zweiten Durchlauf:\", accuracy(train_labels, Y_hat) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Tat nimmt der Loss ab und die Accuracy verbessert sich. Allerdings kann es auch während des Trainings zu einer kurzfristigen Verschlechterung kommen, was aber nicht weiter schlimm ist.  Der Effekt des Trainings wird oft erst nach mehreren Epochen sichtbar. Sie können den Trainingsschritt einfach wiederholen.\n",
    "Um dies effizienter zu tun, schreiben Sie einfach einen `for-loop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.WEIGHTS INITALISIEREN\n",
    "np.random.seed(1234)\n",
    "W, b = init_weights(784, 300, 10)\n",
    "\n",
    "EPOCHS= 50 # wie oft der Datensatz durch das Netzwerk geführt wird\n",
    "for i in range(EPOCHS):\n",
    "    \n",
    "    # 2. FORWARD PROPAGATON\n",
    "    Z_1, A_1, Y_hat = forward_pass(W, b,train_images)\n",
    "    \n",
    "    # 3. LOSS BERECHNEN\n",
    "    loss = calc_loss(Y_hat, train_targets) / Y_hat.shape[0]\n",
    "    acc = accuracy(train_labels, Y_hat)\n",
    "    \n",
    "    print(i, \n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss, acc)\n",
    "    )\n",
    "    \n",
    "    # 4. BACK PROPAGATION\n",
    "    grad_W, grad_b = back_prop(train_images, Z_1, A_1, Y_hat, train_targets)\n",
    "    \n",
    "    # 5. WEIGHTS UPDATEN\n",
    "    W, b = update(W, b, grad_W, grad_b, lr = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können bereits eine Verbesserung feststellen und erreichen eine Accuracy von 73 %. Allerdings dauert das Training sehr lange. Mit einer höheren Lernrate sollte das Training schneller vonstatten gehen. Versuchen Sie eine Lernrate von 0,3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "W, b = init_weights(784, 300, 10)\n",
    "loss= []\n",
    "EPOCHS= 50 # wie oft der Datensatz durch das Netzwerk geführt wird\n",
    "for i in range(EPOCHS):\n",
    "    Z_1, A_1, Y_hat = forward_pass(W, b,train_images)\n",
    "    \n",
    "    loss= calc_loss(Y_hat, train_targets) / Y_hat.shape[0]\n",
    "    \n",
    "    acc = accuracy(train_labels, Y_hat)\n",
    "    \n",
    "    print(i,\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss, acc)\n",
    "    )\n",
    "    \n",
    "    # BACK PROPAGATION\n",
    "    grad_W, grad_b = back_prop(train_images, Z_1, A_1, Y_hat, train_targets)\n",
    "    # WEIGHTS UPDATEN\n",
    "    W, b = update(W, b, grad_W, grad_b, lr = 0.3 )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei einer Lernrate von 0,3 erreichen Sie nach 50 Epochen eine Genauigkeit von 78 %. Sie können das Netz noch weiter verbessern, indem Sie noch länger trainieren. \n",
    "\n",
    "Was müssten Sie am Code ändern, um für 25 weitere Epochen zu trainieren **ohne das Netz erneut von Beginn trainieren zu müssen**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "W, b = init_weights(784, 300, 10)\n",
    "loss= []\n",
    "EPOCHS= 25 # wie oft der Datensatz durch das Netzwerk geführt wird\n",
    "for i in range(EPOCHS):\n",
    "    Z_1, A_1, Y_hat = forward_pass(W, b,train_images)\n",
    "    loss= calc_loss(Y_hat, train_targets) / Y_hat.shape[0]\n",
    "    acc = accuracy(train_labels, Y_hat)\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss, acc)\n",
    "    )\n",
    "    \n",
    "    # BACK PROPAGATION\n",
    "    grad_W, grad_b = back_prop(train_images, Z_1, A_1, Y_hat, train_targets)\n",
    "    # WEIGHTS UPDATEN\n",
    "    W, b = update(W, b, grad_W, grad_b, lr = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung:</strong></summary>\n",
    "\n",
    "```python\n",
    "np.random.seed(1234)\n",
    "# W, b = init_weights(784, 300, 10) # Sie dürfen nicht erneut die Gewichte initialisieren\n",
    "loss= []\n",
    "EPOCHS= 25 # wie oft der Datensatz durch das Netzwerk geführt wird\n",
    "for i in range(EPOCHS):\n",
    "    Z_1, A_1, Y_hat = forward_pass(W, b,train_images)\n",
    "    loss= calc_loss(Y_hat, train_targets) / Y_hat.shape[0]\n",
    "    acc = accuracy(train_labels, Y_hat)\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss, acc)\n",
    "    )\n",
    "    \n",
    "    # BACK PROPAGATION\n",
    "    grad_W, grad_b = back_prop(train_images, Z_1, A_1, Y_hat, train_targets)\n",
    "    # WEIGHTS UPDATEN\n",
    "    W, b = update(W, b, grad_W, grad_b, lr = 0.3)\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie dürfen die Weights einfach nicht erneut initialisieren. Sonst wird alles verloren, was das Netz bereits gelernt hat.\n",
    "\n",
    "Durch weiteres Training haben Sie das Netz um 6% Accuracy verbessern können. Grundsätzlich haben Sie noch die Möglichkeit, etwas an der Größe der Hidden Layers zu ändern. \n",
    "\n",
    "Bevor wir das tun, können Sie zunächst sehen, wie gut unser Modell bei Bildern funktioniert, die es noch nicht gesehen hat. Wir sprechen hier von dem Testdatensatz. Zu diesem Zweck wird der Testdatensatz in das trainierte Netzwerk eingespeist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, test_y_hat = forward_pass(W, b, test_images) # durch das _,_,test_y_hat werden z_1 und a_1 nicht mit ausgegeben, da wir diese nicht brauchen\n",
    "accuracy(test_labels, test_y_hat)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Accuracy für den Testdatensatz beträgt ebenfalls 85 %. Oder: 85 % der Bilder wurden richtig erkannt.\n",
    "Es ist ungewöhnlich, dass Netzwerke auf dem Testdatensatz besser oder gleich gut abschneiden. Dies ist ein Hinweis darauf, dass Sie das Modell nicht lange genug trainiert haben.\n",
    "\n",
    "Als Nächstes können Sie sich ansehen, mit welchen Bildern das Netz die meisten Probleme hat.\n",
    "Der Code in der nächsten Zelle sortiert die falsch erkannten Bilder nach ihrer Wahrscheinlichkeit (dieser Code ist nicht unbedingt einfach zu verstehen, aber für das Verständnis neuronaler Netze auch nicht unbedingt notwendig):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falsche_klassifizierung = np.where(test_labels != np.argmax(test_y_hat, axis=1))[0]# welche Bilder wurde falsch klassifiziert\n",
    "len(falsche_klassifizierung) # soviele Bilder wurden falsch klassifiziert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier werden de Wahrscheinlichkeiten gesammelt die das Modell dem Bild zugeorndet hat in der richtige Kategorie zu sein.\n",
    "probs = [] \n",
    "for image in falsche_klassifizierung:\n",
    "    probs.append(test_y_hat[image,test_labels[image]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir sortieren die Bilder Anhand der Wahrscheinlichkeiten, je kleiner die Wahrscheinlichkeit \n",
    "# desto sicherer war das Model das das Bild nicht in der richtige Kategorie ist.\n",
    "falsche_klassifizierung=falsche_klassifizierung[np.argsort(probs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so sehen 10 Bilder aus die falsch klassifiziert werden\n",
    "for i in range(10):\n",
    "    plt.imshow(test_images[falsche_klassifizierung[i]].reshape([28, 28]), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    print(\n",
    "        \"Predicted Label: %s, Correct Label %s\"\n",
    "        % (\n",
    "            np.argmax(test_y_hat, axis=1)[falsche_klassifizierung[i]],\n",
    "            test_labels[falsche_klassifizierung[i]],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei einigen Bildern kann man deutlich erkennen, warum sie falsch kategorisiert wurden. Bei anderen hingegen ist es für das menschliche Auge recht einfach, die richtige Ziffer zu erkennen, aber das Netzwerk hat Probleme damit.\n",
    "\n",
    "# Übungsaufgabe\n",
    "\n",
    "Schreiben Sie den obrigen Code so um, dass sowohl Test Accuracy und Test Loss nach jedem Epoch berechnet und ausgegeben werden. Dies soll zusätzlich zu der Trainings Accuracy/Loss passieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "W, b = init_weights(784, 300, 10)\n",
    "loss= []\n",
    "EPOCHS= 25 # wie oft der Datensatz durch das Netzwerk geführt wird\n",
    "for i in range(EPOCHS):\n",
    "    z_1, a_1, y_hat = forward_pass(W, b,train_images)\n",
    "    loss= calc_loss(y_hat, train_targets) / y_hat.shape[0]\n",
    "    acc = accuracy(train_labels, y_hat)\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss, acc)\n",
    "    )\n",
    "    \n",
    "    # wir machen die backpropagation\n",
    "    grad_W, grad_b = back_prop(train_images, z_1, a_1, y_hat, train_targets)\n",
    "    # und mit den Gradienten updaten wir die Weights\n",
    "    W, b = update(W, b, grad_W, grad_b, lr = 0.3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2063f454e5583264956edca724ed174a35400d49c5baf96fcf9ea99fcd5830b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
