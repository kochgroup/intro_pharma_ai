{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15ea51ee",
   "metadata": {},
   "source": [
    "# Introduction to Statistics\n",
    "\n",
    "Today we are going to look at some basics of statistics.\n",
    "Statistics can help us to describe and explain data in a simple way. \n",
    "\n",
    "---\n",
    "\n",
    "### In this lesson you'll learn:\n",
    "* how to calculate the mean, variance, and standard deviation in Python.\n",
    "* the difference between a regression and a classification.\n",
    "* how a linear regression functions and the meaning of its coefficients.\n",
    "* about the *Mean Squared Error* and the loss function.\n",
    "* what a logistic regression is and how it relates to linear regressions.\n",
    "* what the Binary Cross Entropy Loss is.\n",
    "* about different metrics such as accuracy and the ROC-AUC.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638fc527",
   "metadata": {},
   "source": [
    "For example, we can look at student grades (german: Noten) from a highschool class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = [1.64, 2.35, 1.88, 2.48, 2.16, 3.92, 2.16, 2. , 1.76, 2.82, 1.81,\n",
    "          2.59, 3.03, 1.7 , 2.87, 3.21, 2.65, 1.97, 1.2, 1.67, 1.77, 1.98,\n",
    "          3.4 , 1.31, 1.72, 2.05, 1.12, 1.56, 2.01, 2.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d2ef96",
   "metadata": {},
   "source": [
    "However, it is very difficult to get an overview using only the data.\n",
    "It is easier to plot the grades.\n",
    "\n",
    "\n",
    "<img src='Img/intro_stats/noten_1.png'></img>\n",
    "\n",
    "Although you now have a better overview, it can be difficult to compare two classes.\n",
    "\n",
    "<img src='Img/intro_stats/noten2.1.png'></img>\n",
    "\n",
    "We can use *density plots* to show the distribution easily. Here the y-axis is used to represent the density. That is, the higher the curve is at a point, the more data points are at that point.\n",
    "\n",
    "<img src='Img/intro_stats/noten_3.1.png'></img>\n",
    "\n",
    "Often, a purely visual view is not sufficient to make clear decisions.\n",
    "Metrics that describe the distribution of the data points (in this example the grades) are needed for this.\n",
    "\n",
    "The best known is probably the mean value, or more precisely the arithmetic mean. It describes the average of a distribution of data points. \n",
    "And to calculate the arithmetic mean, the sum of all values is divided by the number of values.\n",
    "\n",
    "\n",
    "$$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$$\n",
    "\n",
    "The mean is often denoted by $\\bar{x}$.\n",
    "Calculate the arithmetic mean in Python for the highschool class. *Without using Numpy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d65389e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_grades = _____________# formula for the mean\n",
    "mean_grades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51184c89",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python \n",
    "mean_grades = sum(grades)/len(grades)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7298d92b",
   "metadata": {},
   "source": [
    "However, the mean is not sufficient to adequately describe a distribution of values. The two [normal distributions](https://www.statista.com/statistics-glossary/definition/346/normal_distribution/) have the same mean in the example and yet are not identically distributed. \n",
    "<img src='Img/intro_stats/noten_3.png'></img>\n",
    "\n",
    "We can see that the red distribution is much narrower than the black one. That is, the values of the red group are closer to their mean than those of the black group.\n",
    "\n",
    "The width of a distribution is measured by the variance. The variance measures the average distance of the values from their mean. \n",
    "The variance ($s^2$) is calculated as follows:\n",
    "\n",
    "$$s^2 = \\frac{1}{n}\\sum_{i=1}^n(x_i-\\bar{x})^2$$\n",
    "\n",
    "Note that it is not the difference ($x_i-\\bar{x}$) that is summed, but the square ($x_i -\\bar{x})^2$ of the difference. Thus, larger distances have a larger effect on the variance. \n",
    "\n",
    "Calculate the variance of the `grades`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fed6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "squares = 0\n",
    "for x in grades:\n",
    "    squares = squares +((_____-______)**___)\n",
    "variance_grades = squares/len(______) \n",
    "variance_grades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13096a79",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "\n",
    "    \n",
    "```python\n",
    "for x in grades:\n",
    "    squares = squares +((x-mean_grades)**2)\n",
    "variance_grades = squares/len(grades)     \n",
    "```\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1e1560",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary><b>Solution using list comprehension:</b></summary>\n",
    "\n",
    "```python\n",
    "sum([(x - mean_grades)**2 for x in grades])/(len(grades))\n",
    "```    \n",
    "</details>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee60bb4",
   "metadata": {},
   "source": [
    "Instead of the variance, the standard deviation is often used as a measure for the *width* of a distribution. The standard deviation is obtained by taking the square root of the variance. This brings the measure of variance to the scale of the original distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b600bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_grades = __________ # Calculate the standard deviation\n",
    "std_grades "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cced209",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "std_grades= variance_grades**(0.5)\n",
    "```\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d260ec",
   "metadata": {},
   "source": [
    "Of course all functions already exist in `numpy`: `np.mean()`, `np.std()`, `np.var()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e3ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(\"mean: \", np.mean(grades))\n",
    "print(\"variance: \", np.var(grades))\n",
    "print(\"standard deviation: \", np.std(grades))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2736acf0",
   "metadata": {},
   "source": [
    "With the measure of variance/standard deviation and the mean we can already describe some distributions. Of course not all, e.g. with multimodal distributions one would need even more information. \n",
    "\n",
    "<img src='Img/intro_stats/noten_4.png'></img>\n",
    "\n",
    "## Inferential Statistics \n",
    "\n",
    "However, we do not always want to describe data, but also to obtain information from these data. For example, we can use correlation to describe the relationship between height (german: Größe) and weight (german: Gewicht). The taller a person is, the heavier he is. This model is not perfect, of course; body weight does not depend only on height. There are tall lightweight people and short heavy ones. But there exists a basic tendency.  \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src='Img/intro_stats/reg_1.png' alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "<td> <img src='Img/intro_stats/reg_2.png' alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**We can describe the relationship with a linear regression.**\n",
    "You may still know the linear equation $y = mx+t$ (or $y = ax+b$) from school. \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "- $x$ is the input variable, in our case the body height\n",
    "- $y$ is the variable to be predicted (body weight)\n",
    "- $m$ describes the slope of the straight line\n",
    "- $t$ denotes the y-axis intercept, the value of $y$ at $x=0$\n",
    "\n",
    "<img src='Img/intro_stats/reg_3.png' alt=\"Drawing\" width=\"500\"/>\n",
    "\n",
    "Assuming that the equation of the regression line is $y=0.3x+21$, then for the \n",
    "example, the weight of a person with a height of 180 cm would be 75 kg \n",
    "($0.3\\cdot180+21)$.The value for $m$ ($0.3$) indicates how much $y$ increases when $x$ increases by 1.\n",
    "So, according to the model, a person's body weight increases by 0.3 kg when height increases by 1 cm. \n",
    "\n",
    "The value for $t$ indicates how much a person weighs who is 0 cm tall ($x=0$). In the case of height, it makes little sense to interpret the value for $t$. However, suppose we estimate the value of a house based on the size of the terrace. The value for $t$ gives the value of a house when the size of the terrace is $0$. Thus, the value of a house without a terrace is $t$.\n",
    "\n",
    "Back to the previous example: \n",
    "\n",
    "Of course, not every 180 cm tall person weighs 66 kg. This is only the predicted value \n",
    "of our regression equation. To make this clear, we write $\\hat{y}$ instead of $y$.\n",
    "This makes the straight line equation $\\hat{y}=mx+t$.\n",
    "\n",
    "---\n",
    "\n",
    "Write a function that calculates the weight using the straight line equation described above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aaff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg(x,m,t):\n",
    "    _________# What is this function supposed to return?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b2bf06",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "def reg(x,m,t):\n",
    "    return m*x+t\n",
    "```\n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8686128",
   "metadata": {},
   "source": [
    "The variable `x` contains the height in cm of 5 people. Calculate the weight for these five people using the function `reg`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3bc88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [182,167,198,132,178]\n",
    "y_hat = [reg(__,__,__) for ___ in _____ ]\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c149f7e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "y_hat = [reg(weight,0.3,21) for weight in x ]\n",
    "```\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159fe697",
   "metadata": {},
   "source": [
    "As already mentioned, the values are only an estimate of the weight and differ from the actual weight of the person. To assess how well our model can determine the weight, we also need the actual measured weight of the persons. These are given in `y`. For example, we can calculate the difference between `y_hat` and `y`. But for this we first have to convert the lists into `numpy` arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8992c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([78.2,68.3, 81.0,64.3, 70.1 ])\n",
    "y_hat = np.array(y_hat)\n",
    "residual = y - ___ # What do we substract from y?\n",
    "residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ba41c",
   "metadata": {},
   "source": [
    "This difference between the actual and the predicted value ($y - \\hat{y}$) is also called the residual. The symbol for the residual is usually the small epsilon ($\\epsilon$), which is used to measure the magnitude of the error (**E**rror) of the prediction. \n",
    "\n",
    "<img src='Img/intro_stats/reg_4.png' alt=\"Drawing\" width=\"500\"/>\n",
    "\n",
    "For example, to estimate how good a model is overall, we could simply sum the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b38c1",
   "metadata": {},
   "source": [
    "As you can see, the value is very close to zero, a very small error. The problem, however, is that the residuals can be both positive and negative. That is, when you add them together, they cancel each other out. You will always get values close to zero. To avoid this, we do not sum the residuals, but, as with the variance, we sum the squares of the residuals. $$\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2$$ \n",
    "\n",
    "However, the sum alone would lead to models with more data points, i.e., with a larger $n$, automatically having larger error sums. Therefore, we take the mean of the squares instead of the sum: $\\frac{1}{n}\\sum_{i=1}^{n}(y_-\\hat{y}_i)^2$. This value, called the *Mean Squared Error* (MSE), is useful to assess the quality of the predictions. If a model has a small MSE, one can conclude that the residuals must be small, i.e., the differences between the predicted and true values are small. \n",
    "\n",
    "As with the variance and standard deviation, there is also the root mean squared error (RMSE). As you can guess, this is simply taken by taking the square root of the MSE. Write a function that can calculate the RMSE. You can use `numpy`, i.e. you do not need a `for-loop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f5933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y,y_hat):\n",
    "   MSE = np.sum(__________________) /len(_____) # calculate the MSE here \n",
    "   return ___________ # convert the MSE to the RMSE\n",
    "RMSE(y, y_hat)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dc5e49",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "def RMSE(y,y_hat):\n",
    "   MSE = np.sum((y-y_hat)**2)/len(y)\n",
    "   return np.sqrt(MSE) \n",
    "```\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f85247a",
   "metadata": {},
   "source": [
    "In machine learning or in the field of optimization in general, functions like the RMSE are also called loss functions. They measure how well a model fits the data. The loss calculated by these functions must be minimized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9d1020",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Up to now you have always been given the parameters `m` and `t`. In reality, you have to calculate them yourself. In the following example we deal with the prediction of the boiling point. For this we use a data set from the American *National Institute of Standards and Technology*. In the data set, the boiling temperatures for 72 simple alcohols are recorded. In addition, the molecular weight and the number of carbons are given. \n",
    "The data set is located in the folder `../data/boilingpoints/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5950f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://uni-muenster.sciebo.de/s/LJdQyXIQ2geYdyE/download\").values\n",
    "print(\"Größe der Daten: \",data.shape)\n",
    "data[:10,:] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82929aaf",
   "metadata": {},
   "source": [
    "The data set consists of 72 rows and three columns. Each row represents an alcohol and the three columns contain information for one of the three descriptors. The first column contains the melting points, the second the molecular weight and the third column the number of carbons. \n",
    "\n",
    "Our goal is to predict the melting point based on the molecular weight.\n",
    "First, we store the first column (melting points) in the variable `y` and the second column in the variable `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ea9607",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[:,0] # y the variable we want to predict (boilingpoints)\n",
    "x = data[:,1:2] # we could also use data[:,1], but behaves differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf928bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[:5,1])\n",
    "print(data[:5,1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db39e7be",
   "metadata": {},
   "source": [
    "You can see that we select the same values in the example, but in the first variant we reduce the column to a 1-dimensional array of size `(72)`. So a vector of length 72. Some of the functions necessary for linear regression expect our variable `x` to be in the form of a 2-dimensional array. Therefore we select the column with `data[:,1:2]`. Thus we keep the 2D structure of the `array`.\n",
    "\n",
    "We can also plot the data using the library `matplotlib`. With the function `plt.plot()` you can quickly create simple plots. Here you just have to specify what values belong on the x-axis (first position in the function), then specify what belongs on the y-axis (second position). Finally, you can specify whether the individual values should be plotted as a point `\"o\"` or connected with a line `\"-\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c9a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(x, y, \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b11bc2",
   "metadata": {},
   "source": [
    "It can be clearly seen that as the weight increases, the boiling point of the alcohols also increases. \n",
    "\n",
    "In the next cell, we calculate the linear regression parameters that fit the data. \n",
    "For this we need the Python library `sklearn`, which provides many functions for statistical analysis and machine learning.\n",
    "\n",
    "Regardless of which `sklearn` model you want to use, the general structure remains the same. \n",
    "First, the type of the model must be defined.\n",
    "Using `model = LinearRegression()` tells Python to create a linear regression model.\n",
    "\n",
    "Next, the model must be *fitted* to the data `(x,y)`. This is done with the `model.fit(x,y)` statement. This step leads to the calculation of the regression parameters.\n",
    "\n",
    "We get the estimated parameters via `model.coef_[0]` for the slope (`m`) and `model.intercept_` for the y-axis intercept (`t`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e80719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(x,y) # calculates the linear regression\n",
    "m = model.coef_[0] # we can get m and t from the model\n",
    "t = model.intercept_\n",
    "\n",
    "print(m,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c492f",
   "metadata": {},
   "source": [
    "Calculate `y_hat` with the parameters and then the RMSE. Since we are now using `np.arrays`, no `for-loop` is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241c543",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_hat = reg(data[:,1], ___ , ____)\n",
    "RMSE(y, ____) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23515937",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "y_hat = reg(data[:,1], m , t)\n",
    "RMSE(y, y_hat) \n",
    "```\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e471b",
   "metadata": {},
   "source": [
    "Can you find other values for \"m\" and \"t\" that result in a lower RMSE?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8fd581",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = reg(data[:,1], ____  ,  _____  )\n",
    "RMSE(y, y_hat) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e223d0d",
   "metadata": {},
   "source": [
    "In fact, this does not work. When we speak of a linear regression, we usually mean an *ordinary least-square* regression. As the name implies, this regression minimizes squares, the error of the regression line. That is, the regression line is the optimal line that can be found for that data set. In other words, an OLS regression line minimizes the (R)MSE.\n",
    "\n",
    "## Multiple Regression\n",
    "\n",
    "Linear regression can also be performed with more than one $x$ variable. The formula expands to:\n",
    "\n",
    "$$\\hat{y}= \\beta_0 +\\beta_1x_1 +\\beta_2x_2$$\n",
    "\n",
    "In general, the notation with $\\beta$ is more common. Here $\\beta_0$ stands for the $t$ and $\\beta_1$ for the regression coefficient belonging to the first input $x_1$.\n",
    "\n",
    "However, the interpretation of these coefficients does not change.\n",
    "\n",
    "We can use both the number of carbons and the weight to predict the melting points.\n",
    "\n",
    "For this to work, you must first select not only the second but also the third column of `data` in `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224f758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[:,1: ___ ] # Which columns do we need for x?\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a747595",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "x = data[:,1:3]\n",
    "```\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76db29",
   "metadata": {},
   "source": [
    "You can now have the regression coefficients estimated again with `LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be555ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = LinearRegression()\n",
    "model_2.fit(x,y) # calculates the linear regression\n",
    "print(model_2.coef_, model_2.intercept_ ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d0fc1",
   "metadata": {},
   "source": [
    "As you can see, you now get a total of 3 parameters. The regression coefficient for the molecular weight is `-4.65` and for the number of carbon `83.18`. `sklearn` also has a function `predict()`. With it we can automatically make predictions with the previously estimated parameters. In the following example, we used this function to calculate `y_hat` for the `x` values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa42cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model_2.predict(x)\n",
    "RMSE(y, y_hat) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a40d10",
   "metadata": {},
   "source": [
    "By using another variable in the regression, we were able to almost halve the loss (RMSE). This means that the model with two input variables leads to significantly better predictions than the first model with only one input variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff633a3",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "There are also problems where exact values are not to be predicted. For example, we want to decide whether a patient needs to be admitted to the intensive care unit or not. Here we only have to decide between `YES` and `NO`. Mathematically, however, we would speak of `1` or `0`. When a data point can belong to one of two groups, we speak of a **binary classification**. \n",
    "\n",
    "Here we have an example of a basketball player who throws at the hoop from different distances. \n",
    "If he scores, this throw is rated as a `1`. If he does not, the throw is rated with a `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9451c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "throws = np.array([1,1,1,1,1,1,0,1,0,1,1,0,0,1,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0])    \n",
    "distance = np.array([0.,1.,2.,3.,4.,5.,6.,7.,8.,9.,10.,11.,12.,13.,14.,\n",
    "                    15.,16.,17.,18.,19.,20.,21.,22.,23.,24.,25.,26.,27.,28.,29.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1ce03e",
   "metadata": {},
   "source": [
    "It is possible to calculate a simple regression line, but it does not fit the data very well because of the binary variable $y$. One solution is logistic regression. Here, a sigmoid function \"after\" linear regression is used to transform the predicted values. \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src='Img/intro_stats/log1.png' alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "<td> <img src='Img/intro_stats/log2.png' alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "<td> <img src='Img/intro_stats/log3.png' alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "</tr></table>\n",
    "<br>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "<h2>Sigmoid Function</h2>\n",
    "</center>\n",
    "\n",
    "The sigmoid function is a non-linear function. Mathematically, the sigmoid function is written like this:\n",
    "$$sigmoid(z)= \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "To understand what it does exactly, you can take a look at the example.\n",
    "\n",
    "<td> <center><img src='Img/intro_stats/sigmoid.png' alt=\"Drawing\" style=\"width: 250px;\"/> </center>\n",
    "<h8><center>x-axis: before applying the sigmoid function<br>y-axis: after applying the sigmoid function</center></h8>\n",
    "\n",
    "On the x-axis are values between -6 and 6, **before** the sigmoid function is applied to these values. On the y-axis are the same values, but this time after applying the sigmoid function. \n",
    "All values are now between 0 and 1. Values that were very far from 0 before are now very close to `0` or `1`.\n",
    "    \n",
    "The shape of this function fits much better to a binary classification.\n",
    "\n",
    "To perform a logistic regression, we can build on what we have already learned.\n",
    "We have the same situation, we want to make a prediction for `y` based on our inputs `x`.     \n",
    "\n",
    "To do this, we simply substitute the values from the linear regression into the sigmoid function.\n",
    "$$ z = mx+t $$\n",
    "$$\\hat{y} = sigmoid(z) = \\frac{1}{1+e^{-z}} = \\frac{1}{1+e^{-(mx+t)}} $$    \n",
    "\n",
    "Now calculate `z` by applying `reg` to the `distance` values. Since you can now use `numpy`, you no longer need a `for-loop`.\n",
    "For the example with the basketball player, the following parameters are given:\n",
    "- `m` = -0.8\n",
    "- `t` = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df6f1af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = reg(____)  # it is no longer convention to call our input variable x\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5082d10a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "\n",
    "```python\n",
    "z = reg(distance,-0.8,7) \n",
    "```\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb00ca8",
   "metadata": {},
   "source": [
    "Next you need the sigmoid function. For this write a function in Python with `numpy`. $e^x$ can be written as `np.exp(x)` with `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb3820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(value):\n",
    "    return 1/(___________) # wrie the denominator of the sigmoid function here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7445a1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "def sigmoid(value):\n",
    "    return 1/(1+np.exp(-value))\n",
    "```\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beefb2fd",
   "metadata": {},
   "source": [
    "In the last step, calculate `y_hat` using `z` and the `sigmoid` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60726e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = sigmoid(_____)# Which input do you need for the sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae02cd44",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "y_hat = sigmoid(z)\n",
    "```\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14c8c82",
   "metadata": {},
   "source": [
    "As you can see, all values are now between `0` and `1`. Actually, we wanted values that are exacly `0` or `1`, not values in between. But the values of `y_hat` can be understood as a kind of probability. A predicted value of `0.99908895` means that, according to the model, the basketball player will score a basket 0.99% of the time. Conversely, a value of `0.00135852 means that, according to the model, there is only a 0.14% chance of scoring a basket.\n",
    "\n",
    "The following figure shows the predicted values together with the predicted images. \n",
    "\n",
    "<img src='Img/intro_stats/log4.png' alt=\"Zeichnung\" width=\"500px\"/> \n",
    "\n",
    "Normally, the probabilities are interpreted in such a way that the model predicts a `1`, i.e. a hit, from a value `>=0.5` and a `0` (miss) below.\n",
    "\n",
    "Thus, we can judge the accuracy of the model by the percentage of correctly classified throws. \n",
    "First, we round `y_hat`. This gives us only `0` and `1` as predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d5813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.round(y_hat)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072c7e8",
   "metadata": {},
   "source": [
    "You can now compare whether `pred` matches the original `y` variable `throws`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102bb2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred==throws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c0f5b3",
   "metadata": {},
   "source": [
    "Write a function to calculate the accuracy (percentage of correctly classified throws). Remember that `booleans`, i.e., `True` and `False`, can also be written as `1` or `0` in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847a215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true==___) / len(____) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b269b",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true==y_pred)/len(y_true)\n",
    "```\n",
    "</details> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259d99d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(throws, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd20f55",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy Loss\n",
    "\n",
    "An accuracy of 0.73 means that the model predicts the correct result 73% of the time. Similar to the RMSE, this is a metric to estimate how good our model is.\n",
    "\n",
    "Often, however, not just one metric is used. The advantage of accuarcy is that it is very easy to interpret. But some mathematical properties of accuarcy make it unsuitable for certain machine learning methods. Therefore, at least two different metrics are usually used. \n",
    "\n",
    "The additional metric used in classification is the **Cross Entropy** Loss. In the case of a binary classification problem, it is usually referred to as **Binary Cross Entropy** (BCE) Loss. \n",
    "\n",
    "$$Loss =-\\frac{1}{n}\\sum_{i=0}^n[y_i\\cdot log(\\hat{y}_i) + (1-y_i)\\cdot log(1-\\hat{y}_i)]$$\n",
    "\n",
    "The formula looks very complicated at a first glance, but it is relatively easy to understand with the help of examples.\n",
    "Let's assume we want to calculate the loss for only one data point, e.g. for a single shot of the basketball player. Then $n = 1$ and the above formula simplifies:\n",
    "\n",
    "\n",
    "$$Loss =-[y_i\\cdot log(\\hat{y}_i) + (1-y_i)\\cdot log(1-\\hat{y}_i)]$$\n",
    "\n",
    "\n",
    "##### Assuming that the basketball player did not hit the shot, then $y_i=0$.\n",
    "\n",
    "<img src='Img/intro_stats/bce_1.gif' alt=\"Drawing\" width= \"500px\"/> \n",
    "\n",
    "Resulting in:\n",
    "\n",
    "$$\\begin{align}\n",
    "Loss&=-0\\cdot log(\\hat{y}_i) + (1-0)\\cdot log(1-\\hat{y}_i)\\\\\n",
    "&=-log(1-\\hat{y}_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "That is, the loss for this shot is the $log$ of the difference of 1 and $\\hat{y}$ (the predicted probability).\n",
    "\n",
    "You can try out what happens to the loss for different probabilities. Remember that the true value is $y_i=0$. So a good model would predict a low probability, so a small loss is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef070203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put different probabilities into the formula below and see what happens to the loss\n",
    "\n",
    "np.log(1 - 0.___ ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f087ab25",
   "metadata": {},
   "source": [
    "First of all, you will notice that the loss is always negative, which is why there is a minus in the actual formula from above to make the loss positive again. \n",
    "\n",
    "You can see that for particularly high probabilities, the loss moves away from zero. For particularly small probabilities, the loss approaches zero. This means that the more \"wrong\" our model is, the greater the loss, and that is exactly what we want.\n",
    "\n",
    "##### Assuming that our basketball player has hit the shot, then $y_i=1$\n",
    "<img src='Img/intro_stats/bce_2.gif' alt=\"Drawing\" width= \"500px\"/> \n",
    "$$\\begin{align}Loss &=-1\\cdot log(\\hat{y}_i) + (1-1)\\cdot log(1-\\hat{y}_i)\\\\\n",
    "Loss &=-log(\\hat{y}_i)\\end{align}$$\n",
    "\n",
    "This time, a different but still simple part of the formula remains.\n",
    "Try this term with different probabilities as well. \n",
    "This time a probability close to 1 would be correct, which should result in a small loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.log(0.___) # try different probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a4a5c",
   "metadata": {},
   "source": [
    "Again, the loss increases as the probability moves away from the true value. \n",
    "\n",
    "The loss is therefore only complex enough to cover both a true value of `1` and `0`.  The factor $log$ is used so that values further away from the true value have a disproportionate effect on the loss. The previously ignored part of the formula $\\frac{1}{n}\\sum_{i=1}^n$ only calculates the average over all data points in the data set. \n",
    "\n",
    "In the following, the formula for the BCE is defined using `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce086bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCE(y_true, y_hat):\n",
    "    return -np.mean(y_true*np.log(y_hat) +(1-y_true)* np.log(1-y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b00f862",
   "metadata": {},
   "outputs": [],
   "source": [
    "BCE(throws, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd163f",
   "metadata": {},
   "source": [
    "## ROC-AUC \n",
    "\n",
    "Last, we present the ROC-AUC as an alternative to the accuracy. You may know the AUC from an HPLC or NMR. It denotes the *Area Under the Curve*. In this case, we are talking about the area under the ROC curve. \n",
    "\n",
    "Before we get into more detail about this ROC curve, let's clarify why we are using an alternative for accuarcy in the first place. \n",
    "\n",
    "Let's say you are writing a program to distinguish between dogs and cats.\n",
    "You have nine images of dogs and only one of a cat. \n",
    "\n",
    "<img src='Img/intro_stats/catvdogs.png' alt=\"Zeichnung\" width=\"500px\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ce5999",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([\"DOG\", \"CAT\", \"DOG\", \"DOG\", \"DOG\", \"DOG\", \"DOG\", \"DOG\", \"DOG\", \"DOG\", \"DOG\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f97f165",
   "metadata": {},
   "source": [
    "There is a big difference between the number of cats and dogs in the dataset. \n",
    "Can you find a way to always achieve 90% prediction accuracy without ever seeing the images, and if they are randomly ordered?\n",
    "The \"shuffle\" function arranges the elements in random order every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2cb60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(y) # shuffle the elements of the array\n",
    "y_pred = np.array([___,____,____,____,_____,_____,____,____,____,_____]) # write your solution here\n",
    "accuracy(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a605b11d",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "y_pred = np.array([[\"DOG\", \"DOG\", \"DOG\", \"DOG\", \"DOG\", \"DOG\", \"DOG\", \"DOG\", \"DOG\", \"DOG\", \"DOG\"]]) \n",
    "```\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b8fa91",
   "metadata": {},
   "source": [
    "If you simply classify each image as a dog, you will always get an Accuracy of 0.9. \n",
    "This means that a model that recognizes nothing in the image can achieve an Accuracy of 0.9. \n",
    "So we can't really tell from the accuracy whether our model has learned something or just always recognizes `DOG`. \n",
    "The greater the sample size difference between the different classes (*class imbalance*), e.g. `dog` versus `cat`, the less valuable accuracy is as a metric. \n",
    "\n",
    "There are alternative metrics that are more suitable for classifications with *class imbalance*. One of them is the ROC-AUC.\n",
    "\n",
    "ROC is the Receiver Operator Characteristic, a curve that describes the relationship between the *true positive rate* and the *false positive rate*. The AUC is the area under the ROC curve.\n",
    "\n",
    "<img src='Img/intro_stats/roc_auc.png' alt=\"Drawing\" width= \"300px\"/> \n",
    "\n",
    "*What do true and false positve rate mean?*<br><br>\n",
    "Suppose we coded dogs as `1` and cat as `0`. The True Positive Rate (TPR) would then reflect the percentage of correctly identified dog images.<br><br>\n",
    "$$TPR = \\frac{\\textrm{Number of correctly classified dogs images}}{\\textrm{Number of all dogs images}}$$\n",
    "\n",
    "Assuming the model recognizes each image as a dog, what is the True Positive Rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9496b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR = ___/___ \n",
    "TPR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb1f68",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "TPR = 9/9\n",
    "```\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e766ff",
   "metadata": {},
   "source": [
    "As you can imagine, the false positive rate (FPR) is very similar. This time we take the cats.\n",
    "\n",
    "$$FPR = \\frac{\\textrm{Number of cats classified as dogs}}{\\textrm{Number of all cat images}}$$\n",
    "\n",
    "Assuming the model recognizes each image as a dog, what is the True Positive Rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FPR = ___/___\n",
    "FPR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffab391",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "FPR = 1/1\n",
    "```\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e34de1",
   "metadata": {},
   "source": [
    "Now more formally, the ROC AUC provides information about the relationship between a model's performance in dogs and its performance in cats. The calculation of the ROC AUC is a bit more complicated than the calculation of FPR and TPR. \n",
    "But it is important to know these dependencies. \n",
    "A ROC AUC value is always between 0 and 1. A value of 1 means perfect classification, and a value of 0.5 means random classification. \n",
    "To calculate the ROC-AUC, we can use the `roc_auc_score` function from `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_true = np.array([1,0,1,1,1,1,1,1,1,1]) # we have recoded dogs and cats into 1 and 0 this time\n",
    "y_pred = np.array([1,1,1,1,1,1,1,1,1,1])\n",
    "roc_auc_score(y_true ,y_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e060ef",
   "metadata": {},
   "source": [
    "You can see that the ROC AUC value is only 0.5. The model is no better than a random decision.\n",
    "However, in practice we work with predicted probabilities, i.e. values between 0 and 1, instead of just `0` and `1`. This can also be used to calculate the ROC-AUC score.\n",
    "\n",
    "Try changing the probabilities for the cat (second position).\n",
    "Remember that we classify an image as a dog above values of 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1223da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1,0,1,1,1,1,1,1,1,1]) # we have recoded dogs and cats into 1 and 0 this time\n",
    "y_hat = np.array([0.91,____,0.99,0.99,0.99,0.98,0.8,0.7,0.8,0.97])\n",
    "roc_auc_score(y_true ,y_hat )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5bede8",
   "metadata": {},
   "source": [
    "# Practice Exercise\n",
    "\n",
    "Please hand in for grading!\n",
    "\n",
    "There are also logistic regressions with more than one `x` variable. \n",
    "\n",
    "The data used is the *Iris-dataset*. \n",
    "[Here](https://en.wikipedia.org/wiki/Iris_flower_data_set) you can find more information.\n",
    "The aim is to distinguish between two types of iris flowers. *Iris setosa* (`0`) vs. *Iris versicolor* (`1`).\n",
    "\n",
    "The model parameters have already been estimated. Three regression coefficients are given in the following cells.\n",
    "\n",
    "Your task is to use these coefficients to see how well the model works. You determine the membership of five flowers (`x`). You can compare the estimates of the model with the true values in `y`. \n",
    "\n",
    "`beta_1` belongs to the variable in the first column of `x` and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d6e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_1 = 3.0786959\n",
    "beta_2 = -3.0220097\n",
    "beta_0 = -7.306345489594484\n",
    "\n",
    "\n",
    "x =  np.array([[5.1, 3.5],\n",
    "               [5. , 3.6],\n",
    "               [5.4, 3.4],\n",
    "               [6.7, 3.1],\n",
    "               [5.1, 2.5]])\n",
    "\n",
    "y = np.array([0,0,0,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44596e40",
   "metadata": {},
   "source": [
    "First calculate `z`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ae619",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = beta_0 + ___*____ +_____*______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c05198",
   "metadata": {},
   "source": [
    "Convert `z` to probabilities with the `sigmoid` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5aaa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = _____\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca74ae2",
   "metadata": {},
   "source": [
    "Calculate the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5091b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = _____(y_hat)\n",
    "accuracy(______,____)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d3444d",
   "metadata": {},
   "source": [
    "The last thing you do is calculate the the ROC-AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code to calculate the Code for the ROC_AUC"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2063f454e5583264956edca724ed174a35400d49c5baf96fcf9ea99fcd5830b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
