{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5cd938d",
   "metadata": {},
   "source": [
    "# Linear Algebra\n",
    "\n",
    "---\n",
    "### In this lesson you'll learn:\n",
    "\n",
    "- about vectors and matrices and how to do simple calculations with them in Python.\n",
    "- how to calculate the derivative of simple functions.\n",
    "- how the chain rule works and why it is so useful for neural networks.\n",
    "---\n",
    "\n",
    "\n",
    "Today we will explain the essential mathematical principles for neural networks.\n",
    "\n",
    "The first essential mathematical concept is the **vector**.\n",
    "\n",
    "A vector represents a point in a space that is described by several values.\n",
    "For example, a molecule can be described by several descriptors. \n",
    "\n",
    "A vector is represented as follows:\n",
    "\n",
    "$$\\begin{bmatrix}3 & 4 & 0.5\\end{bmatrix}$$ \n",
    "\n",
    "This vector contains exactly three values. We can use vectors to describe individual data points. For example, we could store the data of a house in this vector. The first value indicates how many bathrooms the house has, the second how many bedrooms, and the third value indicates the age of the heating system in years.\n",
    "\n",
    "You may have noticed that a vector has amazing similarities to a 1-dimensional `array`.\n",
    "`np.array([3,4,0.5])`. In fact, `np.arrays` are said to have the same functions as vectors. The mathematical rules that apply to vectors also apply to `arrays`.\n",
    "\n",
    "\n",
    "For example, we can multiply a vector by a number: <br>\n",
    "\n",
    "\n",
    "$$3\\cdot\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix}= \\begin{bmatrix}3\\cdot 3 \\\\ 3 \\cdot 4  \\\\ 3 \\cdot 0.5 \\end{bmatrix}= \\begin{bmatrix}9 \\\\ 12 \\\\ 1.5\\end{bmatrix} $$ \n",
    "\n",
    "<center> <i>For better overview we write the vector as a column. </i> </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d243be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "3 * np.array([3,4,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38713505",
   "metadata": {},
   "source": [
    "The same applies to addition and subtraction:\n",
    "$$3+\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix}= \\begin{bmatrix}3+3 \\\\ 3+4 \\\\ 3+0.5\\end{bmatrix}= \\begin{bmatrix}6 \\\\ 7 \\\\ 3.5\\end{bmatrix} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3074acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "3 + np.array([3,4,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138917dd",
   "metadata": {},
   "source": [
    "We can add two vectors:\n",
    "    \n",
    "    \n",
    "$$\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix} + \\begin{bmatrix}0.3 \\\\ 3 \\\\ -0.2\\end{bmatrix} = \\begin{bmatrix}3 +0.3 \\\\ 4+3 \\\\ 0.5-0.2\\end{bmatrix} =  \\begin{bmatrix}3.3 \\\\ 7 \\\\ 0.3\\end{bmatrix}$$\n",
    "\n",
    "It is important that both vectors have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab074aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([3,4,0.5]) + np.array([0.3,3,-0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d35625",
   "metadata": {},
   "source": [
    "Vectors become really interesting when we multiply several together.\n",
    "\n",
    "Especially the so-called scalar product is important for us and is calculated as follows:\n",
    "$$\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix} \\cdot \\begin{bmatrix}0.3 \\\\ 3 \\\\ -0.2\\end{bmatrix} = (3\\cdot 0.3) + (4 \\cdot 3 )+ (0.5\\cdot -0.2) = 12.8  $$\n",
    "\n",
    "\n",
    "Calculate the scalar product of the vectors by hand: \n",
    "\n",
    "$$\\begin{bmatrix}8 \\\\ 0.25 \\\\ -1\\end{bmatrix} \\cdot \\begin{bmatrix}0.1 \\\\ 12 \\\\ 8\\end{bmatrix} = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a738a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "$$\\begin{bmatrix}8 \\\\ 0.25 \\\\ -1\\end{bmatrix} \\cdot \\begin{bmatrix}0.1 \\\\ 12 \\\\ 8\\end{bmatrix} =(8\\cdot 0.1) + (0.25 \\cdot 12)+ (-1\\cdot 8) = -4.2  $$\n",
    "</details>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7dc70d",
   "metadata": {},
   "source": [
    "In `numpy` we use `np.dot()` to calculate the scalar product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60ffbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(np.array([3,4,0.5]), np.array([0.3,3,-0.2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b686f2d",
   "metadata": {},
   "source": [
    "As you may have noticed, the scalar product is similar to a linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00799a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x    = np.array([3,4,0.5])\n",
    "beta = np.array([0.3,3,-0.2])\n",
    "np.dot(x,beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8094cb88",
   "metadata": {},
   "source": [
    "`x` is the input vector that contains the information for three variables. For example, for a house that has 3 bathrooms and 4 bedrooms. It was equipped with a new heating system half a year ago (`0.5`). The second vector contains the coefficients of the regression. So $\\beta_1, \\beta_2, \\beta_3$. Using the regression, we can then find the value of the house in 100,000 â‚¬. \n",
    "\n",
    "In fact, the scalar product leads to a simplification of the formula. Instead of writing:\n",
    "$$\\hat{y} = \\beta_1x_1 +\\beta_2x_2 +\\beta_3x_3$$\n",
    "we can also write the formula as follows.\n",
    "\n",
    "$$\\hat{y} = x\\beta$$\n",
    "\n",
    "Here we have to assume that $x$ and $\\beta$ are vectors. \n",
    "Of course the $t$ or $\\beta_0$ is still missing. So the intersection of the y-axis. As explained above, single values can simply be added to vectors. \n",
    "\n",
    "So the complete formula is:\n",
    "\n",
    "$$\\hat{y} = x\\beta+\\beta_0$$\n",
    "\n",
    "Can you write this formula using `numpy`? Calculate $\\hat{y}$ for `x`. Use $\\beta_0=-5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c38f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 =-5\n",
    "y_hat = _____________________\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a455ca",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "```python\n",
    "y_hat = np.dot(x,beta)+beta_0\n",
    "    \n",
    "```\n",
    "</details>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2354d961",
   "metadata": {},
   "source": [
    "Assuming we want to determine `y_hat` not only for one house but for several houses at the same time, we can do this with exactly the same formula. \n",
    "\n",
    "`X` now contains not only one vector, but several. As you have already learned, such data structures can be stored as a 2D array. In mathematics, a 2D array is comparable to a matrix. \n",
    "\n",
    "When we talk about matrices, we use capitalized variable names.\n",
    "\n",
    "In the following `X` is given. You can see that `np.dot(X,beta) + beta_0` still gives the correct result. But this time for each of the 4 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[3,4,0.5],\n",
    "              [2,1,1.2],\n",
    "              [4,2,0.12],\n",
    "              [3,3,2]])\n",
    "\n",
    "np.dot(X,beta) + beta_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9c9ec",
   "metadata": {},
   "source": [
    "---\n",
    "The notation with $\\beta$s comes from traditional statistics. In machine learning, the coefficients are denoted by $w$, which stands for \"weights\". In addition, $\\beta_0$, the y-axis intercept, is denoted by $b$ (bias).\n",
    "Thus, the regression equation is:\n",
    "\n",
    "$$Xw+b$$\n",
    "\n",
    "We will keep this spelling from now on.\n",
    "\n",
    "---\n",
    "\n",
    "As you have already learned, the power of neural networks is that they perform more than one regression at a time.\n",
    "That is, we have not just one set of regression coefficients, but several. How many?\n",
    "That is up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728c6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "W =  np.array([beta,\n",
    "              [6,0,-2],\n",
    "              [1,0,3],\n",
    "              [0,0,-1],\n",
    "              [1,2,-1]])\n",
    "b = np.array([beta_0,3,2,0.5,-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b7ecaf",
   "metadata": {},
   "source": [
    "`W` now contains the weights for a total of five linear regressions. The first row still contains our `beta` coefficients from the first regression.  Each additional row contains new coefficients/weights for another regression. So by the number of rows we can see how many regressions we are running. \n",
    "Also `b` contains five values and is therefore now a vector instead of a scalar. For each regression it contains the y-axis intercept.\n",
    "\n",
    "In the context of neural networks, the number of regressions performed corresponds to the number of nodes in the hidden layer of the neural network.\n",
    "\n",
    "If we now use these two matrices, the following happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8b8804",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.dot(X,W)+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c81e8",
   "metadata": {},
   "source": [
    "An error message:\n",
    "```shapes (4,3) and (5,3) not aligned: 3 (dim 1) != 5 (dim 0)```\n",
    "\n",
    "In fact, we can conclude from the error message what the problem is. \n",
    "First, we are given the dimensions (number of rows and columns). \n",
    "`X` has `4` rows and `3` columns. `W` has `5` rows and `3` columns. \n",
    "\n",
    "Then follows: `3 (dim 1) != 5 (dim 0)`. So, `3 (dim 1)`, the number of columns (`3 (dim 1)`) of the first matrix are not equal (`!=`) to the number of rows in the second matrix (`5 (dim 0)`).   \n",
    "\n",
    "**The number of columns in the first matrix should be equal to the number of rows in the second column.**\n",
    "\n",
    "For example, if we flip the `W` matrix by mirroring it \"across the diagonal\" we get rows as columns and columns as rows. Then the number of columns of the first matrix and rows of the second matrix match.\n",
    "\n",
    "Converting columns to rows and vice versa is called the *transpose* of a matrix.\n",
    "`W.tranpose()` performs this transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6fde17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W, \"\\n\")\n",
    "print(W.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30005d55",
   "metadata": {},
   "source": [
    "As you can see, the rows become columns. This also changes the dimensions of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b175f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W.shape, \"\\n\")\n",
    "print(W.transpose().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab72f0c",
   "metadata": {},
   "source": [
    "With the transposition of the matrix `W` the multiplication of the two matrices should work, because now the number of columns/rows is identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e09d26e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.dot(X,W.transpose())+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a3df87",
   "metadata": {},
   "source": [
    "It actually works. For example, look at the first column. These values are indeed the results of the first regression we computed: `np.dot(X, beta)+beta_0`.\n",
    "In fact, each row contains the five regression results for one of the four houses.\n",
    "\n",
    "But how can it be that the regression works even though we have flipped the matrix `W`?\n",
    "\n",
    "This is because of how the matrix multiplication has been defined. The scalar product is not calculated between the corresponding rows. The scalar product is calculated between the rows of the first matrix and the columns of the second matrix (row times column, i.e. dimensions of rows and columns must be equal). \n",
    "\n",
    "![Matthew Scroggs](https://www.mscroggs.co.uk/img/full/multiply_matrices.gif)\n",
    "<center>Source: Matthew Scroggs - 2020 | www.mscroggs.co.uk/blog/73 |</center>\n",
    "\n",
    "\n",
    "This is almost all that is needed for the forward pass in a neural network.\n",
    "\n",
    "---\n",
    "\n",
    "Until now we have always used `np.dot()` for a matrix multiplication. But there is an extra function `np.matmul()`. For large matrices `np.matmul` is faster and therefore we will also use this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f67a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(X,W.transpose())+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db34e67",
   "metadata": {},
   "source": [
    "#  Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41145f44",
   "metadata": {},
   "source": [
    "In order to understand how neural networks learn, one should know at least roughly what derivatives are and how to calculate them.\n",
    "\n",
    "The derivative of a function describes the slope of the original function. \n",
    "Suppose there is a function $f(x)=x^2$. Then the corresponding derivative $\\frac{df}{dx}=2x$ (i.e.: *Derivative of f with respect to x*). \n",
    "\n",
    "In the picture $f(x)$ (*blue*) as well as the derivative $\\frac{df}{dx}$ (*orange*) are drawn. <br>For example, for $x=-5$, $f(-5) = 25$. The slope at this point is: $\\frac{df(-5)}{dx}=2\\cdot -5= -10$. That is, the slope of the function $f(x)=x^2$ is $-10$ when $x=-5$.\n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_1edit.png\"></img>\n",
    "\n",
    "There are some rules about derivation. First, a simple rule with an example: \n",
    "        $$f(x) = x^n \\rightarrow \\frac{df}{dx} = n \\cdot x^{n-1}$$\n",
    "        $$f(x) = x^2 \\rightarrow \\frac{df}{dx} = 2 \\cdot x^{2-1}=2x^1= 2x $$\n",
    "        \n",
    "\n",
    "In principle, constants are always dropped in derivatives.\n",
    "\n",
    "That is:\n",
    "The derivative of $f(x)=x^2 + 5$ is only $2x$, since constants only shift the function, but do not affect its slope. \n",
    "\n",
    "Coefficients are handled differently:\n",
    "\n",
    "$$f(x) = ax^n \\rightarrow \\frac{df}{dx} = (n \\cdot a)\\cdot x^{n-1}$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$$f(x) = 4x^3 \\rightarrow \\frac{df}{dx} = 12x^2$$ \n",
    "\n",
    "\n",
    "**Try to find the derivative of the following functions (probably easier on paper):**\n",
    "\n",
    "$$g(x)= 7x^5 - 3$$\n",
    "\n",
    "$$h(x)= 0.5x^2 + 3x +12$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57264817",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "$$\\frac{dg}{dx} = 35x^4 $$\n",
    "$$\\frac{dh}{dx} = x +3$$\n",
    "\n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58906fd6",
   "metadata": {},
   "source": [
    "# Chain Rule \n",
    "\n",
    "The most important rule for neural networks is the chain rule, where derivatives of chained functions calculated, i.e. general functions of the type: $$f(x) = g(h(x))$$ The derivative of such a function is then: $$\\frac{df}{dx} = \\frac{dg}{dh}\\cdot \\frac{dh}{dx}$$ Based on the formula it might be difficult to understand, but using an example it should be relatively easy.\n",
    "\n",
    "$$\\begin{align}f(x)&= (3x + 1)^2 \\\\&=h^2; \\space\\space\\space\\space\\space\\space h = 3x+1\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{df}{dx} &= \\frac{d}{dh} (h^2)\\cdot \\frac{d}{dx}h\\\\\n",
    "&= 2 h\\cdot \\frac{d}{dx}(3x+1)\\\\\n",
    "&= 2 h \\cdot 3 \\\\\n",
    "&= 6 \\cdot (3x+1)\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "\n",
    "Previously it was said that the derivative describes the slope of the original function. One can also interpret the derivative $\\frac{df}{dx}$ as follows: *By how much does $f(x)$ change if I change $x$?* Here, of course, the amount of change depends on $x$ itself. In the $x^2$ example, small changes in $x$ have a greater impact for values around $x=5$ than for values around $x=1$. \n",
    "\n",
    "If we want to optimize the weights of a neural net, we also need to know how a change in the weights causes a change in the loss. \n",
    "\n",
    "Here again is a schematic of a neural network.\n",
    "\n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_3edit.png\"></img>\n",
    "\n",
    "For the following example we consider only the last part in more detail. The calculation of $\\hat{y}$ is done in two steps. First $Z_2$ is calculated, then a nonlinear function is applied to it, which gives us $\\hat{y}$. \n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_4edit.png\"></img>\n",
    "\n",
    "**For simplicity, we consider only single values in this example.**\n",
    "\n",
    "So $a_1$ is not a vector at this moment, but only a single value, the same is true for $w_2$ and $b_2$.\n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_5.png\"></img>\n",
    "\n",
    "\n",
    "The question is: What influence does $w_2$ / $b_2$ have on the loss $J$. Or how does the loss change when we change $w_2$ / $b_2$?\n",
    "\n",
    "Mathematically, we can call this the derivative of $J$ with respect to $w_1$. \n",
    "We now use $\\partial$ instead of $d$ since we are talking about functions with multiple parameters ($w_2$ and $b_2$).\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2}$$\n",
    "\n",
    "However, there is no direct influence of $w_2$ on the loss. $w_2$ influences $z_2$ and $z_2$ has an effect on $\\hat{y}$. And finally, $\\hat{y}$ has an effect on the loss. So the functions to calculate $\\hat{y}$ and $J$ respectively are *chained*.\n",
    "\n",
    "The chain rule allows us to calculate $\\frac{\\partial J}{\\partial w_2}$ in exactly this way.\n",
    "\n",
    "First, we calculate the effect of $w_2$ on $z_2$:\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}.... $$\n",
    "\n",
    "Next, the effect of $z_2$ on $\\hat{y}$:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}\\frac{\\partial \\hat{y}}{\\partial z_2} $$\n",
    "\n",
    "Last, the effect of $\\hat{y}$ on $J$:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}\\frac{\\partial \\hat{y}}{\\partial z_2}\\frac{\\partial J}{\\partial \\hat{y}} $$\n",
    "\n",
    "\n",
    "The chain rule allows us to simply multiply these effects to get the desired derivative.\n",
    "This chain can become arbitrarily long, therefore a network can also become arbitrarily large. \n",
    "Since, as you may recall, there is also a $w_1$ and $b_1$, their effect on $J$ can also be calculated. For this the chain rule works the same way, the \"chain\" only becomes longer.\n",
    "\n",
    "\n",
    "## Example:\n",
    "\n",
    "$$e_1 = 2x+3$$\n",
    "$$e_2 = 0.5e_1^3$$\n",
    "\n",
    "Try calculating $\\frac{de_2}{dx}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab201c6",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "$$\\frac{de_2}{dx}= \\frac{de_1}{dx}\\frac{de_2}{de_1} $$\n",
    "$$\\frac{de_2}{dx}= 2(1.5e_1^2) $$\n",
    "    \n",
    "Because we know that $e_1 = 2x+3$, we can write.\n",
    "$$\\frac{de_2}{dx}= 2(1.5(2x+3)^2) $$ \n",
    "$$\\frac{de_2}{dx}= 2(1.5(4x^2+12x+9)) $$     \n",
    "$$\\frac{de_2}{dx}= 2(6x^2+18x+13.5) $$ \n",
    "$$\\frac{de_2}{dx}= 12x^2+36x+27 $$   \n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc15184",
   "metadata": {},
   "source": [
    "# Practice Exercise\n",
    "\n",
    "In this exercise you will also calculate the gradient for $w$ as in a neural network. \n",
    "Simplified, of course, and only for one value of $w$. In this example we use a simple loss function and also not a real non-linear function. The loss function would not work in the real application. The same is true for the nonlinear function, since it is linear. A non-linear function, would be beyond the scope of this exercise.\n",
    "\n",
    "\n",
    "Please try to solve this exercise to the best of your ability. As we have said many times, it is not important for us that you get the correct result, but that you have studied the subject. Some people find math easier than others, we are aware of that. \n",
    "\n",
    "\n",
    "\n",
    "Back to our \"faux\" neural network.\n",
    "Let's assume that the last layer of our network works as follows:\n",
    "\n",
    "$$z_2 = a_1w_2+b_2$$\n",
    "$$\\hat{y} = z_2^3-3$$\n",
    "$$J = \\hat{y}^2- y^2$$\n",
    "\n",
    "\n",
    "Calculate $\\frac{\\partial J}{\\partial w_2}$, the \"influence\" $w_2$ has on $J$ (Loss).\n",
    "For this we give the values:\n",
    "<center>\n",
    "$ a_1 = 2 $ <br> $ b_2=1.4 $ <br>   $ w_2 =0.6 $  <br>  $ y=1 $ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate first z_2, y_hat, and J. A simplified forward pass. \n",
    "weight =0.6\n",
    "\n",
    "z_2 = ___*weight+___\n",
    "\n",
    "y_hat = (z_2**__)-___\n",
    "\n",
    "J = ____-____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbb49f0",
   "metadata": {},
   "source": [
    "You have performed the forward pass, now follows the calculation of the gradients. To do this, we first need to calculate only the individual derivatives.\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}\\frac{\\partial \\hat{y}}{\\partial z_2}\\frac{\\partial J}{\\partial \\hat{y}} $$\n",
    "\n",
    "First you calculate $\\frac{\\partial z_2}{\\partial w_2}$ which we will call `dw_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a525c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0c233",
   "metadata": {},
   "source": [
    "Next, you calculate $\\frac{\\partial \\hat{y}}{\\partial z_2}$, we'll call it `dz_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926cadee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz_2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746dd778",
   "metadata": {},
   "source": [
    "Finally, you'll calculate $\\frac{\\partial J}{\\partial \\hat{y}}$, we'll call it `dy_hat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292cde37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_hat = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac66d47",
   "metadata": {},
   "source": [
    "To calculate the gradient, you now only need to multiply these three together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e17169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = dw_2*dz_2*dy_hat\n",
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bedff77",
   "metadata": {},
   "source": [
    "That's it! You have calculated the gradients.\n",
    "\n",
    "**You don`t have to submit the following task, but you can try your hand at it.**\n",
    "\n",
    "If we put these derivatives in a `for-loop` and change the weighting a bit against the gradients, we can see that the loss slowly gets smaller, we are training the \"neural network\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c790b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight =0.6\n",
    "for i in range(10):\n",
    "    z_2 = ___*weight+___\n",
    "    y_hat = (z_2**__)-___\n",
    "    J = ____-____\n",
    "    dw_2 = \n",
    "    dz_2 = \n",
    "    dy_hat = \n",
    "    gradient = dw_2*dz_2*dy_hat\n",
    "    weight -=  0.0001* gradient # updating the weights\n",
    "    print(J)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
