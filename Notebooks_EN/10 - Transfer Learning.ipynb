{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a9797f",
   "metadata": {},
   "source": [
    "# CNN & Transfer Learning\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### In this lesson you'll learn:\n",
    "\n",
    "* what is meant by transfer learning.\n",
    "* how to load and customize pre-trained models.\n",
    "* how to read images from folders into Python.\n",
    "---\n",
    "\n",
    "Last week you developed a CNN that can recognize numbers. Today, we focus on taking advantage of pre-trained neural networks. First, you will train a model that can distinguish between various breeds of dogs and cats. In the training task, you will use [ResNet](https://en.wikipedia.org/wiki/Residual_neural_network) to detect pneumonia in X-ray images.\n",
    "\n",
    "When using transfer learning you arr using an already trained model for a novel problem for twhich the original model was not trained for. \n",
    "\n",
    "<img align=\"center\" src=\"https://pennylane.ai/qml/_images/transfer_learning_general.png\" width=\"400\">\n",
    "<h6 align=\"center\">Pennylane.ai</h6>\n",
    "\n",
    "\n",
    "The pre-trained model is usually a model that has been trained with a lot of general data. This allowes the model to learn enough general information, which can also be relevant to very specific problem.\n",
    "\n",
    "For example, ResNet was trained with data from ImageNet which does not contain X-ray images. But by combining layers of the ResNet model that have already been trained with new untrained layers, we can leverage the \"knowledge\" of ResNet.\n",
    "\n",
    "#### By using pre-trained models we do not have to invest time and computational resources to train these models ourselfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e15f358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch import sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "  !wget https://raw.githubusercontent.com/kochgroup/intro_pharma_ai/main/utils/utils.py\n",
    "  %run utils.py\n",
    "else:\n",
    "  %run ../utils/utils.py\n",
    "%matplotlib inline\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942dd395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data for today, may take a while\n",
    "if 'google.colab' in sys.modules:\n",
    "    !wget https://uni-muenster.sciebo.de/s/TaOR0Lk50rjPHUU/download\n",
    "    !unzip download ../\n",
    "else:\n",
    "    import wget\n",
    "    import zipfile\n",
    "    wget.download(\"https://uni-muenster.sciebo.de/s/TaOR0Lk50rjPHUU/download\")\n",
    "    with zipfile.ZipFile(\"data.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274e9e4",
   "metadata": {},
   "source": [
    "MNIST is a relatively small data set and therefore can be loaded into memory all at once. However, as discussed in the lecture, images are usually larger than the ones in the MNIST dataset. To deal with large image datasets (among other things), PyTorch has its own library `torchvision`. This library provies important functions that we do not have in \"regular\" `torch`. \n",
    "\n",
    "Now, more than even, it is important to pay close attention to how the data/images are stored on your device. If you navigate to the folder `data/images_animals/` you will see two folders. The first folder `train` contains the training images. The second folder `val` contains the test data. Within these folders there are subfolders named after the labels of the images contained within each folder. E.g. the folder `beagel` contains only images of beagels.\n",
    "\n",
    "If a folder structure exists that mimics the above described one, we can read in the data very easily with `torchvision`. But before we can read in the data, we have to transforme the images. \n",
    "\n",
    "First, the images are too large. Most pre-trained models expect an image size of 224 x 224 pixels, since this is the size of the images in the ImageNet dataset. Also, the images still need to be converted to a `tensor`. In a last step we scale the data. This time we do not use the `minmax` scaler, but normalize the images. **To do this, we use the mean and standard deviation of the ImageNet images. Because the network was trained with these images.\n",
    "\n",
    "The function `transforms.Compose()` works similar to `nn.Sequential`. All transformations are applied to all images one afterthe other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c885c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize((224,224)), #reduziert die Größe des Bildes\n",
    "        transforms.ToTensor(), #konvertiert das Bild zu einem Tensor\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) #Normalisiert die Bilder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d194a454",
   "metadata": {},
   "source": [
    "Now that the transformations are defined, you can create a PyTorch dataset. But this time we will use the special class `datasets.ImageFolder`. This special `dataset` class is exactly designed to work with our folder structure. We only need to specify the `path` to the images and which transformations we want to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e112fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder('../data/images_animals/train',data_transforms)\n",
    "test_data = datasets.ImageFolder('../data/images_animals/val',data_transforms)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef23177",
   "metadata": {},
   "source": [
    "You can see that we have a total of 5913 images in our training folder. Also listed are the transformations that will be applied.\n",
    "\n",
    "As a last step we create the `DataLoader`. We do this also for our test dataset, because we can\\`t load all images at once \"into the network\" and therefore also the test set evaluation must be done in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f0709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1235)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=16,\n",
    "                                             shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=16,\n",
    "                                             shuffle=True)\n",
    "\n",
    "example_batch = datasets.ImageFolder('../data/example_batch',data_transforms)\n",
    "example_batch = torch.utils.data.DataLoader(example_batch, batch_size=6,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a273dbf4",
   "metadata": {},
   "source": [
    "You don't know yet which and how many different class we have. We can get this information from the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf26858d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_names = train_data.classes\n",
    "print(class_names)\n",
    "print(\"\\nNumber of Classes: \",len(class_names))\n",
    "\n",
    "# We save a single batch to analyze it better\n",
    "inputs_example, targets_example = next(iter(example_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ce1ec8",
   "metadata": {},
   "source": [
    "In total we have 37 different types of dogs and cats. We can also look at the pictures with a custom function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c51084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = torchvision.utils.make_grid(inputs_example[:6])\n",
    "imshow(out, title=[\"birman\", \"birman\", \"persian\", \"persian\", \"pug\", \"sphynx\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecd8cc9",
   "metadata": {},
   "source": [
    "### ResNet\n",
    "\n",
    "You now have the data in the correct format. However, before we can start training, we also need to get our model into the correct format. \n",
    "As mentioned earlier, PyTorch provides several models that have already been trained. These can be easily loaded. *When loading a model for the first time, the weights still need to be loaded from the internet, which may take some time.* \n",
    "\n",
    "We also use ResNet18, since all larger networks would be too slow to train on the university servers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c9febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a668d38",
   "metadata": {},
   "source": [
    "`resnet18` gives you an overview which PyTorch layers are used in which order. Pay special attention to the last layer named `fc`. We can select ththis layer directly with `resnet.fc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9024958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18.fc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c016962f",
   "metadata": {},
   "source": [
    "This layer is an `nn.Linear` layer that you should remember from the PyTorch introduction notebook. It has 512 features as input and 1000 as output. These 1000 output neurons correspond to the 1000 different classes in the ImageNet dataset.\n",
    "\n",
    "In order to use ResNet for out task of classifying we need to further prepare the \"ResNet\" model. Let's first freeze all layers of ResNet. This means that these layers will not receive weight updates and thus cannot be trained further. We can do this becasue the model has already been trained.\n",
    "The following code iterates through all the layers and sets `requires_grad` to `False`. This lets PyTorch know that no gradients need to be calculated for these layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7be72",
   "metadata": {},
   "source": [
    "The last thing we need to do is to replace the `fc` layer. Since we don\\`t have 1000 classes, but only 37 to predict. Hence we need a new `nn.Linear` layer, which has as input the size 512 and as output the size 37."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4f763",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "resnet18.fc = nn.Linear(512, 37) #replaced the linear layer\n",
    "\n",
    "print(resnet18.fc)\n",
    "list(resnet18.fc.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d356c",
   "metadata": {},
   "source": [
    "You can see that the new `fc` layer has `requires_grad` set to `True`. \n",
    "These weights will be updated during training. So the `fc` layer is the only layer to be trained in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1beb053",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "Now we can start with the training loop. Before we do this, we define the loss function and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7cfaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet18.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b9df3",
   "metadata": {},
   "source": [
    "The training loop will look a bit more complex today. This is because the test set needs to be evaluated using minibatches. To still calculate the metrics correctly, we use the variables `running_loss` and `running_corrects` to keeo track of out predictions and average the performance at the end of the loop.\n",
    "The training process will take quite a long time due to the many calculations, even if only one layer is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8137322",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(3333)\n",
    "for epoch in range(3):\n",
    "    \n",
    "    #### Training ####\n",
    "    resnet18.train()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output=resnet18(inputs)\n",
    "        _ , preds = torch.max(output, 1)\n",
    "        loss = loss_funktion(output,targets)\n",
    "        running_loss +=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_corrects +=torch.sum(preds == targets.data)   \n",
    "    epoch_loss = running_loss/len(train_loader)    \n",
    "    epoch_acc = running_corrects.double() / len(train_data)  \n",
    "    print('Trainings Loss: {:.4f} Trainings Acc: {:.4f}'.format(\n",
    "        epoch_loss, epoch_acc))\n",
    "    \n",
    "    #### Evaluation #####\n",
    "    resnet18.eval()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    for inputs, targets in tqdm(test_loader):\n",
    "        output=resnet18(inputs)\n",
    "        _ , preds =torch.max(output, 1)\n",
    "        loss = loss_funktion(output,targets)\n",
    "        running_loss +=loss.item()\n",
    "        running_corrects +=torch.sum(preds == targets.data)\n",
    "    epoch_acc = running_corrects.double() / len(test_data) \n",
    "    epoch_loss = running_loss/len(test_loader)    \n",
    "    print('Test Loss: {:.4f} Test Acc: {:.4f}'.format(\n",
    "        epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a29f1c",
   "metadata": {},
   "source": [
    "After three epochs, we already achieve a test accuracy of `0.8`. 80% of the images are classified correctly.\n",
    "\n",
    "To really make sure that the pre-training of the model has made a difference, we train the same model again. This time, however, without loading the pre-trained weights:\n",
    "\n",
    "`pretrained=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db76ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=False) #  <- ResNet is loaded without the pre-trained weights\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "resnet18.fc = nn.Linear(512, 37)\n",
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet18.parameters(), lr=0.0001)\n",
    "torch.manual_seed(3333)\n",
    "for epoch in range(3):\n",
    "    \n",
    "    #### Training ####\n",
    "    resnet18.train()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output=resnet18(inputs)\n",
    "        _ , preds = torch.max(output, 1)\n",
    "        loss = loss_funktion(output,targets)\n",
    "        running_loss +=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_corrects +=torch.sum(preds == targets.data)   \n",
    "    epoch_loss = running_loss/len(train_loader)    \n",
    "    epoch_acc = running_corrects.double() / len(train_data)  \n",
    "    print('Trainings Loss: {:.4f} Trainings Acc: {:.4f}'.format(\n",
    "        epoch_loss, epoch_acc))\n",
    "    \n",
    "    #### Evaluation #####\n",
    "    resnet18.eval()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    for inputs, targets in tqdm(test_loader):\n",
    "        output=resnet18(inputs)\n",
    "        _ , preds =torch.max(output, 1)\n",
    "        loss = loss_funktion(output,targets)\n",
    "        running_loss +=loss.item()\n",
    "        running_corrects +=torch.sum(preds == targets.data)\n",
    "    epoch_acc = running_corrects.double() / len(test_data) \n",
    "    epoch_loss = running_loss/len(test_loader)    \n",
    "    print('Test Loss: {:.4f} Test Acc: {:.4f}'.format(\n",
    "        epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9f8c8",
   "metadata": {},
   "source": [
    "After 3 epochs, we are not nearly as accurate as if we had used the \"pretrained\" model. This is because the pretrained convolutions are doing some sort of feature generation/extraction.\n",
    "\n",
    "We can see this more clearly by looking at the convolution activations. \n",
    "For this we use the example images from the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d8454a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = torchvision.utils.make_grid(inputs_example[:6])\n",
    "imshow(out, title=[\"birman\", \"birman\", \"persian\", \"persian\", \"pug\", \"sphynx\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064fdb47",
   "metadata": {},
   "source": [
    "First, we reload the pretrained ResNet model. Again, we remove the `fc` layer, but this time do not replace it with a new linear layer. This gives us direct access to the output of the convolution layer. We call this model `resnet_convolutions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0460706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet_convolutions = nn.Sequential(*list(resnet18.children())[:-1])\n",
    "resnet_convolutions.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26168826",
   "metadata": {},
   "source": [
    "Finally, we pass the six images from before through this special network and save the output (`feature_encoding`). This output will later serve as the input for the linear layer that makes the final prediction.\n",
    "\n",
    "<img align=\"center\" src=\"Img/transferlearning/tf_2.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b2aa34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_encodings=resnet_convolutions(inputs_example)[:6,:,0,0]\n",
    "feature_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938748d5",
   "metadata": {},
   "source": [
    "These \"encodings\" are supposed to be a reduced representation of the original image. A kind of fingerprint. If it is true that the pre-trained convolutions find identify features that are relevant for classification, then similar images should also have similar reduced representations/encodings.\n",
    "\n",
    "For example, the third and fourth images each show a \"persian\" cat. So the encodings of the images should also be similar. We can use the `cosine_similarity` to judge how similar two vectors are. The cosine similarity is always between -1 (very dissimilar) and 1 (very similar). \n",
    "\n",
    "We can calculate the similarity between the third image (`persian`) and all other images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06018359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cosine_similarity(feature_encodings[2:3].detach(),feature_encodings.detach()).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824b0499",
   "metadata": {},
   "source": [
    "The similarity of the third image to the third image is of course `1`, because it is the same image. But to the other images the similarity is much lower. The most similar is the fourth image with a cosine similarity of`0.891`. This image is also a picture of a `persian` cat. This indicates that this pre-trained model was already able to recognize certain similarities in the images.   \n",
    "\n",
    ">But the images could also be similar before the convolutions?\n",
    "\n",
    "That is correct, but we can check that too. In the following cell we calculate the similarity of the original images before the convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9dbf5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cosine_similarity(inputs_example.flatten(1)[2:3],inputs_example.flatten(1)[0:6]).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9c1cef",
   "metadata": {},
   "source": [
    "Here we notice that the second image of a `persian` cat is in fact the most dissimilar, although both images show the same cat breed. We can conclude that the network can indeed find non-trivial fetaures in images.\n",
    "\n",
    "**Full disclosure: ImageNet, the daatset on whcih ResNet was orginally trained on, includes also various cat and dog breeds, also the breed \"persian\". Effects of using pretraining will most likely be less pronouced when classifiyng inages completely \"new to\" ResNet**\n",
    "\n",
    "Finally, we try out how well our network performs when we load the pre-trained model and create our own linear layer. This time, however, we do not freeze the pre-trained convolutional layers, but train them further as well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d6190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True) #PRETRAIN = TRUE\n",
    "torch.manual_seed(1234)\n",
    "resnet18.fc = nn.Linear(512, 37) \n",
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet18.parameters(), lr=0.0001)\n",
    "torch.manual_seed(3333)\n",
    "for epoch in range(3):\n",
    "    \n",
    "    #### Training ####\n",
    "    resnet18.train()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output=resnet18(inputs)\n",
    "        _ , preds = torch.max(output, 1)\n",
    "        loss = loss_funktion(output,targets)\n",
    "        running_loss +=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_corrects +=torch.sum(preds == targets.data)   \n",
    "    epoch_loss = running_loss/len(train_loader)    \n",
    "    epoch_acc = running_corrects.double() / len(train_data)  \n",
    "    print('Trainings Loss: {:.4f} Trainings Acc: {:.4f}'.format(\n",
    "        epoch_loss, epoch_acc))\n",
    "    \n",
    "    #### Evaluierung #####\n",
    "    resnet18.eval()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    for inputs, targets in tqdm(test_loader):\n",
    "        output=resnet18(inputs)\n",
    "        _ , preds =torch.max(output, 1)\n",
    "        loss = loss_funktion(output,targets)\n",
    "        running_loss +=loss.item()\n",
    "        running_corrects +=torch.sum(preds == targets.data)\n",
    "    epoch_acc = running_corrects.double() / len(test_data) \n",
    "    epoch_loss = running_loss/len(test_loader)    \n",
    "    print('Test Loss: {:.4f} Test Acc: {:.4f}'.format(\n",
    "        epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b377c",
   "metadata": {},
   "source": [
    "This network leads to the best results. This is due to the weights of the convolutions now being trained further. Thus, the feature generation is also better adapted to our data set. \n",
    "\n",
    "In practice, different learning rates are often used for the new linear layers and the already trained convolutions. This allows the new linear layer to be trained faster than the convolutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c80e4",
   "metadata": {},
   "source": [
    "# Practise Exercise\n",
    "\n",
    "Please restart the kernel before working on the exercise.\n",
    "\n",
    "As discussed several times in the lecture, today for the exercise we will use a pre-trained model to detect pneumonia from X-ray images.\n",
    "\n",
    "To do this, you will need to read in the data correctly, prepare the model, and fill in the `for-loop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2aa1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from torch.nn.functional import sigmoid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch import sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "%run ../utils/utils.py\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4a6b3c",
   "metadata": {},
   "source": [
    "First, navigate to the folder that contains the animal images. There you will find a folder `chest_xray`. This folder contains subfolders with the respective training and test datasets.\n",
    "First determine which transformations are to be applied to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce697294",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize((224,224)), #reduces the size of the image\n",
    "        transforms.ToTensor(), #converts the image to a tensor\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) #Normalizes the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0d7dac",
   "metadata": {},
   "source": [
    "Next, load the appropriate `Datasets` and `DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940feba4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder('../data/chest_xray/train',data_transforms)\n",
    "test_data = datasets.ImageFolder('../data/chest_xray/val',data_transforms)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d40f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1235)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cbd8ce",
   "metadata": {},
   "source": [
    "Find out how many different classes we have, and remember that this affects the definition of our loss function and network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87273af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_data.classes\n",
    "print(class_names)\n",
    "print(\"\\nNumber of Classes: \",len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f56a574",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inputs_example, targets_example = next(iter(train_loader))\n",
    "\n",
    "out = torchvision.utils.make_grid(inputs_example[:2])\n",
    "imshow(out, title=[class_names[x] for x in targets_example[:2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d9cb91",
   "metadata": {},
   "source": [
    "First load the **pre-trained** `resnet18`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa754da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(____________)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c5aaa",
   "metadata": {},
   "source": [
    "Prevent the `resnet` layers from being trained even further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c2475",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet18.parameters():\n",
    "    __________ = ____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f3878f",
   "metadata": {},
   "source": [
    "Replace the correct layer with a new layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5270c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "______ = ___________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2197e868",
   "metadata": {},
   "source": [
    "Define loss function and optimizer. Which loss function should we take for this number of classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f3f11c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_funktion = __________\n",
    "optimizer = optim.Adam(_______________, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a6c06d",
   "metadata": {},
   "source": [
    "Finally, fill in the training loop. We use `type_as(output)` to get the correct `dtype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14803a02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(3333)\n",
    "for epoch in range(1):\n",
    "    \n",
    "    #### Training ####\n",
    "    resnet18.train()\n",
    "    \n",
    "    # Needed for the Loss and AUC calculation\n",
    "    running_loss = 0\n",
    "    pred_ll = []\n",
    "    targets_ll = []\n",
    "    \n",
    "    \n",
    "    for inputs, targets in tqdm(_______):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward Propagation\n",
    "        output=resnet18(__________).squeeze()\n",
    "        loss = loss_funktion(_______ , _____.type_as(output))\n",
    "        \n",
    "        # Saving the Loss and Predictions\n",
    "        pred_ll.append(sigmoid(output).squeeze().detach().clone().numpy())\n",
    "        targets_ll.append(targets.detach().numpy())\n",
    "        running_loss +=loss.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "         \n",
    "    epoch_loss = running_loss/len(train_loader)    \n",
    "    epoch_auc =  roc_auc_score(targets_ll,pred_ll)\n",
    "    print('Trainings Loss: {:.4f} Trainings AUC: {:.4f}'.format(\n",
    "        epoch_loss, epoch_auc))\n",
    "    \n",
    "    \n",
    "    #### Evaluation #####\n",
    "    resnet18.eval()\n",
    "    \n",
    "    # Needed for the Loss and AUC calculation\n",
    "    running_loss = 0\n",
    "    pred_ll = []\n",
    "    targets_ll = []\n",
    "    \n",
    "    for inputs, targets in tqdm(__________):\n",
    "        \n",
    "        #Forward Propagation\n",
    "        output=resnet18(_______).squeeze()\n",
    "        loss = loss_funktion(______,______.type_as(output))\n",
    "        \n",
    "        pred_ll.append(sigmoid(output).squeeze().detach().clone().numpy())\n",
    "        targets_ll.append(targets.detach().numpy())\n",
    "        running_loss +=loss.item()\n",
    "\n",
    "    epoch_auc =  roc_auc_score(targets_ll,pred_ll)\n",
    "    epoch_loss = running_loss/len(test_loader)    \n",
    "    print('Test Loss: {:.4f} Test Auc: {:.4f}'.format(\n",
    "        epoch_loss, epoch_auc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
