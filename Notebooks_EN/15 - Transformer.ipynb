{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c485ff",
   "metadata": {
    "id": "88c485ff"
   },
   "source": [
    "# Transformers\n",
    "\n",
    "### In this lesson you'll learn:\n",
    "\n",
    "- The core concepts behind the Transformer architecture\n",
    "- How self-attention works \n",
    "- How to implement multi-head attention from scratch\n",
    "- How to build positional encodings\n",
    "- How to construct the complete Transformer (Encoder + Decoder)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652362a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "652362a1",
    "outputId": "eeb3b77d-f146-4681-d5f7-f4245ff9acf4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import re\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fecc35",
   "metadata": {
    "id": "e2fecc35"
   },
   "source": [
    "## 1. Understanding Self-Attention\n",
    "\n",
    "Self-attention is the core mechanism that allows Transformers to model relationships between all positions in a sequence simultaneously. The attention mechanism can be summarized as:\n",
    "\n",
    "$$\n",
    "\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **Q** (Query): What we're looking for\n",
    "- **K** (Key): What we're looking at  \n",
    "- **V** (Value): The actual content we want to extract\n",
    "- **d_k**: Dimension of the key vectors (for scaling)\n",
    "\n",
    "Think of it like a search engine:\n",
    "- **Query**: Your search terms\n",
    "- **Key**: The keywords of each document\n",
    "- **Value**: The actual content of each document\n",
    "- **Attention weights**: How relevant each document is to your search\n",
    "\n",
    "Try to implement the formula in the cell below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818495aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "818495aa",
    "outputId": "412da9c9-1043-4e7a-e541-a4103e1023e6"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "\n",
    "    Args:\n",
    "        Q: Query matrix [batch_size, seq_len, d_k]\n",
    "        K: Key matrix [batch_size, seq_len, d_k]\n",
    "        V: Value matrix [batch_size, seq_len, d_v]\n",
    "        mask: Optional mask to prevent attention to certain positions\n",
    "\n",
    "    Returns:\n",
    "        output: Attended values [batch_size, seq_len, d_v]\n",
    "        attention_weights: Attention weights [batch_size, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the dimension of keys for scaling\n",
    "    d_k = Q.size(-1)\n",
    "\n",
    "    # Step 1: Compute attention scores (Q * K^T)\n",
    "    scores = ____________________\n",
    "\n",
    "    print(f\"Before scaling\", scores)\n",
    "    # Step 2: Scale by sqrt(d_k) to prevent softmax saturation\n",
    "    # You can try without scaling and see the difference\n",
    "    scores = _______________\n",
    "    print(f\"After scaling\", scores)\n",
    "    # Step 3: Apply mask if provided (set masked positions to large negative value)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    # Step 4: Apply softmax to get attention weights\n",
    "    attention_weights = _______________\n",
    "\n",
    "    # Step 5: Apply attention weights to values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "# Test with a simple example\n",
    "batch_size, seq_len, d_model = 1, 4, 8\n",
    "\n",
    "# Create random Q, K, V matrices\n",
    "Q = torch.randn(batch_size, seq_len, d_model)\n",
    "K = torch.randn(batch_size, seq_len, d_model)\n",
    "V = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Input shapes: Q{Q.shape}, K{K.shape}, V{V.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"\\nAttention weights (should sum to 1 across last dimension):\")\n",
    "print(attention_weights[0])\n",
    "print(f\"Row sums: {attention_weights[0].sum(dim=-1)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ed3dc8",
   "metadata": {
    "id": "67ed3dc8"
   },
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "```python\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "scores = scores / math.sqrt(d_k)\n",
    "\n",
    "attention_weights = F.softmax(scores, dim=-1)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee6673",
   "metadata": {
    "id": "3aee6673"
   },
   "source": [
    "###  Understanding Attention Weights\n",
    "\n",
    "Let's visualize what the attention mechanism is actually doing. Complete the code below to create a heatmap of the attention weights:\n",
    "\n",
    "How to read the heatmap:\n",
    "\n",
    "\n",
    "*   Brighter cell at row i, column j means position i relies more on position j.\n",
    "*   A bright diagonal means  tokens manily attend to local neighbors. Bright off-diagonals indicate long-range dependencies\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be919b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "18be919b",
    "outputId": "6bc44c6c-7c2d-4c55-ad25-20aa8c0dc449"
   },
   "outputs": [],
   "source": [
    "# Create a simple example with interpretable patterns\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create Q, K, V where first position attends strongly to last position\n",
    "seq_len = 6\n",
    "d_model = 4\n",
    "\n",
    "# Create patterns in Q and K that will create attention patterns\n",
    "Q = torch.zeros(1, seq_len, d_model)\n",
    "K = torch.zeros(1, seq_len, d_model)\n",
    "V = torch.randn(1, seq_len, d_model)\n",
    "print(f\"Q: {Q}\")\n",
    "print(f\"K: {K}\")\n",
    "print(f\"V:{V}\")\n",
    "# Make first query similar to last key. In other words, make \"first token\" strongly looks at the last token\n",
    "Q[0, 0, :] = torch.tensor([1.0, 0.0, 0.0, 0.0])\n",
    "K[0, -1, :] = torch.tensor([1.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "# Make middle positions attend to themselves\n",
    "for i in range(1, seq_len-1):\n",
    "    Q[0, i, :] =_____________________\n",
    "    K[0, i, :] =_____________________\n",
    "print(f\"Q: {Q}\")\n",
    "print(f\"K: {K}\")\n",
    "\n",
    "# Fill in the missing code to compute attention and visualize\n",
    "output, attention_weights = scaled_dot_product_attention(_____, ______, _____)\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attention_weights[0].detach().numpy(), cmap='Blues')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Attention Weights Heatmap')\n",
    "plt.show()\n",
    "\n",
    "print(\"Attention weights matrix:\")\n",
    "print(attention_weights[0].detach().numpy().round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3014b765",
   "metadata": {
    "id": "3014b765"
   },
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "```python\n",
    "\n",
    "for i in range(1, seq_len-1):\n",
    "    Q[0, i, :] = torch.tensor([0.0, 1.0, 0.0, 0.0])\n",
    "    K[0, i, :] = torch.tensor([0.0, 1.0, 0.0, 0.0])\n",
    "\n",
    "\n",
    "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c099ec90",
   "metadata": {
    "id": "c099ec90"
   },
   "source": [
    "## 2. Multi-Head Attention\n",
    "\n",
    "While single-head attention is powerful, Multi-Head Attention allows the model to attend to different types of information simultaneously. Think of it as having multiple \"attention experts\" that each focus on different aspects of the input. This has several advantages:\n",
    "\n",
    "- Different representation subspaces: Each head can learn different types of relationships\n",
    "- Increased model capacity: More parameters to learn complex patterns  \n",
    "- Parallel processing: All heads can be computed simultaneously\n",
    "\n",
    "The formula for multi-head attention is:\n",
    "\n",
    "$$\n",
    "\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h) W^O\n",
    "$$\n",
    "\n",
    "Where each head is:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\mathrm{Attention}(Q W^Q_i, K W^K_i, V W^V_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72394d73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72394d73",
    "outputId": "307cef0c-c354-4508-bb1d-b2a233e5de38"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Multi-Head Attention Layer\n",
    "\n",
    "        Args:\n",
    "            d_model: Model dimension (must be divisible by num_heads)\n",
    "            num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "\n",
    "        # Linear projections for Q, K, V and outout for all heads (computed in parallel)\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        q_seq_len = query.size(1)\n",
    "        k_seq_len = key.size(1)\n",
    "        v_seq_len = value.size(1)\n",
    "\n",
    "        # 1. Linear projections\n",
    "        Q = ____________\n",
    "        K = ____________\n",
    "        V = ____________\n",
    "\n",
    "        # 2. Reshape to separate heads: [batch_size, num_heads, seq_len, d_k]\n",
    "        Q = ________________________________\n",
    "        K = ________________________________\n",
    "        V = ________________________________\n",
    "\n",
    "        # 3. Apply attention to each head\n",
    "        attention_output, attention_weights = _____________________\n",
    "\n",
    "        # 4. Concatenate heads: [batch_size, q_seq_len, d_model]\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, q_seq_len, self.d_model\n",
    "        )\n",
    "\n",
    "        # 5. Apply output projection\n",
    "        output = self.W_o(attention_output)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "# Test the multi-head attention\n",
    "d_model, num_heads = 512, 8\n",
    "seq_len, batch_size = 10, 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attention_weights = mha(x, x, x)  # Self-attention: Q=K=V=x\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in mha.parameters())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56039413",
   "metadata": {
    "id": "56039413"
   },
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "```python\n",
    "\n",
    "Q = self.W_q(query)\n",
    "K = self.W_k(key)  \n",
    "V = self.W_v(value)\n",
    "\n",
    "\n",
    "Q = Q.view(batch_size, q_seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "K = K.view(batch_size, k_seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "V = V.view(batch_size, v_seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "attention_output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5705fc33",
   "metadata": {
    "id": "5705fc33"
   },
   "source": [
    "## Multi-Head Attention Parameters\n",
    "\n",
    "Calculate how many parameters are in our Multi-Head Attention layer and understand where they come from.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f2334",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4c4f2334",
    "outputId": "1f1e5bb8-b312-4f80-c5bb-a26f49424e93"
   },
   "outputs": [],
   "source": [
    "# Parameter calculation for Multi-Head Attention\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "# Each linear layer has weight matrix [input_dim, output_dim] + bias [output_dim]\n",
    "\n",
    "\n",
    "W_q_params = _______________________\n",
    "W_k_params = _______________________\n",
    "W_v_params = _______________________\n",
    "W_o_params = _______________________\n",
    "\n",
    "total_params = W_q_params + W_k_params + W_v_params + W_o_params\n",
    "\n",
    "print(f\"W_q parameters: {W_q_params:,}\")\n",
    "print(f\"W_k parameters: {W_k_params:,}\")\n",
    "print(f\"W_v parameters: {W_v_params:,}\")\n",
    "print(f\"W_o parameters: {W_o_params:,}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Verify with PyTorch\n",
    "mha_test = MultiHeadAttention(d_model, num_heads)\n",
    "pytorch_params = sum(p.numel() for p in mha_test.parameters())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c32605f",
   "metadata": {
    "id": "8c32605f"
   },
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "```python\n",
    "# Each linear layer has weight matrix [input_dim, output_dim] + bias [output_dim]\n",
    "W_q_params = d_model * d_model + d_model  # 512*512 + 512 = 262,656\n",
    "W_k_params = d_model * d_model + d_model  # 512*512 + 512 = 262,656\n",
    "W_v_params = d_model * d_model + d_model  # 512*512 + 512 = 262,656\n",
    "W_o_params = d_model * d_model + d_model  # 512*512 + 512 = 262,656\n",
    "```\n",
    "\n",
    "**Explanation**: Each of the 4 linear layers transforms from d_model (512) dimensions to d_model (512) dimensions, requiring 512×512 weight parameters plus 512 bias parameters each. In this implementation, the total number of parameters doens't depend on num of heads because splitting into heads is just a reshape.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b752e86",
   "metadata": {
    "id": "4b752e86"
   },
   "source": [
    "## 3. Positional Encoding\n",
    "\n",
    "One key limitation of the attention mechanism is that it has no inherent sense of position. Unlike RNNs that process sequences step by step, attention can access all positions simultaneously - but this means it doesn't know the order of elements!\n",
    "\n",
    "**Example**: The sentences \"The cat sat on the mat\" and \"The mat sat on the cat\" would have identical attention computations without positional information.\n",
    "\n",
    "### Solution: Positional Encoding\n",
    "\n",
    "We add positional information to the input embeddings using sinusoidal functions:\n",
    "\n",
    "$$\n",
    "\\mathrm{PE}(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "$$\n",
    "\\mathrm{PE}(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `pos` is the position in the sequence\n",
    "- `i` is the dimension index\n",
    "- Even dimensions use sine, odd dimensions use cosine\n",
    "\n",
    "As you would see in the Figure below, each position gets a unique pattern acroos dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec25349d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "id": "ec25349d",
    "outputId": "a3a76c8e-5330-4fbf-e1fd-a8c84a168748"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=5000):\n",
    "        \"\"\"\n",
    "        Sinusoidal Positional Encoding\n",
    "\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            max_seq_len: Maximum sequence length to pre-compute\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Allocate \"pe\" of shape [max_seq_len, d_model] filled with zero\n",
    "        pe = __________________\n",
    "        # Build position as [max_seq_len, 1] containing 0,1,2,.... for broadcasting\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "\n",
    "        # Create the div_term for the sinusoidal pattern\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                           -(math.log(10000.0) / d_model))\n",
    "\n",
    "        # Fill even and odd dimensions (you can use torch.sin and torch.cos)\n",
    "        pe[:, 0::2] = ___________________\n",
    "\n",
    "        pe[:, 1::2] = _____________________\n",
    "\n",
    "        # Add batch dimension and register as buffer (not a parameter)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_seq_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to input embeddings\n",
    "\n",
    "        Args:\n",
    "            x: Input embeddings [batch_size, seq_len, d_model]\n",
    "        Returns:\n",
    "            x + positional encodings\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# Test positional encoding\n",
    "d_model = 512\n",
    "max_seq_len = 100\n",
    "\n",
    "pos_enc = PositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "# Create dummy input\n",
    "batch_size, seq_len = 2, 20\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Add positional encoding\n",
    "x_with_pos = pos_enc(x)\n",
    "\n",
    "print(f\"Original input shape: {x.shape}\")\n",
    "print(f\"With positional encoding shape: {x_with_pos.shape}\")\n",
    "print(f\"Positional encoding matrix shape: {pos_enc.pe.shape}\")\n",
    "\n",
    "# Visualize positional encodings\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(pos_enc.pe[0, :50, :].numpy(), cmap='RdBu', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Positional Encoding Visualization (first 50 positions)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddb8f4d",
   "metadata": {
    "id": "eddb8f4d"
   },
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "```python\n",
    "pe = torch.zeros(max_seq_len, d_model)\n",
    "\n",
    "# Apply sine to odd indices\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "# Apply cosine to odd indices  \n",
    "pe[:, 1::2] = torch.cos(position * div_term)\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d5f5e",
   "metadata": {
    "id": "cd9d5f5e"
   },
   "source": [
    "## 4. Feed-Forward Network\n",
    "\n",
    "Each Transformer layer also includes a position-wise feed-forward network that processes each position independently. It's a simple two-layer MLP with ReLU activation:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathrm{FFN}{x} = \\max(0,\\, xW_1 + b_1)\\, W_2 + b_2\n",
    "$$\n",
    "\n",
    "This adds non-linearity and allows the model to transform the attention outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0229e98f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0229e98f",
    "outputId": "4e1bb298-8ac3-4f7d-a0ca-d827a3b8f0af"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Position-wise Feed-Forward Network\n",
    "\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            d_ff: Hidden dimension of feed-forward layer (usually 4 * d_model)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.linear1 = _______________________\n",
    "        self.linear2 = _______________________\n",
    "        self.dropout = _______________________\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return ___________________________\n",
    "\n",
    "# Test feed-forward network\n",
    "d_model = 512\n",
    "d_ff = 2048  # Typically 4 * d_model\n",
    "\n",
    "ff = FeedForward(d_model, d_ff)\n",
    "x = torch.randn(2, 20, d_model)\n",
    "output = ff(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Feed-forward parameters: {sum(p.numel() for p in ff.parameters()):,}\")\n",
    "\n",
    "# The parameters come from two linear layers:\n",
    "# Linear1: d_model * d_ff + d_ff = 512 * 2048 + 2048 = 1,050,624\n",
    "# Linear2: d_ff * d_model + d_model = 2048 * 512 + 512 = 1,049,088\n",
    "# Total: 2,099,712\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e70cfb",
   "metadata": {
    "id": "67e70cfb"
   },
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "```python\n",
    "self.linear1 = nn.Linear(d_model, d_ff)\n",
    "self.linear2 = nn.Linear(d_ff, d_model)\n",
    "self.dropout = nn.Dropout(dropout)\n",
    "def forward(self, x):\n",
    "  return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5031b0",
   "metadata": {
    "id": "de5031b0"
   },
   "source": [
    "## 5. Transformer Encoder Layer\n",
    "\n",
    "Now we can combine all components into a complete Transformer Encoder Layer. Each layer consists of:\n",
    "\n",
    "1. Multi-Head Self-Attention\n",
    "2. Residual connection (original input) + Layer Normalization\n",
    "3. Feed-Forward Network\n",
    "4. Residual connection + Layer Normalization\n",
    "\n",
    "The residual connections help with gradient flow, and layer normalization stabilizes training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30121ef5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30121ef5",
    "outputId": "4fa2ca87-1fcf-4525-888a-f22db3207979"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Single Transformer Encoder Layer\n",
    "\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            d_ff: Feed-forward hidden dimension\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "\n",
    "        self.multi_head_attention = ______________\n",
    "        self.feed_forward = _____________________\n",
    "        self.dropout = ____________\n",
    "        # Layer normalization\n",
    "        self.norm1 = _____________\n",
    "        self.norm2 = __________\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # define the forward function\n",
    "        # First step is attention mechanism\n",
    "        # Second step is adding and normalizing the attention output\n",
    "        # Third step is passing the output from the previous step through the feed forward network\n",
    "        # Fourth step is adding and anormalziing the output from the previous step\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "# Test encoder layer\n",
    "d_model, num_heads, d_ff = 512, 8, 2048\n",
    "encoder_layer = TransformerEncoderLayer(d_model, num_heads, d_ff)\n",
    "\n",
    "x = torch.randn(2, 20, d_model)\n",
    "output = encoder_layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Encoder layer parameters: {sum(p.numel() for p in encoder_layer.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y4cU1gC9-NmE",
   "metadata": {
    "id": "Y4cU1gC9-NmE"
   },
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "self.multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "self.norm1 = nn.LayerNorm(d_model)\n",
    "self.norm2 = nn.LayerNorm(d_model)\n",
    "self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "def forward(self, x, mask=None):\n",
    "\n",
    "    # Multi-head attention with residual connection and layer norm\n",
    "    attn_output, _ = self.multi_head_attention(x, x, x, mask)\n",
    "    x = self.norm1(x + self.dropout(attn_output))\n",
    "    \n",
    "    # Feed-forward with residual connection and layer norm\n",
    "    ff_output = self.feed_forward(x)\n",
    "    x = self.norm2(x + self.dropout(ff_output))\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70f786a",
   "metadata": {
    "id": "a70f786a"
   },
   "source": [
    "## 6. Transformer Decoder Layer\n",
    "\n",
    "The decoder turns partial target sequences into the next-token predictions, one layer at a time. each layer takes the current target states x (embeddings plus positional encodings) and the encoder's output, then refines x through three sublayers with residual connection and LayerNorm.\n",
    "First, masked self-attention lets each target position attend only to earlier or current target positions (a casual mask blocks the future). This produces context-aware states while preserving autoregressive order.\n",
    "Second, cross attention lets those states query the encoder output so the the decoder can align with and extact information from the source sequence.\n",
    "Third, a position wise feed-forward network transforms each position to increase represententaional power.\n",
    "After each sublayer, a residual connection and LayerNorm stabilize training:dropout helps regularize.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c65771a",
   "metadata": {
    "id": "3c65771a"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "      # Initialize the layers (multi-head attenton, cross-attention, feed forward, 3 layer normalization and dropout)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KDTaLv54BqKU",
   "metadata": {
    "id": "KDTaLv54BqKU"
   },
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Transformer Decoder Layer\n",
    "        \n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            d_ff: Feed-forward hidden dimension\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Forward pass through decoder layer\n",
    "        \n",
    "        \"\"\"\n",
    "        attn_output, _ = self.multi_head_attention(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output, _ = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45ab354",
   "metadata": {
    "id": "c45ab354"
   },
   "source": [
    "## 8. Completing Transformer Structure\n",
    "Now we'll put every components (Embeddings, positionale encoding,encoder layers and decoder layetrs) together. Finally, we will have built the structure from the original paper. (\"Attention is all you need\", 2017)\n",
    "\n",
    "\n",
    "![Source: Attention is all you need, 2017](Img/transformer/transformer_structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54963c5e",
   "metadata": {
    "id": "54963c5e"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        \"\"\"\n",
    "        Complete Transformer model\n",
    "\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            d_ff: Feed-forward hidden dimension\n",
    "            num_layers: Number of encoder/decoder layers\n",
    "            vocab_size: Size of vocabulary\n",
    "            max_seq_len: Maximum sequence length\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embeddings = nn.Embedding(_______,______) #Embedd the source information\n",
    "        self.decoder_embeddings = nn.Embedding(______,______) # Embed the target\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        self.encoder_layers = nn.ModuleList(______________________________________________)\n",
    "        self.decoder_layers = nn.ModuleList(______________________________________________)\n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.pos_encoding(_________________))\n",
    "        tgt_embedded = self.dropout(self.pos_encoding(__________________))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            ______________ = _______________________\n",
    "\n",
    "        output = self.linear(____________)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce10cdad",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "```python\n",
    "\n",
    "self.encoder_embeddings = nn.Embedding(src_vocab_size,d_model) #Embedd the source information\n",
    "self.decoder_embeddings = nn.Embedding(tgt_vocab_size,d_model) # Embed the target\n",
    "self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "self.encoder_layers = nn.ModuleList([TransformerEncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "self.decoder_layers = nn.ModuleList([TransformerDecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "self.softmax = nn.Softmax(dim=-1)\n",
    "self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "def forward(self, src, tgt):\n",
    "    src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "    src_embedded = self.dropout(self.pos_encoding(self.encoder_embeddings(src)))\n",
    "    tgt_embedded = self.dropout(self.pos_encoding(self.decoder_embeddings(tgt)))\n",
    "\n",
    "    enc_output = src_embedded\n",
    "    for enc_layer in self.encoder_layers:\n",
    "        enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "    dec_output = tgt_embedded\n",
    "    for dec_layer in self.decoder_layers:\n",
    "        dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "    output = self.linear(dec_output)\n",
    "    return output\n",
    "\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e69836",
   "metadata": {},
   "source": [
    "## Training the Transformer\n",
    "Just for demonstration purposes, we'll create a random data points and train the transformer with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac9e82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeac9e82",
    "outputId": "53fec798-2172-4a57-a30e-2007a1aeb313"
   },
   "outputs": [],
   "source": [
    "\n",
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(5): # Just for demonstration purposes\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f982a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7f982a2",
    "outputId": "c50f95d0-ab58-4ccd-cbbf-c1ebcaac31f6"
   },
   "outputs": [],
   "source": [
    "transformer.eval()\n",
    "\n",
    "# Generate random sample validation data\n",
    "val_src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))\n",
    "val_tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
    "    val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_data[:, 1:].contiguous().view(-1))\n",
    "    print(f\"Validation Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32209503",
   "metadata": {},
   "source": [
    "Further Reading\n",
    "- The Illustrated Transformer, https://jalammar.github.io/illustrated-transformer/\n",
    "- Transformers from scratch, https://e2eml.school/transformers.html\n",
    "- Attention is all you need, https://doi.org/10.48550/arXiv.1706.03762\n",
    "- Transformer Model Tutorial in PyTorch: From Theory to Code, https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "intro-pharma-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
