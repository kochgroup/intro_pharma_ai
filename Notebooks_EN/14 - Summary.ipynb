{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb6c2e50",
   "metadata": {},
   "source": [
    "# An overview\n",
    "\n",
    "---\n",
    "Learning Objectives\n",
    "\n",
    "- You understand the relationship between a (logistic) regression and neural networks.\n",
    "---\n",
    "\n",
    "In this notebook we will have a last look at different model architectures and how they are related.\n",
    "\n",
    "For this we will work again with the MNIST data set. First we load the data and normalize it. We also one-hot encode the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ace8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "def one_hot(x):\n",
    "    \"\"\"The labels of the images still need to be encoded into vectors of length 10\"\"\"\n",
    "    dod = len(set(x)) # Checks how many different digits there are in the data set\n",
    "    target = np.zeros([x.shape[0], dod]) # A matrix of zeros is created\n",
    "    for i in range(x.shape[0]): # The for-loop puts a 1 in the matrix depending on which label the image has\n",
    "        target[i, x[i]] = 1\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81def756",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt('../data/mnist/mnist_train.csv', delimiter=',', skip_header =False)  \n",
    "test_data=np.genfromtxt('../data/mnist/mnist_test.csv', delimiter=',', skip_header =False) \n",
    "\n",
    "train_labels=train_data[:,0].astype(int) \n",
    "train_images = train_data[:,1:]\n",
    "\n",
    "test_labels=test_data[:,0].astype(int)\n",
    "test_images = test_data[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e041a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets=one_hot(train_labels)\n",
    "test_targets = one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2935155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = min_max(train_images)\n",
    "test_images = min_max(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc937fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[0].reshape([28, 28]), cmap=\"gray\")\n",
    "print(\"Correct Label: %s\" % train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e54a43",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "We start with a simple regression. A linear regression can also be represented as a neural network.\n",
    "\n",
    "<img src=\"Img/summary/lin_reg.png\" width =\"450px\">\n",
    "\n",
    "The output is composed of the weighted sum of the pixel values. That is, each pixel is assigned a weight. *The neuron may also still have a bias associated with it, but this is not shown*. \n",
    "\n",
    "Since we only have one output neuron, we can only make a single prediction. This means that we can only do a binary classification. For example: Is there a five shown in the picture? YES or NO.\n",
    "\n",
    "We can also perform this linear regression in Python.\n",
    "To do this, we use the `train_images` as input and the column of `train_targets` which corresponds to the label `5`. In this case this is the fifth column `train_targets[:,5]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97faed1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(train_images, train_targets[:,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd898602",
   "metadata": {},
   "source": [
    "We can output the weights with `linear_reg_model.coef_`. There are 784 weights in total, one for each pixel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed115ac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linear_reg_model.coef_[:5], linear_reg_model.coef_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a832de9",
   "metadata": {},
   "source": [
    "To see how well our model performs, we can use the `.predict()` function to predict the value for our test data set. Remember, we only want to predict zeros or ones.\n",
    "\n",
    "`1` = \"Five\"\n",
    "\n",
    "`0` = \"Not a five\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e6db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = linear_reg_model.predict(test_images)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851eb70",
   "metadata": {},
   "source": [
    "These values are neither `0` nor `1`. We have to round them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303b62c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_y = np.round(pred_y)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e2aab8",
   "metadata": {},
   "source": [
    "Now we can calculate the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a94f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(pred_y== test_targets[:,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818d6b9d",
   "metadata": {},
   "source": [
    "`0.9456` is not so bad. But keep in mind that only about 10% of the images show a `5`. This also means that 90% of the images do not show a `5`. For these 90%, our model would have to predict a `0` to be correct. If the model simply predicts a `0` for all images, it would have an Accuarcy of `0.90`. So our accuracy may perform worse than originally thought.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbe7450",
   "metadata": {},
   "source": [
    "We have another problem. Take a look at the predictions for `pred_y[1677]` or `pred_y[1162]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ae3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y[1677],pred_y[1162]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8340240b",
   "metadata": {},
   "source": [
    "These values are neither `1` nor `0`. How could this happen?\n",
    "In a linear regression we do not use activation functions. Therefore, the output of a linear regression can take infinitely large or small values. If the values are outside the range `[-1.5, 1.5]`, they are not rounded to `0` or `1`.\n",
    "\n",
    "This is not a problem at first, we could assign `0` or `1` to these values manually. But the problem remains in principle: How do we prevent the model to predict values that are out of the possible range. \n",
    "\n",
    "A `sigmoid` function does the trick. It transforms all values so that they always lie between `0` and `1`. So we can simply \"attach\" a `sigmoid` function to the linear regression. This would solve the problem. And that is exactly what happens in logistic regression.\n",
    "\n",
    "<img src=\"Img/summary/log_reg.png\" width=\"450px\">\n",
    "\n",
    "We can also calculate this in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a8784",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model = LogisticRegression(solver = 'lbfgs', max_iter=1000,  random_state=134)\n",
    "log_reg_model.fit(train_images, train_targets[:,5])\n",
    "log_reg_model.coef_[0,256:261], log_reg_model.coef_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbceea4",
   "metadata": {},
   "source": [
    "We obtain again `784` weights. One for each pixel. We also see that our predictions for the testset set are now already rounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f174864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = log_reg_model.predict(test_images)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85190e3d",
   "metadata": {},
   "source": [
    "Again, we calculate the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07aa814",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(pred_y == test_targets[:,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c328659d",
   "metadata": {},
   "source": [
    "With the help of logistic regression, we could increase the accuracy. So far, however, we only distinguish between \"Five\" and \"Not a five\". But actually we want to be able to recognize every digit. This is also possible with a logistic regression.\n",
    "\n",
    "For this to work we need multiple output nodes. Ten in total, one for each digit. \n",
    "\n",
    "<img src=\"Img/summary/log_reg_2.png\" width=\"540px\">\n",
    "\n",
    "We now use the `softmax` function. Unlike the `sigmoid` function, the `softmax` function ensures that the sum of activations over the 10 outputs is always exactly `1`. If we were to use the `sigmoid` function, it could happen that an image is detected as a five and a one. \n",
    "\n",
    "\n",
    "For this logistic regression we now need to add the complete `train_labels` matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c3aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_complete = LogisticRegression(solver = 'lbfgs', max_iter=1000,  random_state=134)\n",
    "log_reg_model_complete.fit(train_images, train_labels)\n",
    "log_reg_model_complete.coef_[0,256:261], log_reg_model_alle.coef_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152f511",
   "metadata": {},
   "source": [
    "The weight matrix `log_reg_model_all.coef_` has now the size `[10,784]`. So each output neuron has 784 weights\n",
    "\n",
    "We also obtain predictions, which contain the digit recognized by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5432398e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_y = log_reg_model_complete.predict(test_images)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d15d07b",
   "metadata": {},
   "source": [
    "We calculate again the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628ce0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(pred_y==test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84c600a",
   "metadata": {},
   "source": [
    "The model can correctly recognize 92.5% of the digits. This is of course worse than before, but this time the task is much more complex, because it is not only about one digit, but all digits need to be recognized. \n",
    "With a simple logistic regression we can achieve a relatively good accuracy. \n",
    "\n",
    "So why do we need neural networks? These can also give us the last percentage points of performance. The difference between our current model and a neural network is the lack of hidden layers. \n",
    "\n",
    "<img src=\"Img/summary/nn1.png\" width=\"450px\">\n",
    "\n",
    "We can add hidden layers using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514cd1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_images =torch.tensor(train_images, dtype = torch.float32)\n",
    "test_images =torch.tensor(test_images, dtype = torch.float32)\n",
    "\n",
    "train_labels =torch.tensor(train_labels, dtype = torch.long)\n",
    "test_labels =torch.tensor(test_labels, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404a7ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn = nn.Sequential(nn.Linear(784,10),nn.ReLU() ,nn.Linear(10,10))\n",
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updater = optim.Adam(simple_nn.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b6d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "for epoch in range(135):\n",
    "    updater.zero_grad()\n",
    "    output = simple_nn(train_images)\n",
    "    loss = loss_funktion(output, train_labels)\n",
    "    loss.backward()\n",
    "    updater.step()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cae5cfb",
   "metadata": {},
   "source": [
    "The code should be familiar to you by now. However, if we look at the accuracy, we see that the neural network performance comparable to that of the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b00c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y=torch.argmax(simple_nn(test_images),1).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e5c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(pred_y==test_labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c48b226",
   "metadata": {},
   "source": [
    "This can have several reasons. In principle, neural networks do not have to work better than simpler models.\n",
    "In this case, however, it is probably due to the network itself. We could use more or larger layers or we change the optimizer or the learning rate to imrpove the performance of the network.\n",
    "\n",
    "\n",
    "\n",
    "# Exercise\n",
    "\n",
    "\n",
    "Today's notebook is shorter than usual, so you have more time for the excercise. \n",
    "Today's exercise is about applying what you have learned to the MNIST dataset again. \n",
    "\n",
    "You will be given three data sets (randomly shuffled):\n",
    "\n",
    "- Training data: use it to train the model\n",
    "- Test data: use it to evaluate the trained network\n",
    "- External test dataset: images only, no labels â†’ you send me the predictions for this dataset.\n",
    "\n",
    "The external dataset has no labels (at least none that you can see). \n",
    "In the exercise task, you will also hand in **your** predictions for the external dataset.\n",
    "\n",
    "We will then compare your predictions to the true values. \n",
    "*Which of you creates the best model?*\n",
    "\n",
    "An initial model has been given.\n",
    "From there, you can improve the network.\n",
    "\n",
    "There are several ways to improve it:\n",
    "Here are a few examples.\n",
    "\n",
    "- Adjust hyperparameters, e.g. number of epochs, batch size, learning rate or number of hidden layers.\n",
    "- Batchnorm and dropout\n",
    "- use of a CNN\n",
    "- Optimizers\n",
    "\n",
    "Be careful not to overfit to the test dataset. This can also happen.\n",
    "\n",
    "At the end of the notebook is a cell that you can use to create and save the prediction for the test data set. \n",
    "This will be saved in the `data` folder as `my_prediction.csv`.\n",
    "\n",
    "Please submit both your prediction and the notebook.\n",
    "\n",
    "\n",
    "# Data \n",
    "\n",
    "First load all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc2eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils import data\n",
    "import pandas as pd\n",
    "\n",
    "def min_max(x):\n",
    "    return (x - 0.) / (255. - 0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cecd01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt('../data/mnist/mnist_train.csv', delimiter=',', skip_header =False) \n",
    "train_labels = train_data[:,0].astype(int) \n",
    "train_images = min_max(train_data[:,1:])\n",
    "del train_data \n",
    "\n",
    "test_data = np.genfromtxt('../data/mnist/mnist_test.csv', delimiter=',', skip_header =False) \n",
    "test_labels = test_data[:,0].astype(int)\n",
    "test_images = min_max(test_data[:,1:])\n",
    "del test_data \n",
    "\n",
    "external_images = min_max(np.genfromtxt('../data/mnist/external.csv', delimiter=',', skip_header =False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b4079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = torch.tensor(train_images, dtype = torch.float32)\n",
    "test_images = torch.tensor(test_images, dtype = torch.float32)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype = torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype = torch.long)\n",
    "\n",
    "external_images = torch.tensor(external_images, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49e5d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.TensorDataset(train_images, train_labels) \n",
    "loader = data.DataLoader(train_data, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc0c6e8",
   "metadata": {},
   "source": [
    "##  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn = nn.Sequential(nn.Linear(784,10),nn.ReLU() ,nn.Linear(10,10))\n",
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updater = optim.Adam(simple_nn.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ffd5fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "for epoch in range(20):\n",
    "    simple_nn.train()\n",
    "    for images, labels in loader:\n",
    "        updater.zero_grad()\n",
    "        output = simple_nn(images)\n",
    "        loss = loss_funktion(output, labels)\n",
    "        loss.backward()\n",
    "        updater.step()\n",
    "    \n",
    "    simple_nn.eval()\n",
    "    # EVALUATE #\n",
    "    # Train\n",
    "    output = simple_nn(train_images)\n",
    "    loss = loss_funktion(output, train_labels)\n",
    "    prediction = torch.argmax(output,1).detach().numpy()\n",
    "    acc  = np.mean(prediction == train_labels.detach().numpy()  )\n",
    "    # Tets\n",
    "    output = simple_nn(test_images)\n",
    "    test_loss = loss_funktion(output, test_labels)\n",
    "    prediction = torch.argmax(output,1).detach().numpy()\n",
    "    test_acc  = np.mean(prediction == test_labels.detach().numpy()  )\n",
    "    print(f\"Epoch {epoch} | Trainings Loss: {loss:.3f} Training Acc: {acc:.3f} | Test Loss: {test_loss:.3f} Test Acc:  {test_acc:.3f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3baff30",
   "metadata": {},
   "source": [
    "# Externe Daten "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d391273",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn.eval()\n",
    "externe_pred = torch.argmax(simple_nn(external_images),1).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8261ba",
   "metadata": {},
   "source": [
    "The next cell generates a `.csv` file with your predictions. Please submit this with the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f8fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(externe_pred.reshape(10000,1)).to_csv(\"../data/my_prediction.csv\", index =False,header =False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
