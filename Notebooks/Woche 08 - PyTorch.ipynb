{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://venturebeat.com/wp-content/uploads/2019/06/pytorch-e1576624094357.jpg?w=1200&strip=all\" style=\"width: 800px;\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Lernziele\n",
    "* Sie sind in der Lage, ein einfaches neuronales Netz mit PyTorch zu programmieren.\n",
    "* Sie sind in der Lage, fortgeschrittene Schichten von Neuronen in Ihr Netz einzubauen (Dropout, Batchnorm).\n",
    "* Sie lernen fortgeschrittene Optimierungen (Momentum, adam).\n",
    "---\n",
    "\n",
    "Letzte Woche haben Sie selbst ein einfaches neuronales Netz programmiert. Wie bereits erwähnt, ist es nicht notwendig, jedes Netz selbst zu programmieren. Bestimmte Softwarepakete übernehmen viele der Unannehmlichkeiten beim Erstellen und Trainieren von Netzen.\n",
    "\n",
    "Im Wesentlichen gibt es zwei Libaries, die verwendet werden können: PyTorch und TensorFlow. TensorFlow wird von Google entwickelt. TensorFlow ist die populärere Wahl, besonders in der Industrie. PyTorch hingegen wird vor allem in der wissenschaftlichen Welt verwendet. Grundsätzlich gilt PyTorch als das einfacher zu erlernende Framework und ist insgesamt ein wenig benutzerfreundlicher. \n",
    "\n",
    "Während es früher große Unterschiede gab, ähneln sich die beiden Libraries heute in ihrer Funktionalität immer mehr.\n",
    "\n",
    "Schließlich gibt es noch Keras und PyTorch Lightning. Beide zielen darauf ab, die Erstellung neuronaler Netze noch einfacher zu machen. Keras verwendet TensorFlow im Hintergrund, erleichtert aber das Training von Netzen, insbesondere für Anfänger. Das Gleiche gilt für PyTorch Lightning und PyTorch.\n",
    "\n",
    "In der Chemieinformatik ist PyTorch jedoch eine gute Wahl, da spezielle Libraries wie z.B. für Graph Neural Networks nur für PyTorch existieren bzw. existierten.\n",
    "\n",
    "\n",
    "Ein wesentlicher Bestandteil von PyTorch ist **autograd**. Autograd ist eine Libraries, die, wie der Name schon sagt, die Gradienten automatisch berechnen und sammeln kann. So müssen Sie die Gradienten nicht selbst berechnen.\n",
    "Außerdem gibt es viele Funktionen, wie Aktivierungsfunktionen oder lineare Transformationen, die bereits in PyTorch implementiert sind. \n",
    "\n",
    "*TensorFlow besitzt diese Funktionalitäten natürlich auch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "Während Sie bisher mit `numpy`-Arrays gearbeitet haben, werden wir heute `tensors` verwenden, genauer gesagt PyTorch `tensors`. \n",
    "\n",
    "**Was ist der Unterschied?**\n",
    "\n",
    "Zunächst einmal keiner. Arrays und Tensoren sind sich in vielerlei Hinsicht ähnlich. Beide speichern Zahlen/Werte in einer strukturierten Form. So kann man in NumPy Matrizen in einem 2D-Array speichern, aber man kann die gleiche Matrix auch in einem 2D-Tensor speichern.\n",
    "Außerdem können Tensoren in Arrays und Arrays in Tensoren konvertiert werden.\n",
    "\n",
    "Der Unterschied zwischen den beiden \"Speicheroptionen\" ist, dass PyTorch Tensor von PyTorch entwickelt wurden. Und NumPy Arrays wurden von den Entwicklern von NumPy entwickelt. \n",
    "Viele Funktionen, die NumPy anbietet, sind auch bei PyTorch für deren Tensoren verfügbar (werden aber möglicherweise anders genannt). \n",
    "PyTorch hat seine Tensoren entwickelt, um mathematische Operationen schneller durchzuführen. Außerdem können Tensoren auch auf die Grafikkarte \"geladen\" werden, was die Geschwindigkeit der Operationen um ein Vielfaches erhöht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Rechnen mit `tensors` ist fast identisch zu dem Rechnen mit `np.arrays`. Aber die Funktionen können unterschiedliche Namen haben. Zum Beispiel ist `torch.mm()` die Funktion für die Matrixmultiplikation und `.t()` ist die Transponierung einer Matrix. Ähnlich wie `np.array()` Arrays erzeugt, erzeugt `torch.tensor()` Tensoren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # lädt PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[1,2,3],\n",
    "                [4,5,6]])\n",
    "\n",
    "W = torch.tensor([[8,9,10],\n",
    "                 [11,12,313]])\n",
    "\n",
    "b = torch.tensor([1,2])\n",
    "\n",
    "torch.mm(X,W.t())+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist die lineare Transformation, die Sie auch schon kennen.<br>\n",
    "PyTorch vereinfacht diesen Schritt jedoch. \n",
    "So gibt es in PyTorch ein Modul namens `nn`, dieses enthält viele Funktionen, die bei der Erstellung von neuronalen Netzen hilfreich sind.\n",
    "\n",
    "Wir können das Modul `nn` mit `from torch import nn` laden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronales Netzwerk mit PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch `nn` bietet unter anderem die Funktion `nn.Linear`. Sie führt die lineare Transformation $xW^T +b $ durch.\n",
    "Als Input nimmt die Funktion:\n",
    "\n",
    "\n",
    "* `in_features` die Anzahl der Features, die der Input vor der Transformation hat, oder die Größe der Inputlayer. Gestern hatten die Bilder 784 Pixel, also 784 Features\n",
    "* `out_features` die Anzahl der Features, die der Input nach der Transformation haben soll. Also definiert `out_features` die Größe der Hidden Layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = nn.Linear(in_features = 784, out_features=300, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fehlt den nicht der Input für die Layer?\n",
    "\n",
    "Stimmt, bis jetzt haben Sie auch keine lineare Transformation durchgeführt, sondern nur eine Variable  `layer_1` erstellt. Diese kann dann die lineare Transformation für uns durchführen.\n",
    "\n",
    "---\n",
    "*Technisch gesehen ist `nn.Linear` keine Funktion, sondern eine Klasse (`class`). Klassen sind besondere Python Objekte. Wie genau Klassen funktionieren, ist nicht relevant für diesen Kurs. Wichtig zu verstehen ist, dass*\n",
    "\n",
    "```\n",
    "layer_1 = nn.Linear(in_features = 784, out_features=300, bias=True)\n",
    "```\n",
    "*ein Objekt `layer_1` erstellt welches der Klasse `nn.Linear` zugehört. Jede Klasse in Python kann eigenen Funktionen haben. Zum Beispiel, haben die meisten `nn` Klassen eine `forward` Funktion die einen bestimmten Forward Pass ausführt.*\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Praktisch an `nn` Layer ist, dass die Weights dieser Layer automatisch von PyTorch initialisiert werden. Das nimmt Ihnen bereits ein wenig Arbeit ab.\n",
    "Die Gewichte $W$ dieser Schichten können auch eingesehen werden.\n",
    "\n",
    "Dazu verwendet man `list(layer_1.parameters())[0]`.\n",
    "Wenn Sie die genaue Größe der Gewichtsmatrix wissen wollen, können Sie `.shape` wie NumPy verwenden: `list(layer_1.parameters())[0].shape`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(layer_1.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(layer_1.parameters())[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie sehen, hat die Weightmatrix die gleiche Größe wie die Matrix der letzten Woche. Sie können auch sehen, dass die Matrix tatsächlich Weights enthält.\n",
    "Alles, was Sie jetzt brauchen, ist ein Input (Bilder), die Sie mit dieser linearen Transformation verändern wollen. \n",
    "Dazu wird der Trainingsdatensatz von letzter Woche mit `numpy` geladen.\n",
    "\n",
    "Zusätzlich müssen die Bilder in einen Tensor umgewandelt werden. Dazu benutzen Sie `torch.tensor()`.\n",
    "\n",
    "\n",
    "Natürlich müssen Sie die Daten wieder skalieren. Dazu verwenden Sie den Min-Max-Scaler.\n",
    "In PyTorch muss man den Datentypen mehr Aufmerksamkeit schenken. \n",
    "Deshalb definieren wir den Datentyp `dtype`. Der Datentyp für unsere Bilder ist `float32`. Sie kennen `float` von gestern, die `32` definiert, wie genau diese Zahl sein kann.\n",
    "`long` wird Ihnen vielleicht nichts sagen, aber es bezeichnet einfach `integers`.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Besonders Interessierte HIER KLICKEN:</strong></summary>\n",
    "\n",
    "Im letzten Notebook haben wir diskutiert, warum wir die Weightmatrix auf diese bestimmte Weise initialisieren.\n",
    "Tatsächlich werden in TensorFlow die Weightmatrizen so erstellt, dass eine Matrixmultiplikation ohne `transpose` durchgeführt werden kann.\n",
    "\n",
    "    \n",
    "Hier ist die Beschreibung des Codes für den Forwardpass von Tensorflow.\n",
    "    \n",
    "```\n",
    "  Dense` implements the operation:\n",
    "  `output = activation(dot(input, kernel) + bias)`\n",
    "  where `activation` is the element-wise activation function\n",
    "  passed as the `activation` argument, `kernel` is a weights matrix\n",
    "  created by the layer, and `bias` is a bias vector created by the layer\n",
    "  (only applicable if `use_bias` is `True`). These are all attributes of\n",
    "  `Dense`.\n",
    "```\n",
    "PyTorchs Beschreibung kann [hier](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) gefunden werden.\n",
    "    \n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "\n",
    "train_data = np.genfromtxt('../data/mnist/mnist_train.csv', delimiter=',', skip_header =False) #genfromtxt reads .txt files if we chose delimiter =\",\" the function can read also .csv files  (comma seperated values)\n",
    "\n",
    "train_images = min_max(train_data[:,1:])\n",
    "train_images = torch.tensor(train_images, dtype = torch.float32)\n",
    "train_labels=torch.tensor(train_data[:,0].astype(int), dtype = torch.long) \n",
    "\n",
    "\n",
    "test_data = np.genfromtxt('../data/mnist/mnist_test.csv', delimiter=',', skip_header =False) #genfromtxt reads .txt files if we chose delimiter =\",\" the function can read also .csv files  (comma seperated values)\n",
    "\n",
    "test_images = min_max(test_data[:,1:])\n",
    "test_images = torch.tensor(test_images, dtype = torch.float32)\n",
    "test_labels=torch.tensor(test_data[:,0].astype(int), dtype = torch.long) \n",
    "\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Datensatz umfasst 60000 Bilder mit je 784 Pixeln.\n",
    "Diese können Sie nun als Input für die lineare Transformation verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_1=layer_1(train_images)\n",
    "print(z_1)\n",
    "z_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `layer_1` gibt den Output (`z_1`) aus. Diese hat die Form `[60000,300]`. Also immer noch 60000 Bilder, aber dieses Mal hat jedes nur 300 Merkmale (Größe der versteckten Schicht). Wie es auch bei der Definition von `layer_1` festgelegt wurde.\n",
    "\n",
    "\n",
    "Was Ihnen auffallen sollte, ist, dass am Ende des Tensors `z_1` `grad_fn=<AddmmBackward>` steht. Man kann erkennen, dass *autograd* die Gradienten für diese Transformation erfasst hat. Sie können auch sehen, dass wir eine Matrixmultiplikation `mm` und eine Addition `Add` durchgeführt haben. Es wird immer die zuletzt durchgeführte Transformation des Tensors angezeigt.\n",
    "\n",
    "Was Ihnen jetzt noch fehlt, ist die Aktivierungsfunktion. Auch hier kann PyTorch helfen. \n",
    "Die `nn`-Library von PyTorch hat ein Untermodul `functional`. Hier sind viele zusätzliche mathematische Funktionen enthalten, unter anderem die `relu` und `sigmoid` Funktionen. Da die `relu`-Funktion in der Praxis noch bessere Ergebnisse liefert als die `sigmoid`-Funktion, werden wir sie jetzt verwenden.<br>\n",
    "\n",
    "`functional` lässt sich wie folgt importieren: `from torch.nn import functional as F`. Wir nennen `functional` zu `F` um, quasi eine Norm, wenn man mit PyTorch arbeitet.\n",
    "\n",
    "`F.relu()` kann jetzt benutzt werden um die ReLU Funktion anzuwenden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "a_1 = F.relu(z_1)\n",
    "print(a_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn Sie `a_1` mit `z_1` vergleichen, können Sie sehen, dass alle Werte, die vorher negativ waren, zu Null wurden und alle Werte, die positiv waren, unverändert sind.\n",
    "Auch kann man in der `grad_fn` sehen, dass eine ReLU angewendet wurde. Auch dies wurde von *autograd* aufgezeichnet. \n",
    "\n",
    "Der erste Teil des Forward Pass ist damit erledigt. \n",
    "Für den zweiten Schritt können wir einfach eine weitere Layer erstellen, die `a_1` als Eingabe erhält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2 = nn.Linear(300,10) # 10 ist die Anzahl der out_features, da wir 10 Ziffern haben.\n",
    "z_2=layer_2(a_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wieder brauchen Sie eine Aktivierungsfunktion. Aber dieses Mal die `softmax`-Funktion, um die Wahrscheinlichkeiten zu erhalten.\n",
    "Die Funktion `nn.functional` hat auch eine `softmax()`-Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = F.softmax(z_2,dim=1)# der dim Parameter legt fest ob über Spalten oder Reihen die Softmax funktion angewandt wird.\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch können Sie auch verschiedene Schichten kombinieren. Mit `nn.Sequential` können Sie die lineare Transformation und die Aktivierungsfunktion direkt nacheinander schalten. Die Eingabe wird automatisch durch jede der Layers geleitet.\n",
    "Das macht deinen Code übersichtlicher und einfacher zu schreiben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netzwerk = nn.Sequential(nn.Linear(784,300), \n",
    "                         nn.ReLU(), \n",
    "                         nn.Linear(300,10))\n",
    "netzwerk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie sehen können, wurde ein Netzwerk mit einer Hidden Layer erstellt. Was Ihnen auffallen sollte, ist, dass anstelle von `F.relu` `nn.ReLU` verwendet wurde. Wenn eine `relu`-Funktion innerhalb von `Sequential()` verwendet werden soll, müssen Sie immer `nn.ReLU` verwenden. \n",
    "\n",
    "Das `netzwerk` kann nun die Bilder klassifizieren:\n",
    "Mit `network(input)` kann man dInput, z.B. unsere Bilder, durch das Netzwerk leiten.\n",
    "Anhand der Tensorgröße der Ausgabe kann man sehen, dass am Ende tatsächlich 60000 Bilder mit je 10 Merkmalen (die 10 Ziffern) als Output ausgegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = netzwerk(train_images)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine weitere Änderung ist, dass Sie nicht mehr die letzte Aktivierungsfunktion verwenden. PyTorch wählt die Aktivierungsfunktion automatisch aus. Die Entscheidung, welche Aktivierungsfunktion in der letzten Schicht zu verwenden ist, hängt von der Wahl der Lossfunktion ab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Funktion\n",
    "\n",
    "\n",
    "`nn` kann auch bei der Lossfunktion helfen. Die gängigsten Lossfunktionen sind bereits in PyTorch enthalten.\n",
    "Sie können einfach eine neue Variable erstellen und ihr die Funktion `nn.CrossEntropyLoss()` zuweisen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funktion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `loss_function` kann nun den Loss berechnen, indem sie automatisch die Softmax-Funktion anwendet.\n",
    "Dazu muss man nur `y_hat` und die `train_labels` in die Funktion eingeben. Hier sieht man ein weiterer Vorteil von Pytorch: man  muss die Labels nicht noch `one-hot` kodieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_funktion(output, train_labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation\n",
    "\n",
    "Der letzte Schritt besteht darin, eine Backpropagation durchzuführen. Dank *autograd* ist dies einfach mit dem Befehl `loss.backward()` möglich. Er berechnet die Gradienten für alle Weightmatrizen.\n",
    "\n",
    "Danach müssen Sie nur noch die Weightmatrizen aktualisieren. Wie Sie sich vorstellen können, kümmert sich PyTorch auch um diese Aufgabe.\n",
    "PyTorch bietet sogar eine Vielzahl von  unterschiedlichen Algorithmen, welche die Weight auf unterschiedliche Weise aktualisieren.\n",
    "\n",
    "Für das Aktualisieren der Gewichte wird ein neues Modul von PyTorch benötigt.\n",
    "Dazu lädt man `from torch import optim`. `optim` enthält Optimierer, Funktionen, die das Netz für uns optimieren - also die Weights aktualisieren.\n",
    "\n",
    "Ähnlich wie bei der Lossfunktion kann man einfach eine Variable erstellen und ihr eine Updatefunktion zuweisen. \n",
    "Sie können nun die Funktion `optim.SGD()` zur Aktualisierung der Weights verwenden. SGD = Stochastic Gradient Descent.  In der Funktion selbst legen Sie fest, welche Parameter (Weights) verändert werden sollen. Außerdem legen Sie hier die Lernrate fest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward() #sammelt die Gradienten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "weights_updaten=optim.SGD(netzwerk.parameters(), lr=0.01) # Sie legen fest welche Parameter und mit welche Lernrate diese verändert werden sollen\n",
    "\n",
    "weights_updaten.step()  # mit- step() updaten Sie die Weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt haben Sie alles, was Sie brauchen, um ein Netz zu trainieren.\n",
    "\n",
    "Sie können wieder einen `for-loop` verwenden, um das Training zu automatisieren.\n",
    "\n",
    "Sie werden feststellen, dass wir zusätzlich noch `updata.zero_grad()` verwenden. Diese Funktion wird verwendet, um die Gradienten des vorherigen Updates zu löschen. Wenn dies nicht geschieht, würde der Optimierer alle Gradienten aus allen Epochen konstant summieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definieren von Netzwerk, LossFunktion und Update Algorithmus\n",
    "netzwerk = nn.Sequential(nn.Linear(784,300), \n",
    "                         nn.ReLU(), \n",
    "                         nn.Linear(300,10))\n",
    "\n",
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updaten = optim.SGD(netzwerk.parameters(), lr=0.3)\n",
    "EPOCHS = 50\n",
    "\n",
    "## Trainings Loop\n",
    "for i in range(50):\n",
    "    updaten.zero_grad()\n",
    "    output = netzwerk(train_images) # Forward Propagation\n",
    "    \n",
    "    loss   = loss_funktion(output, train_labels)\n",
    "    loss.backward()\n",
    "    acc=((output.max(dim=1)[1]==train_labels).sum()/float(output.shape[0])).item()\n",
    "    print(i, \n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss.item(), acc)\n",
    "    )\n",
    "    \n",
    "    updaten.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie sehen, dass Sie ein neuronales Netz mit viel weniger Aufwand trainieren können. Sie können auch ohne großen Aufwand eine zweite oder dritte Hidden Layer zu Ihrem Modell hinzufügen.\n",
    "Fügen Sie einfach eine `nn.ReLU`- und eine `nn.Linear`-Schicht in `Sequential` ein, und *autograd* kann die Gradienten auch für diese Schichten berechnen. Alles andere bleibt gleich. Denken Sie nur daran, dass die Dimensionen von einer Layer zur nächsten passen müssen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definieren von Netzwerk, Loss Funktion und Update Algorithmus\n",
    "netzwerk = nn.Sequential(nn.Linear(784,300), \n",
    "                         nn.ReLU(), \n",
    "                         nn.Linear(300,300),# <----- EXTRA LAYER\n",
    "                         nn.ReLU(), \n",
    "                         nn.Linear(300,10))\n",
    "print(netzwerk)\n",
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updaten = optim.SGD(netzwerk.parameters(), lr=0.3)\n",
    "EPOCHS = 50\n",
    "\n",
    "## Trainings Loop\n",
    "for i in range(50):\n",
    "    updaten.zero_grad()\n",
    "    output = netzwerk(train_images) # Forward Propagation\n",
    "    \n",
    "    loss   = loss_funktion(output, train_labels)\n",
    "    loss.backward()\n",
    "    \n",
    "    acc=((output.max(dim=1)[1]==train_labels).sum()/float(output.shape[0])).item()\n",
    "    print(i,\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss.item(), acc)\n",
    "    )\n",
    "    \n",
    "    updaten.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie haben vielleicht bemerkt, dass wir den Stochastic Gradient Descent als Optimierer verwenden (Weights aktualisieren). Bislang haben wir nur über den Gradient Descent gesprochen.\n",
    "Tatsächlich wird der Gradient Descent, wie gestern erklärt, eigentlich nicht mehr verwendet, sondern alternativ der Stochastic Gradient Descent.\n",
    "\n",
    "Der Unterschied:<br>\n",
    "Im Stochastic Gradient Descent wird der Datensatz nicht auf einmal durch das Netz geschickt, sondern der Datensatz wird in kleineren Teilen (**minibatch**) durch das Netzwerk geführt. <br>\n",
    "In diesem Datensatz gibt es insgesamt 60000 Bilder. Beim Gradient Descent, wird der Forwardpass gleichzeitig mit 60000 Bilder durchgeführt, und für die 60000 Bilder wird gleichzeitig der Loss berechnet. Danach werden die Weights **einmal** upgedatet.\n",
    "Dann wiederholt sich der Schritt. \n",
    "\n",
    "Es wäre effizienter, wenn nicht nach allen 60000 Bilder ein Update stattfinden würde. Sondern schon nach 200 oder sogar nur 100, so dass das Netz viel schneller lernen kann.\n",
    "Genau das macht der Stochastic  Gradient Descent.  Nicht alle Bilder, sondern nur z.B. 32 Bilder werden auf einmal durch das Netz geschickt. Für diese 32 Bilder wird dann der Loss berechnet und die Updates werden durchgeführt.\n",
    "Dann wird dieser Schritt wiederholt, aber diesmal mit 32 neuen Bildern. Auf diese Weise können die Weights innerhalb einer Epoche sehr viel häufiger aktualisiert werden.\n",
    "\n",
    "Die Batchsize gibt an, wie groß ein Minibatch (der kleine Teil der Daten, der durch das Netz geführt wird) sein soll, und kann ebenfalls die Performance des Modells beeinflussen.\n",
    "<center>\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcT0TVrYkk0A0FfPvnzYTe747F0qPLG2rU2Bmg&usqp=CAU\" style=\"width: 600px;\">\n",
    "</center>\n",
    "<h8><center>Source: Insu Han and Jongheon Jeong, http://alinlab.kaist.ac.kr/resource/Lec2_SGD.pdf</center></h8>\n",
    "\n",
    "\n",
    "Vorteile:<br>\n",
    "* schneller\n",
    "* braucht weniger Speicherplatz (auf der Graphikkarte)\n",
    "\n",
    "Nachteile: <br>\n",
    "* kann nicht das Optimum finden (kann aber auch gut sein, um overfitting zu verhindern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Vorteile des Stochastic Gradient Descent nutzen zu können, müssen wir die Daten zunächst in Minibatches aufteilen. Auch hierfür kann man PyTorch verwenden, die entsprechenden Funktionen sind im Submodul `torch.utils.data` verfügbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `torch.utils.data` gibt es zwei Funktionen, die Sie benötigen:\n",
    "\n",
    "* `data.TensorDataset(input,labels)` erzeugt einen PyTorch-Dataset aus den Daten.\n",
    "* `data.DataLoader(Dataset, batch_size)` erzeugt Minibatches der angegebenen Größe aus einem PyTorch Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.TensorDataset(train_images, train_labels) # input sind unsere Tensors die einmal die Bilder und einmal die Labels beinhalten\n",
    "loader = data.DataLoader(train_data, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Variable `loader` enthält nun 1875 Minibatches mit jeweils 32 Bildern und deren 32 Beschriftungen. In der nächsten Zelle können Sie den Inhalt des ersten Batches sehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(loader)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um alles zusammenzubringen, braucht man einen zweiten `for-loop`, der die Minibatches einen nach dem anderen innerhalb des ersten `for-loops` auswählt und sie durch das Netzwerk führt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definieren von Netzwerk, LossFunktion und Update Algorithmus\n",
    "netzwerk = nn.Sequential(nn.Linear(784,300), \n",
    "                         nn.ReLU(), \n",
    "                         nn.Linear(300,300),\n",
    "                         nn.ReLU(), \n",
    "                         nn.Linear(300,10))\n",
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updaten = optim.SGD(netzwerk.parameters(), lr=0.3)\n",
    "EPOCHS = 2\n",
    "\n",
    "## Trainings Loop\n",
    "for i in range(EPOCHS):\n",
    "    loss_list = [] # diese Liste speichtert den Loss jedes Minibatches\n",
    "                   # damit können wir am Ende den Durschnittslosd innerhalb des Epochs berechnen\n",
    "    for minibatch in loader: # for-loop geht durch alle Minibatches\n",
    "        images, labels = minibatch # minibatch wird in Bilder und Labels geteilt\n",
    "        \n",
    "        updaten.zero_grad()\n",
    "        output = netzwerk(images) # Forward Propagation\n",
    "    \n",
    "        loss   = loss_funktion(output, labels)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        updaten.step()\n",
    "        \n",
    "    output = netzwerk(train_images)\n",
    "    acc=((output.max(dim=1)[1]==train_labels).sum()/float(output.shape[0])).item()\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), acc)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach nur zwei Epochen ist die Accuracy viel höher als je zuvor. Ein einzelner Epoch dauert viel länger im Vergleich zu \"normalen\" Gradient Descent , aber die insgesamte Trainingszeit wird reduziert.\n",
    "\n",
    "Zur Berechnung der Accuracy wird der gesamte Datensatz, nachdem alle Minibatches das Netzwerk durchlaufen haben, erneut durch das Netzwerk geschickt (ohne die Gewichte zu ändern). Die Accuracy wird dann auf der Grundlage dieser Vorhersagen berechnet, Der Epochloss ist der durchschnittliche Loss der Minibatches.\n",
    "\n",
    "Tipp: Wenn Sie den Wert eines Tensors und nicht den Tensor selbst kopieren/ausgeben wollen, können Sie `x.item()` verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fortgeschrittene Layers\n",
    "\n",
    "\n",
    "Im Folgenden werden wir uns mit den neuen Layers befassen, die man neben linearen Layers verwendet.\n",
    "\n",
    "## Dropout\n",
    "\n",
    "Dropout wird während des Trainings verwendet, um einzelne Neuronen nach dem Zufallsprinzip aus dem Netz *zu entfernen*. Mit anderen Worten, sie werden für eine kurze Zeit ausgeschaltet. Mathematisch gesehen bedeutet dies, dass ihre Output einfach auf Null gesetzt wird.\n",
    "Jedes Mal, wenn ein Batch das Netz durchläuft, wird der Output von zufällig ausgewählten Neuronen auf Null gesetzt. \n",
    "\n",
    "Diese zufällige temporäre *Löschung*  von Neuronen zwingt das Netz, sich nicht auf einzelne Neuronen zu verlassen. Ähnlich wie bei Random Forest, wo zufällig Variablen entfernt werden, verhindert Dropout ein *overfitting* .\n",
    "Während der Validierung des Netzwerkes (Valideriungs- oder Testset), werden jedoch keine Neuronen mehr entfernt. Das heißt, die Dropout-Layer wird übersprungen.\n",
    "\n",
    "Wie viele Neuronen aus dem Netzwerk entfernt werden, ist ein weiterer Hyperparameter, der ebenso wie die Lernrate von Ihnen gewählt werden kann. Der Dropout wird in Prozent angegeben. Ein Dropout von `0,8` bedeutet also, dass in dieser Layer 80% der Neuronen entfernt werden bzw. ihr Output auf Null gesetzt wird. Ein Standardwert für den Dropout ist `0.2`. Oft wird eine Dropout-Layer auch direkt nach dem Input verwendet.\n",
    "\n",
    "In PyTorch wird eine Dropout-Layer wie folgt definiert: `nn.Dropout(0.2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1235)\n",
    "\n",
    "beispiel_x = torch.tensor([[1.,2.,3.,4.,5.]] )\n",
    "do = nn.Dropout(0.5)\n",
    "do(beispiel_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drei der Werte wurden auf `0` gesetzt, aber die anderen Werte haben sich verdoppelt. **Warum ist das passiert?**\n",
    "\n",
    "Das liegt daran, dass wir mit Dropout trainieren, aber ohne Dropout evaluieren. Das bedeutet, dass bei einem Dropout von \"`0.5` nur die Hälfte der Neuronen verwendet wird. Bei dem Forward Pass werden die Werte als gewichtete Summe weitergegeben. Wenn die Hälfte der Neuronen den Wert `0` hat, ist die Summe natürlich viel kleiner als in einem Netz, in dem kein Dropout verwendet wird.I m Laufe des Trainings wird die Skala der Weights an die erwartete Skala der Inputs angepasst.\n",
    "\n",
    "Ohne Dropout: $$$z = \\beta_0 + \\beta_11 + \\beta_22 +\\beta_33 +\\beta_44 +\\beta_55$$\n",
    "\n",
    "Mit Dropout: $$\\begin{align}z&= \\beta_0 + \\beta_10 + \\beta_22 +\\beta_30 +\\beta_44 +\\beta_50 \\\n",
    "&=\\beta_0 + \\beta_22+\\beta_44\\end{align}$$\n",
    "\n",
    "\n",
    "Die Summe $z$ mit Dropout ist immer kleiner als die Summe ohne Dropout. Indem wir die Größe der weitergeleiteten Inputs erhöhen, stellen wir sicher, dass sich die Weights nicht an die falsche Skala der Inputs anpassen.\n",
    "\n",
    "Denn bei der Evaluierung, bei der kein Dropout verwendet wird, haben wir plötzlich doppelt so viele Inputs. Diese Diskreptanz zwischen Training und Evaluierung wird damit vorgebeugt.\n",
    "\n",
    "Sie können eine Schicht oder ein komplettes Netz vom Trainings- in den Evaluierungsmodus wechseln, indem Sie `network.eval()` verwenden. Um es in den Trainingsmodus zu versetzen, wird `.train()` verwendet. WICHTIG ist, dass sich standardmäßig jede Schicht und jedes Netz im Modus `train()` befindet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "do.eval()\n",
    "do(beispiel_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im `.eval()` Modus wird der Dropout nicht angewendet.\n",
    "\n",
    "## Batchnorm\n",
    "\n",
    "\n",
    "Batchnorm-Layers sind eine weitere häufig verwendete Layer in neuronalen Netzen. Wie der Name suggeriert, normalisieren sie die Batches oder die besser gesagt die Activations der Minibatches.\n",
    "Erinnern Sie sich daran, dass wir unsere Eingaben skalieren. Batchnorm tut dasselbe, aber im Netz selbst, so dass die Werte tiefer im Netz auch ungefähr die selbe Größenordnung haben. Grundsätzlich werden die Variablen wie folgt normalisiert:\n",
    "\n",
    "\n",
    "$$x_s = \\frac{x-\\bar{x}}{sd_x}$$\n",
    "\n",
    "Dabei steht $\\bar{x}$ für den Mittelwert und $sd_x$ ist die Standardabweichung von $x$.<br><br>\n",
    "In einem klassischen neuronalen Netz werden die Activations einer Layer in zwei Schritten berechnet. Zunächst wird die lineare Transformation durchgeführt:\n",
    "\n",
    "$$Z = XW^T+b$$\n",
    "*Hier ist $X$ ein Minibatch, also zum Beispiel nur 32 Bilder*\n",
    "\n",
    "Darauf folgt eine nicht-lineare Aktivierungsfunktion:\n",
    "\n",
    "$$A = \\sigma(Z)$$\n",
    "\n",
    "Mit Batchnorm werden die Werte vor der Aktivierungsfunktion noch einmal normalisiert.\n",
    "\n",
    "$$Z_s = (\\frac{Z-\\bar{Z}}{sd_Z}) \\cdot \\gamma + \\beta $$\n",
    "\n",
    "Dies geschieht jedes Neuron unabhängig von einander. Der Mittelwert $\\bar{Z}$ und die Standardabweichung $sd_Z$ werden auch hier nur pro Minibatch berechnet. Neu ist das $\\gamma$ und $\\beta$. Dies sind nur zwei einzelne Parameter, die die Normalverteilung $N(0,1)$ verschieben und skalieren können. Auch diese beiden Parameter sind lernbar, d.h. sie werden auch beim Training verändert.\n",
    "\n",
    "\n",
    "Wichtig ist auch, dass während des Trainings die Mittelwerte der Minibatches zusammengefasst werden zu einem Mittelwert über alle Minibatches. Dieser Durchschnitt wird dann in der Evaluierung des Testsets zur Normalisierung der Minibatches verwendet.\n",
    "\n",
    "*Ob man zuerst die Aktivierungsfunktion und dann die Batchnorm anwendet oder umgekehrt, ist eine Frage, die Ihnen niemand beantworten kann. Es gibt Pro und Contra für beide Methoden.\n",
    "\n",
    "Vervollständige die `layer_one`. Die Größe der Hidden Layers und des Dropouts ist Ihnen überlassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = next(iter(loader)) # hier wird der erste Minibatch ausgewählt\n",
    "\n",
    "layer_one = nn.Sequential(nn.Linear(____,___),\n",
    "                         nn.BatchNorm1d(_____),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(____))\n",
    "layer_one(batch_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "layer_one = nn.Sequential(nn.Linear(784,300),\n",
    "                         nn.BatchNorm1d(300),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(0.2))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erweitern Sie nun das komplette Netz von vorhin.\n",
    "Wichtig: Nach der letzten linearen Layer wird weder Batchnorm noch Dropout verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netzwerk = nn.Sequential(nn.Linear(784,300), \n",
    "                         ______________,\n",
    "                         ______________,\n",
    "                         ______________,\n",
    "                         nn.Linear(300,300),\n",
    "                         ______________,\n",
    "                         ______________,\n",
    "                         ______________,\n",
    "                         nn.Linear(300,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "netzwerk = nn.Sequential(nn.Linear(784,300), \n",
    "                         nn.BatchNorm1d(300),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(0.2),\n",
    "                         nn.Linear(300,300),\n",
    "                         nn.BatchNorm1d(300),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(0.2),\n",
    "                         nn.Linear(300,10))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "\n",
    "Optimieres bestimmen, wie genau die Weights des Netzes geupdatet werden. Bisher haben Sie immer den `SGD` verwendet.  In der PyTorch-Implementierung vom `SGD` gibt es mehr Parameter für `SGD`, die das Training effektiver machen können. Einer davon ist \"Momentum\". Wenn Sie Momentum für das Training verwenden, werden die Weights nicht nur gemäß den Gradienten des aktuellen Minibatch aktualisiert. Auch die Gradienten der vorherigen Minibatches haben einen Einfluss. Der Parameter `momentum` gibt an, wie stark oder schwach der Einfluss der vorherigen Minibatches ist.\n",
    "\n",
    "Das Momentum soll den Loss in einer geraden Linie optimieren. Als Beispiel kann man sich einen Ball vorstellen, der einen Hügel hinunterrollt. Je länger diese in dieselbe Richtung rollt, desto schneller wird sie. Und je schneller sie wird, desto weniger Einfluss haben kleine Richtungsänderungen, die durch Veränderungen des Geländes (Gradients) ausgelöst werden. \n",
    "\n",
    "Wenn also die Gradients eine Zeit lang in dieselbe Richtung zeigen, dann sollte ein einzelner Minibatch diese Richtung nicht auf einmal ändern. \n",
    "\n",
    "Einer der am häufigsten verwendeten Algorithmen für das Training von Netzen ist der \"ADAM\"-Algorithmus. Er kombiniert viele Verbesserungen von SGD, einschließlich einer Version von Momentum.  \n",
    "\n",
    "**ADAM muss nicht immer besser sein als SGD**.\n",
    "\n",
    "Eine Übersicht über alle verfügbaren Optimierungsalgorithmen finden Sie [hier](https://pytorch.org/docs/stable/optim.html#algorithms). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updaten = optim.Adam(netzwerk.parameters(), lr=0.1)\n",
    "EPOCHS = 10\n",
    "\n",
    "## Trainings Loop\n",
    "for i in range(EPOCHS):\n",
    "    loss_list = [] # diese Liste speichtert den Loss jedes Minibatches\n",
    "                   # damit können wir am Ende den Durschnittsloss innerhalb des Epochs berechnen\n",
    "    netzwerk.train()\n",
    "    for minibatch in loader: # for-loop geht durch alle Minibatches\n",
    "        images, labels = minibatch # minibatch wird in Bilder und Labels geteilt\n",
    "        \n",
    "        updaten.zero_grad()\n",
    "        output = netzwerk(images) # Forward Propagation\n",
    "    \n",
    "        loss   = loss_funktion(output, labels)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        updaten.step()\n",
    "    netzwerk.eval()    \n",
    "    output = netzwerk(train_images)\n",
    "    acc=((output.max(dim=1)[1]==train_labels).sum()/float(output.shape[0])).item()\n",
    "    \n",
    "    print(i,\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), acc)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übungsaufgabe\n",
    "\n",
    "Für die Übungsaufgabe werden Sie ein Netzwerk trainieren, aber dieses Mal mit den Toxizitätsdaten aus Woche 5.\n",
    "Laden Sie zunächst erneut alle erforderlichen Libraries und Daten. Verwenden Sie dieses Mal auch Batchnorm, Dropout und den ADAM-Algorithmus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "%run ../utils/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_tox = pd.read_csv(\"../data/toxicity/sr-mmp.tab\", sep = \"\\t\")\n",
    "data_tox = data_tox.iloc[:,1:] #alle Spalten bis auf die erste (index 0) werden ausgewählt\n",
    "data_tox.columns = [\"smiles\", \"activity\"]\n",
    "data_tox.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Nächstes berechnen Sie die Fingerprints. Wie in Woche 05 steht Ihnen dafür die Funktion `get_fingerprints` zur Verfügung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fps = get_fingerprints(data_tox)\n",
    "fps[\"activity\"] = data_tox.activity\n",
    "fps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor Sie sie in Pytorch verwenden können, müssen Sie sowohl die Fingerprints als auch die `acitivty` in `tensors` umwandeln. Beachten Sie, dass beide im DataFrame `fps` sind. \n",
    "\n",
    "`.values` konvertiert einen DataFrame in ein `np.array`.\n",
    "\n",
    "Dann werden die Daten in einen Trainings- und einen Testsatz aufgeteilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = torch.tensor(___.values, dtype=torch.float32) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test=train_test_split(fps,test_size= 0.2 , train_size= 0.8, random_state=1234)\n",
    "\n",
    "\n",
    "train_x = train[:,:-1]\n",
    "train_y = train[:,-1]\n",
    "test_x = test[:,:-1]\n",
    "test_y = test[:,-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt wollen wir wieder Minibatches verwenden. Dazu müssen wir unsere Trainingsdaten noch in einen `DataLoader` umwandeln. Warum nur die Trainingsdaten? Die Verwendung von Minibatches ist nur für das Training relevant. Solange Ihr Computer in der Lage ist, den Testdatensatz auf einmal durch das Netz zu führen, brauchen wir den Testdatensatz nicht in Minibatches aufzuteilen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=data.TensorDataset(______, _____) # input sind unsere Tensors die einmal die Fingerprints die activities\n",
    "loader=data.DataLoader(train_data, batch_size = 32)\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passen Sie das Netz so an, dass der Input und der Output die richtige Größe haben. Also die Länge der Fingerprinzs und die Anzahl der Klassen, die wir vorhersagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netzwerk = nn.Sequential(nn.Linear(), \n",
    "                         nn.BatchNorm1d(),\n",
    "                         nn.ReLU(), \n",
    "                         nn.Dropout(),\n",
    "                         nn.Linear(),\n",
    "                         nn.BatchNorm1d(),\n",
    "                         nn.ReLU(), \n",
    "                         nn.Dropout(),\n",
    "                         nn.Linear())\n",
    "\n",
    "loss_funktion = nn.BCEWithLogitsLoss()\n",
    "updaten = ___________________, lr=0.1)    \n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als letztes füllen Sie den `for loop`. \n",
    "\n",
    "`.squeeze` konvertiert den `(n,1)` `output` tensor zu einem 1-dimensionalen `tensor` der Länge `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(EPOCHS):\n",
    "    loss_list = [] # diese Liste speichtert den Loss jedes Minibatches\n",
    "    for minibatch in loader: # for-loop geht durch alle minibatches\n",
    "        updaten.__________\n",
    "        molecules, activity = minibatch # Minibatchh wird in Bilder und Labels geteilt\n",
    "        output = netzwerk(____________) # Forward Propagation\n",
    "        loss   = loss_funktion(output.squeeze(), ____________)\n",
    "        loss._______\n",
    "        loss_list.append(loss.item())\n",
    "        updaten.________\n",
    "    # Hier wird die Accuracy für den Testsatz berechnet\n",
    "    output = netzwerk(test_x)\n",
    "    acc = torch.sum((output>0).squeeze().int() == test_y)/float(test_y.shape[0])\n",
    "   \n",
    "    print(\n",
    "        \"Training Loss: %.2f Test Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), acc.item())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
