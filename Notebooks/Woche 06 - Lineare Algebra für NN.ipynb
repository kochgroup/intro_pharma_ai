{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5cd938d",
   "metadata": {},
   "source": [
    "# Lineare Algebra\n",
    "\n",
    "---\n",
    "### Lernziele\n",
    "\n",
    "- Sie verstehen, was Vektoren und Matrizen sind und können damit einfache Berechnungen in Python durchführen\n",
    "- Sie sind in der Lage, verschiedene einfache Funktionen abzuleiten.\n",
    "- Sie verstehen, wie die Kettenregel funktioniert und warum sie für neuronale Netze so nützlich ist.\n",
    "---\n",
    "\n",
    "\n",
    "Heute werden wir die wesentlichen mathematischen Grundlagen für neuronale Netze erklären.\n",
    "\n",
    "Das erste essentielle mathematische Konzept ist der **Vektor**.\n",
    "\n",
    "Ein Vektor stellt einer Punkt in einem Raum da, der durch von mehreren Werten beschrieben wird.\n",
    "Zum Beispiel, kann ein Molekül von mehreren Deskriptoren beschrieben werden. \n",
    "\n",
    "Eine Vektor wird wie folgt dargestellt:\n",
    "\n",
    "$$\\begin{bmatrix}3 & 4 & 0.5\\end{bmatrix}$$ \n",
    "\n",
    "Dieser Vektor enthält genau drei Werte. Wir können Vektoren verwenden, um einzelne Datenpunkte zu beschreiben. Zum Beispiel könnten wir die Daten eines Hauses in diesem Vektor speichern. Der erste Wert gibt an, wie viele Bäder das Haus hat, der zweite, wie viele Schlafzimmer, und der dritte Wert gibt das Alter der Heizungsanlage in Jahren an.\n",
    "\n",
    "Sie haben sicher bemerkt, dass ein Vektor erstaunliche Ähnlichkeiten mit einem 1-dimensionalen `array` hat.\n",
    "`np.array([3,4,0.5])`. In der Tat sollen `np.arrays` die gleichen Funktionen wie Vektoren haben. Die mathematischen Regeln, die für Vektoren gelten, gelten auch für die `arrays`.\n",
    "\n",
    "\n",
    "Wir können zum Beispiel einen Vektor mit einer Zahl multiplizieren: <br>\n",
    "\n",
    "\n",
    "$$3\\cdot\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix}= \\begin{bmatrix}3\\cdot 3 \\\\ 3 \\cdot 4  \\\\ 3 \\cdot 0.5 \\end{bmatrix}= \\begin{bmatrix}9 \\\\ 12 \\\\ 1.5\\end{bmatrix} $$ \n",
    "\n",
    "<center> <i>Für bessere Übersicht schreiben wir den Vektor untereinander </i> </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d243be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "3 * np.array([3,4,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38713505",
   "metadata": {},
   "source": [
    "Gleiches gilt auch für die Addition und Substraktion:\n",
    "$$3+\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix}= \\begin{bmatrix}3+3 \\\\ 3+4 \\\\ 3+0.5\\end{bmatrix}= \\begin{bmatrix}6 \\\\ 7 \\\\ 3.5\\end{bmatrix} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3074acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "3 + np.array([3,4,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138917dd",
   "metadata": {},
   "source": [
    "Auch können wir zwei Vektoren addieren:\n",
    "    \n",
    "    \n",
    "$$\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix} + \\begin{bmatrix}0.3 \\\\ 3 \\\\ -0.2\\end{bmatrix} = \\begin{bmatrix}3 +0.3 \\\\ 4+3 \\\\ 0.5-0.2\\end{bmatrix} =  \\begin{bmatrix}3.3 \\\\ 7 \\\\ 0.3\\end{bmatrix}$$\n",
    "\n",
    "Hierbei ist wichtig, dass beide Vektoren die selbe Länge haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab074aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([3,4,0.5]) + np.array([0.3,3,-0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d35625",
   "metadata": {},
   "source": [
    "Vektoren werden erst wirklich interessant, wenn wir mehrere miteinander multiplizieren.\n",
    "\n",
    "Besonders das sogenannte Skalarprodukt ist für uns wichtig und wird wie folgt berechnet:\n",
    "$$\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix} \\cdot \\begin{bmatrix}0.3 \\\\ 3 \\\\ -0.2\\end{bmatrix} = (3\\cdot 0.3) + (4 \\cdot 3 )+ (0.5\\cdot -0.2) = 12.8  $$\n",
    "\n",
    "\n",
    "Berechnen Sie das Skalarprodukt der Vektoren per Hand: \n",
    "\n",
    "$$\\begin{bmatrix}8 \\\\ 0.25 \\\\ -1\\end{bmatrix} \\cdot \\begin{bmatrix}0.1 \\\\ 12 \\\\ 8\\end{bmatrix} = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a738a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung. HIER klicken</strong></summary>\n",
    "\n",
    "$$\\begin{bmatrix}8 \\\\ 0.25 \\\\ -1\\end{bmatrix} \\cdot \\begin{bmatrix}0.1 \\\\ 12 \\\\ 8\\end{bmatrix} =(8\\cdot 0.1) + (0.25 \\cdot 12)+ (-1\\cdot 8) = -4.2  $$\n",
    "</details>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7dc70d",
   "metadata": {},
   "source": [
    "\n",
    "In `numpy` benutzen wir `np.dot()`, um das Skalarprodukt zu berechnen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60ffbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(np.array([3,4,0.5]), np.array([0.3,3,-0.2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b686f2d",
   "metadata": {},
   "source": [
    "Wie Sie vielleicht bemerkt haben, ist das Skalarprodukt einer linearen Regression ähnlich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00799a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x    = np.array([3,4,0.5])\n",
    "beta = np.array([0.3,3,-0.2])\n",
    "np.dot(x,beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8094cb88",
   "metadata": {},
   "source": [
    "`x` ist der Eingabevektor, der die Informationen für drei Variablen enthält. Zum Beispiel für ein Haus, das 3 Badezimmer und 4 Schlafzimmer hat. Es wurde vor einem halben Jahr mit einer neuen Heizungsanlage ausgestattet (`0,5`). Der zweite Vektor enthält die Koeffizienten der Regression. Also $\\beta_1, \\beta_2, \\beta_3$. Mit Hilfe der Regression können wir dann den Wert des Hauses in 100.000 € ermitteln. \n",
    "\n",
    "Tatsächlich führt das Skalarprodukt zu einer Vereinfachung der Formel. Anstatt zu schreiben:\n",
    "$$\\hat{y} = \\beta_1x_1 +\\beta_2x_2 +\\beta_3x_3$$\n",
    "können wir die Formel auch wie folgt schreiben.\n",
    "\n",
    "$$\\hat{y} = x\\beta$$\n",
    "\n",
    "Hier müssen wir annehmen, dass $x$ und $\\beta$ Vektoren sind. \n",
    "Es fehlt natürlich noch das $t$ bzw. $\\beta_0$. Also der Schnittpunkt der y-Achse. Wie oben erläutert, können Einzelwerte einfach zu Vektoren addiert werden. \n",
    "\n",
    "Die vollständige Formel lautet also:\n",
    "\n",
    "$$\\hat{y} = x\\beta+\\beta_0$$\n",
    "\n",
    "Können Sie diese Formel mit `numpy` schreiben? Berechnen Sie $\\hat{y}$ für `x`. Dabei ist $\\beta_0=-5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c38f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 =-5\n",
    "y_hat = _____________________\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a455ca",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung. HIER klicken</strong></summary>\n",
    "\n",
    "```python\n",
    "y_hat = np.dot(x,beta)+beta_0\n",
    "    \n",
    "```\n",
    "</details>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2354d961",
   "metadata": {},
   "source": [
    "Angenommen, wir wollen `y_hat` nicht nur für ein Haus, sondern für mehrere Häuser gleichzeitig bestimmen, so können wir dies mit genau der gleichen Formel tun. \n",
    "\n",
    "`X` enthält nun nicht nur einen Vektor, sondern mehrere. Wie Sie bereits gelernt haben, können solche Datenstrukturen als 2D-Array gespeichert werden. Ein 2D-Array ist in der Mathematik mit einer Matrix vergleichbar. \n",
    "\n",
    "Wenn wir über Matrizen sprechen, verwenden wir groß geschriebene Variablennamen.\n",
    "\n",
    "Nachfolgend ist `X` angegeben. Sie können sehen, dass \"np.dot(X,beta) + beta_0\" immer noch das richtige Ergebnis liefert. Diesmal aber für jede der 4 Zeilen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[3,4,0.5],\n",
    "              [2,1,1.2],\n",
    "              [4,2,0.12],\n",
    "              [3,3,2]])\n",
    "\n",
    "np.dot(X,beta) + beta_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9c9ec",
   "metadata": {},
   "source": [
    "---\n",
    "Die Notation mit $\\beta$s stammt aus der traditionellen Statistik. Beim maschinellen Lernen werden die Koeffizienten mit $w$ bezeichnet, was für \"Gewichte\" steht. Darüber hinaus wird $\\beta_0$, der y-Achsenabschnitt, mit $b$ (bias) bezeichnet.\n",
    "Die Regressionsgleichung lautet somit:\n",
    "\n",
    "$$Xw+b$$\n",
    "\n",
    "Wir werden diese Schreibweise von nun an beibehalten.\n",
    "\n",
    "---\n",
    "\n",
    "Wie Sie bereits gelernt haben, besteht die Stärke neuronaler Netze darin, dass sie mehr als eine Regression gleichzeitig durchführen.\n",
    "Das heißt, wir haben nicht nur einen Satz von Regressionskoeffizienten, sondern mehrere. Wie viele?\n",
    "Das bleibt Ihnen überlassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728c6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "W =  np.array([beta,\n",
    "              [6,0,-2],\n",
    "              [1,0,3],\n",
    "              [0,0,-1],\n",
    "              [1,2,-1]])\n",
    "b = np.array([beta_0,3,2,0.5,-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b7ecaf",
   "metadata": {},
   "source": [
    "`W` enthält nun die Gewichte für insgesamt fünf lineare Regressionen. Die erste Zeile enthält immer noch unsere `beta`-Koeffizienten aus der ersten Regression.  Jede weitere Zeile enthält neue Koeffizienten/Gewichte für eine wreite Regression. Anhand der Anzahl der Zeilen können wir also erkennen, wie viele Regressionen wir durchführen. \n",
    "Auch `b` enthält fünf Werte und ist daher jetzt ein Vektor anstelle eines Skalars. Für jede Regression enthält er den y-Achsenabschnitt.\n",
    "\n",
    "Im Zusammenhang mit neuronalen Netzen entspricht die Anzahl der durchgeführten Regressionen der Anzahl der Knoten in der Hidden Layer des neuronalen Netzes.\n",
    "\n",
    "Wenn wir nun mit diesen beiden Matrizen rechnen wollen, geschieht folgendes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8b8804",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.dot(X,W)+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c81e8",
   "metadata": {},
   "source": [
    "Eine Fehlermeldung:\n",
    "\n",
    "```shapes (4,3) and (5,3) not aligned: 3 (dim 1) != 5 (dim 0)```\n",
    "\n",
    "Tatsächlich können wir aus der Fehlermeldung schließen, wo das Problem liegt. \n",
    "Zunächst werden uns die Dimensionen (Anzahl der Zeilen und Spalten) angegeben. \n",
    "`X` hat `4` Zeilen und `3` Spalten. `W` hat `5` Zeilen und `3` Spalten. \n",
    "\n",
    "Danach folgt: `3 (dim 1) != 5 (dim 0)`. Also, `3 (dim 1)`, die Anzahl der Spalten (`3 (dim 1)`) der ersten Matrix sind ungleich (`!=`) der Anzahl der Zeilen der zweiten Spalte (`5 (dim 0)`).   \n",
    "\n",
    "**Die Anzahl der Spalten der ersten Matrix sollten gleich der Anzahl der Reihen in der zweiten Spalte sein.**\n",
    "\n",
    "Wenn wir zum Beispiel, die `W` Matrix umdrehen indem wir sie \"über die Diagonale\" spiegeln erhalten wir Reihen als Spalten und Spalten als Reihen. Dann stimmen die Anzahl der Spalten der ersten Matrix und Reihen der zweiten Matrix überein.\n",
    "\n",
    "Das Konvertieren von Spalten zu Reihen und umgekehrt, nennt sich das *Transpose* einer Matrix.\n",
    "`W.tranpose()` führt diese Transformation aus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6fde17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W, \"\\n\")\n",
    "print(W.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30005d55",
   "metadata": {},
   "source": [
    "Wie Sie sehen können, werden die Zeilen zu Spalten. Dadurch änderen sich auch die Dimensionen der Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b175f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W.shape, \"\\n\")\n",
    "print(W.transpose().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab72f0c",
   "metadata": {},
   "source": [
    "Mit der Transponierung der Matrix `W` sollte die Multiplikation der beiden Matrizen funktionieren, da nun die Anzahl der Spalten/Zeilen identisch ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e09d26e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.dot(X,W.transpose())+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a3df87",
   "metadata": {},
   "source": [
    "Es funktioniert tatsächlich. Sehen Sie sich zum Beispiel die erste Spalte an. Diese Werte sind in der Tat die Ergebnisse der ersten Regression, die wir berechnet haben: `np.dot(X, beta)+beta_0`.\n",
    "Tatsächlich enthält jede Zeile die fünf Regressionsergebnisse für eines der vier Häuser.\n",
    "\n",
    "Aber wie kann es sein, dass die Regression funktioniert, obwohl wir die Matrix `W` umgedreht haben?\n",
    "\n",
    "Das liegt daran, wie die Matrixmultiplikation definiert worden ist. Das Skalarprodukt wird nicht zwischen den entsprechenden Zeilen berechnet. Das Skalarprodukt wird zwischen den Zeilen der ersten Matrix und den Spalten der zweiten Matrix berechnet (Zeile mal Spalte, d. h. Dimensionen der Zeilen und Spalten müssen gleich sein). \n",
    "\n",
    "![Matthew Scroggs](https://www.mscroggs.co.uk/img/full/multiply_matrices.gif)\n",
    "<center>Credit: Matthew Scroggs - 2020 | www.mscroggs.co.uk/blog/73 |</center>\n",
    "\n",
    "\n",
    "Das auch schon fast alles, was für den Forward Pass in einem neuronalen Netzwerk gebraucht wird.\n",
    "\n",
    "---\n",
    "\n",
    "Bis jetzt haben wir immer `np.dot()` für eine Matrixmultiplikation benutzt. Es gibt aber eine extra Funktion `np.matmul()`. Für große Matrizen ist `np.matmul` schneller und wir werden deswegen auch diese Funktion benutzen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f67a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(X,W.transpose())+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db34e67",
   "metadata": {},
   "source": [
    "# Ableitungen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41145f44",
   "metadata": {},
   "source": [
    "Um zu verstehen, wie neuronale Netze lernen, sollte man zumindest in groben Zügen wissen, was Abeleitungen sind und wie man sie berechnet.\n",
    "\n",
    "Die Ableitung einer Funktion beschreibt die Steigung der ursprünglichen Funktion. \n",
    "Angenommen, es gibt eine Funktion $f(x)=x^2$. Dann ist die entsprechende Ableitung $\\frac{df}{dx}=2x$ (d.h.: *Ableitung von f nach x*). \n",
    "\n",
    "Im Bild sind sowohl $f(x)$ (*blau*) also auch die Ableitung $\\frac{df}{dx}$ (*orange*) eingezeichnet. <br>Zum Beispiel für $x=-5$ ist $f(-5) = 25$. Die Steigung an diesem Punkt ist: $\\frac{df(-5)}{dx}=2\\cdot -5= -10$. Das heißt, die Steigung der Funktion $f(x)=x^2$ ist $-10$ wenn $x=-5$ ist.\n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_1edit.png\"></img>\n",
    "\n",
    "Es gibt einige Regeln zu Ableitung. Hier zuerst eine einfache Regel mit Beispiel: \n",
    "        $$f(x) = x^n \\rightarrow \\frac{df}{dx} = n \\cdot x^{n-1}$$\n",
    "        $$f(x) = x^2 \\rightarrow \\frac{df}{dx} = 2 \\cdot x^{2-1}=2x^1= 2x $$\n",
    "        \n",
    "\n",
    "Grundsätzlich fallen Konstanten immer in Ableitungen weg.\n",
    "\n",
    "Das heißt:\n",
    "Die Ableitung von $f(x)=x^2 + 5$ ist trotzdem nur $2x$, da Konstanten die Funktion nur verschieben, aber nicht in ihre Steigung beeinflussen. \n",
    "\n",
    "Anders werden Koeffizienten gehandhabt:\n",
    "\n",
    "$$f(x) = ax^n \\rightarrow \\frac{df}{dx} = (n \\cdot a)\\cdot x^{n-1}$$\n",
    "\n",
    "Ein Beispiel:\n",
    "\n",
    "$$f(x) = 4x^3 \\rightarrow \\frac{df}{dx} = 12x^2$$ \n",
    "\n",
    "\n",
    "**Probieren Sie folgende Funktionen abzuleiten (wahrscheinlich einfacher auf Papier):**\n",
    "\n",
    "$$g(x)= 7x^5 - 3$$\n",
    "\n",
    "$$h(x)= 0.5x^2 + 3x +12$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57264817",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung. HIER klicken</strong></summary>\n",
    "\n",
    "$$\\frac{dg}{dx} = 35x^4 $$\n",
    "$$\\frac{dh}{dx} = x +3$$\n",
    "\n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58906fd6",
   "metadata": {},
   "source": [
    "# Kettenregel \n",
    "\n",
    "Die wichtigste Regel für neuronale Netze ist die Kettenregel, bei der verkettete Funktionen abgeleitet werden, d.h. allgemeine Funktionen des Typs: $$f(x) = g(h(x))$$ Die Ableitung einer solchen Funktion lautet dann: $$\\frac{df}{dx} = \\frac{dg}{dh}\\cdot \\frac{dh}{dx}$$ Anhand der Formel ist sie schwierig zu verstehen, doch anhand eines Beispieles sollte es relativ einfach sein.\n",
    "\n",
    "$$\\begin{align}f(x)&= (3x + 1)^2 \\\\&=h^2; \\space\\space\\space\\space\\space\\space h = 3x+1\\end{align}$$<br>\n",
    "$$\\begin{align}\n",
    "\\frac{df}{dx} &= \\frac{d}{dh} (h^2)\\cdot \\frac{d}{dx}h\\\\\n",
    "&= 2 h\\cdot \\frac{d}{dx}(3x+1)\\\\\n",
    "&= 2 h \\cdot 3 \\\\\n",
    "&= 6 \\cdot (3x+1)\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "\n",
    "Zuvor wurde gesagt, dass die Ableitung die Steigung der ursprünglichen Funktion beschreibt. Man kann die Ableitung $\\frac{df}{dx}$ auch wie folgt interpretieren: *Um wie viel ändert sich $f(x)$, wenn ich $x$ ändere? Hier hängt der Betrag der Änderung natürlich von $x$ selbst ab. Im Beispiel $x^2$ haben kleine Änderungen von $x$ bei Werten um $x=5$ größere Auswirkungen als bei Werten um $x=1$. \n",
    "\n",
    "Wenn wir die Gewichte eines Netzes optimieren wollen, müssen wir auch wissen, wie eine Änderung der Gewichte eine Änderung des Verlustes bewirkt. \n",
    "\n",
    "Hier noch einmal ein schematisches Beispiel für ein neuronales Netz.\n",
    "\n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_3edit.png\"></img>\n",
    "\n",
    "Für das folgende Beispiel betrachten wir nur den letzten Teil genauer. Die Berechnung von $\\hat{y}$ erfolgt in zwei Schritten. Zuerst wird $Z_2$ berechnet, dann wird eine nichtlineare Funktion darauf angewendet, die uns $\\hat{y}$ liefert. \n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_4edit.png\"></img>\n",
    "\n",
    "**Für dieses Beispiel betrachten wir ein Beispiel mit nur einem Wert**.\n",
    "\n",
    "$a_1$ ist also in diesem Moment kein Vektor, sondern nur ein einzelner Wert, dasselbe gilt für $w_2$ und $b_2$.\n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_5.png\"></img>\n",
    "\n",
    "\n",
    "Die Frage ist: Welchen Einfluss hat $w_2$/$b_2$ auf den Verlust $J$. Oder wie ändert sich der Verlust, wenn wir $w_2$/$b_2$ ändern?\n",
    "\n",
    "Mathematisch gesehen können wir dies als die Ableitung von $J$ nach $w_1$ bezeichnen. \n",
    "Wir verwenden jetzt $\\partial$ anstelle von $d$, da wir über Funktionen mit mehreren Parametern ($w_2$ und $b_2$) sprechen.\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2}$$\n",
    "\n",
    "Allerdings gibt es keinen direkten Einfluss von $w_2$ auf den Loss. $w_2$ beeinflusst $z_2$ und $z_2$ hat einen Effekt auf $\\hat{y}$. Und schlussendlich hat $\\hat{y}$ Einfluss auf den Loss. Die Funktionen zur Berechnung von $\\hat{y}$ bzw. $J$ sind also *verkettet*.\n",
    "\n",
    "Die Kettenregel erlaubt es uns genau so $\\frac{\\partial J}{\\partial w_2}$ zu berechnen.\n",
    "\n",
    "Zunächst berechnen wir den Effekt von $w_2$ auf $z_2$:\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}.... $$\n",
    "\n",
    "Als Nächstes kommt, der Effekt von $z_2$ auf $\\hat{y}$ dazu:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}\\frac{\\partial \\hat{y}}{\\partial z_2} $$\n",
    "\n",
    "Als Letztes noch der Effekt von $\\hat{y}$ auf $J$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}\\frac{\\partial \\hat{y}}{\\partial z_2}\\frac{\\partial J}{\\partial \\hat{y}} $$\n",
    "\n",
    "\n",
    "Die Kettenregel erlaubt es uns, diese Effekte einfach zu multiplizieren, um die gewünschte Ableitung zu erhalten.\n",
    "Diese Kette kann beliebig lang werden, daher kann auch ein Netzwerk beliebig groß werden. \n",
    "Da es, wie Sie sich erinnern können, auch ein $w_1$ und $b_1$ gibt, kann auch deren Wirkung auf $J$ berechnet werden. Dazu funktioniert die Kettenregel genauso, die \"Kette\" wird nur länger.\n",
    "\n",
    "\n",
    "## Beispiel:\n",
    "\n",
    "$$e_1 = 2x+3$$\n",
    "$$e_2 = 0.5e_1^3$$\n",
    "\n",
    "Berechnen Sie $$\\frac{de_2}{dx}$$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab201c6",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung. HIER klicken</strong></summary>\n",
    "\n",
    "$$\\frac{de_2}{dx}= \\frac{de_1}{dx}\\frac{de_2}{de_1} $$\n",
    "$$\\frac{de_2}{dx}= 2(1.5e_1^2) $$\n",
    "    \n",
    "Da wir wissen, dass $e_1 = 2x+3$ ist, können wir diese auch in die Ableitung einsetzen.\n",
    "$$\\frac{de_2}{dx}= 2(1.5(2x+3)^2) $$ \n",
    "$$\\frac{de_2}{dx}= 2(1.5(4x^2+12x+9)) $$     \n",
    "$$\\frac{de_2}{dx}= 2(6x^2+18x+13.5) $$ \n",
    "$$\\frac{de_2}{dx}= 12x^2+36x+27 $$   \n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc15184",
   "metadata": {},
   "source": [
    "# Übungsaufgabe\n",
    "\n",
    "In dieser Übung berechnen Sie auch den Gradienten für $w$ wie in einem neuronalen Netz. \n",
    "Natürlich vereinfacht und nur für einen Wert von $w$. In diesem Beispiel verwenden wir eine einfache Lossfunktion und auch keine echte nicht-lineare Funktion. Die Lossfunktion würde in der realen Anwendung nicht funktionieren. Das Gleiche gilt für die nichtlineare Funktion, da sie linear ist. Eine nichtlineare Funktion, würde den Rahmen dieser Übung sprengen.\n",
    "\n",
    "\n",
    "Bitte versuchen Sie, diese Aufgabe nach bestem Wissen und Gewissen zu lösen. Wie wir schon oft gesagt haben, ist es für uns nicht wichtig, dass Sie das richtige Ergebnis erhalten, sondern dass Sie sich mit dem Thema beschäftigt haben. Manchen Menschen fällt Mathe leichter als anderen, das ist uns bewusst. \n",
    "\n",
    "\n",
    "\n",
    "Zurück zu unserem \"faux\" neuronalen Netz.\n",
    "Nehmen wir an, die letzte Schicht unseres Netzwerks funktioniert wie folgt:\n",
    "\n",
    "$$z_2 = a_1w_2+b_2$$\n",
    "$$\\hat{y} = z_2^3-3$$\n",
    "$$J = \\hat{y}^2- y^2$$\n",
    "\n",
    "\n",
    "Berechnen Sie $\\frac{\\partial J}{\\partial w_2}$, also den „Einfluss“ von $w_2$ auf $J$ (Loss).\n",
    "Hierfür geben wir die Werte:\n",
    "<center>\n",
    "$ a_1 = 2 $ <br> $ b_2=1.4 $ <br>   $ w_2 =0.6 $  <br>  $ y=1 $ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnen sie zunächst z_2, y_hat, und J. Also quasi der Forwardpass \n",
    "weight =0.6\n",
    "\n",
    "z_2 = ___*weight+___\n",
    "\n",
    "y_hat = (z_2**__)-___\n",
    "\n",
    "J = ____-____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbb49f0",
   "metadata": {},
   "source": [
    "Sie haben den Forward Pass durchgeführt, nun folgt die Berechnung der Gradienten. Dazu müssen wir zunächst nur die einzelnen Ableitungen berechnen.\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}\\frac{\\partial \\hat{y}}{\\partial z_2}\\frac{\\partial J}{\\partial \\hat{y}} $$\n",
    "\n",
    "Als Erstes berechnen Sie $\\frac{\\partial z_2}{\\partial w_2}$ welches wir `dw_2` nennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a525c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0c233",
   "metadata": {},
   "source": [
    "Als Nächstes berechnen Sie $\\frac{\\partial \\hat{y}}{\\partial z_2}$ welches wir `dz_2` nennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926cadee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz_2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746dd778",
   "metadata": {},
   "source": [
    "Als Letzes berechnen Sie $\\frac{\\partial J}{\\partial \\hat{y}}$ welches wir `dy_hat` nennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292cde37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_hat = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac66d47",
   "metadata": {},
   "source": [
    "Um den Gradienten zu berechnen, müssen Sie nun nur diese drei miteinander multiplizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e17169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = dw_2*dz_2*dy_hat\n",
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bedff77",
   "metadata": {},
   "source": [
    "Das war's auch schon! Sie haben die Gradienten berechnet.\n",
    "\n",
    "**Sie müssen die folgende Aufgabe nicht einreichen, aber Sie können sich daran versuchen.\n",
    "\n",
    "Wenn wir diese Ableitungen in einen `for-loop` setzen und die Gewichtung entgegen den Gradienten ein wenig ändern, können wir sehen, dass der Loss langsam kleiner wird. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c790b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight =0.6\n",
    "for i in range(10):\n",
    "    z_2 = ___*weight+___\n",
    "    y_hat = (z_2**__)-___\n",
    "    J = ____-____\n",
    "    dw_2 = \n",
    "    dz_2 = \n",
    "    dy_hat = \n",
    "    gradient = dw_2*dz_2*dy_hat\n",
    "    weight -=  0.0001* gradient # updaten des weights\n",
    "    print(J)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
